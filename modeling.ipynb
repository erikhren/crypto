{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Detect anomalies on seperate 3 seperate datasets \n",
    "    - Use different anomaly thresholds (1% -5%) \n",
    "    - And compare/verify similar dates and relationship \n",
    "    - Create a ranking/voting system for \"popular\" anomaly dates \n",
    "    - Additionally use Leave-one out method\n",
    "2. Same but merge 3 datasets \n",
    "3. Create labels from anomalies and run supervised models\n",
    "4. Optimization\n",
    "5. Build a website "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prof Notes:\n",
    "- which gets similar results to ensable\n",
    "- dif models and contamination, and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/in-12-minutes-stocks-analysis-with-pandas-and-scikit-learn-a8d8a7b50ee7\n",
    "https://stackoverflow.com/questions/47211866/how-to-mark-specific-data-points-in-matplotlib-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Import all models\n",
    "import sklearn\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.feature_bagging import FeatureBagging\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.loci import LOCI\n",
    "from pyod.models.lscp import LSCP\n",
    "from pyod.models.mcd import MCD\n",
    "from pyod.models.mo_gaal import MO_GAAL\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.sos import SOS\n",
    "from pyod.models.vae import VAE\n",
    "# from pyod.models.xgbod import XGBOD \n",
    "\n",
    "from scipy import stats\n",
    "pd.options.display.max_rows = 365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on each Dataset Seperetly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Dataset\n",
    "- load in data and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volumefrom</th>\n",
       "      <th>volumeto</th>\n",
       "      <th>CCI</th>\n",
       "      <th>ichimoku_leadSpanA</th>\n",
       "      <th>ichimoku_leadSpanB</th>\n",
       "      <th>MACD</th>\n",
       "      <th>...</th>\n",
       "      <th>ema_5</th>\n",
       "      <th>ema_10</th>\n",
       "      <th>ema_20</th>\n",
       "      <th>ema_30</th>\n",
       "      <th>ema_50</th>\n",
       "      <th>ema_100</th>\n",
       "      <th>ema_200</th>\n",
       "      <th>vwma</th>\n",
       "      <th>hull_Moving</th>\n",
       "      <th>Mkt_Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.309506</td>\n",
       "      <td>0.323185</td>\n",
       "      <td>0.314349</td>\n",
       "      <td>0.311447</td>\n",
       "      <td>0.158511</td>\n",
       "      <td>0.165547</td>\n",
       "      <td>0.610031</td>\n",
       "      <td>0.360129</td>\n",
       "      <td>0.336818</td>\n",
       "      <td>0.567335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340866</td>\n",
       "      <td>0.341051</td>\n",
       "      <td>0.342827</td>\n",
       "      <td>0.340625</td>\n",
       "      <td>0.333706</td>\n",
       "      <td>0.318544</td>\n",
       "      <td>0.299443</td>\n",
       "      <td>0.373789</td>\n",
       "      <td>0.382758</td>\n",
       "      <td>0.312638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.173618</td>\n",
       "      <td>0.181712</td>\n",
       "      <td>0.171755</td>\n",
       "      <td>0.174608</td>\n",
       "      <td>0.120752</td>\n",
       "      <td>0.150214</td>\n",
       "      <td>0.147846</td>\n",
       "      <td>0.207782</td>\n",
       "      <td>0.236865</td>\n",
       "      <td>0.167418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195385</td>\n",
       "      <td>0.203388</td>\n",
       "      <td>0.210658</td>\n",
       "      <td>0.215703</td>\n",
       "      <td>0.221806</td>\n",
       "      <td>0.226211</td>\n",
       "      <td>0.227650</td>\n",
       "      <td>0.186165</td>\n",
       "      <td>0.189751</td>\n",
       "      <td>0.172265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.225885</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.236921</td>\n",
       "      <td>0.226146</td>\n",
       "      <td>0.077413</td>\n",
       "      <td>0.061453</td>\n",
       "      <td>0.505965</td>\n",
       "      <td>0.254712</td>\n",
       "      <td>0.192137</td>\n",
       "      <td>0.470921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242474</td>\n",
       "      <td>0.231902</td>\n",
       "      <td>0.218803</td>\n",
       "      <td>0.202378</td>\n",
       "      <td>0.178738</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>0.130824</td>\n",
       "      <td>0.282522</td>\n",
       "      <td>0.295410</td>\n",
       "      <td>0.232673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.264223</td>\n",
       "      <td>0.276441</td>\n",
       "      <td>0.267946</td>\n",
       "      <td>0.264287</td>\n",
       "      <td>0.124178</td>\n",
       "      <td>0.117303</td>\n",
       "      <td>0.592183</td>\n",
       "      <td>0.305660</td>\n",
       "      <td>0.234630</td>\n",
       "      <td>0.598206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287933</td>\n",
       "      <td>0.285601</td>\n",
       "      <td>0.273462</td>\n",
       "      <td>0.257315</td>\n",
       "      <td>0.255200</td>\n",
       "      <td>0.249962</td>\n",
       "      <td>0.231692</td>\n",
       "      <td>0.336388</td>\n",
       "      <td>0.337407</td>\n",
       "      <td>0.263083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.386708</td>\n",
       "      <td>0.406506</td>\n",
       "      <td>0.391708</td>\n",
       "      <td>0.390215</td>\n",
       "      <td>0.211678</td>\n",
       "      <td>0.220504</td>\n",
       "      <td>0.726902</td>\n",
       "      <td>0.458545</td>\n",
       "      <td>0.416687</td>\n",
       "      <td>0.650943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428683</td>\n",
       "      <td>0.437853</td>\n",
       "      <td>0.411689</td>\n",
       "      <td>0.418349</td>\n",
       "      <td>0.430019</td>\n",
       "      <td>0.422910</td>\n",
       "      <td>0.401396</td>\n",
       "      <td>0.498581</td>\n",
       "      <td>0.483048</td>\n",
       "      <td>0.386545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            close        high         low        open  volumefrom    volumeto  \\\n",
       "count  365.000000  365.000000  365.000000  365.000000  365.000000  365.000000   \n",
       "mean     0.309506    0.323185    0.314349    0.311447    0.158511    0.165547   \n",
       "std      0.173618    0.181712    0.171755    0.174608    0.120752    0.150214   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.225885    0.230769    0.236921    0.226146    0.077413    0.061453   \n",
       "50%      0.264223    0.276441    0.267946    0.264287    0.124178    0.117303   \n",
       "75%      0.386708    0.406506    0.391708    0.390215    0.211678    0.220504   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "              CCI  ichimoku_leadSpanA  ichimoku_leadSpanB        MACD  ...  \\\n",
       "count  365.000000          365.000000          365.000000  365.000000  ...   \n",
       "mean     0.610031            0.360129            0.336818    0.567335  ...   \n",
       "std      0.147846            0.207782            0.236865    0.167418  ...   \n",
       "min      0.000000            0.000000            0.000000    0.000000  ...   \n",
       "25%      0.505965            0.254712            0.192137    0.470921  ...   \n",
       "50%      0.592183            0.305660            0.234630    0.598206  ...   \n",
       "75%      0.726902            0.458545            0.416687    0.650943  ...   \n",
       "max      1.000000            1.000000            1.000000    1.000000  ...   \n",
       "\n",
       "            ema_5      ema_10      ema_20      ema_30      ema_50     ema_100  \\\n",
       "count  365.000000  365.000000  365.000000  365.000000  365.000000  365.000000   \n",
       "mean     0.340866    0.341051    0.342827    0.340625    0.333706    0.318544   \n",
       "std      0.195385    0.203388    0.210658    0.215703    0.221806    0.226211   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.242474    0.231902    0.218803    0.202378    0.178738    0.152300   \n",
       "50%      0.287933    0.285601    0.273462    0.257315    0.255200    0.249962   \n",
       "75%      0.428683    0.437853    0.411689    0.418349    0.430019    0.422910   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "          ema_200        vwma  hull_Moving     Mkt_Cap  \n",
       "count  365.000000  365.000000   365.000000  365.000000  \n",
       "mean     0.299443    0.373789     0.382758    0.312638  \n",
       "std      0.227650    0.186165     0.189751    0.172265  \n",
       "min      0.000000    0.000000     0.000000    0.000000  \n",
       "25%      0.130824    0.282522     0.295410    0.232673  \n",
       "50%      0.231692    0.336388     0.337407    0.263083  \n",
       "75%      0.401396    0.498581     0.483048    0.386545  \n",
       "max      1.000000    1.000000     1.000000    1.000000  \n",
       "\n",
       "[8 rows x 49 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "price = pd.read_csv('btc_preprocessed.csv', index_col = 0)\n",
    "# scale the data function using min-max\n",
    "def scale(df):\n",
    "    cols = df.columns\n",
    "    index = df.index\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(df)\n",
    "    X = pd.DataFrame(X)\n",
    "    X.columns = cols\n",
    "    X = X.set_index(index)\n",
    "    return X\n",
    "# save results in variable X\n",
    "X = scale(price)\n",
    "print(X.shape)\n",
    "# data is standerdized \n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Run Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VAE neurons adjusted (default nbr of neurons too much for blockchain dataset) \n",
    "    - for innitial run I would like to keep all the same\n",
    "- VAE returns:<br>\n",
    "UserWarning: Output model_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  5 INLIERS :  360 ABOD\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  0 INLIERS :  365 FB\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 HBOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 IF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  1 INLIERS :  364 Average KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 OCSVM\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 SOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  2 INLIERS :  363 LOF\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 49)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 49)           2450        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          6400        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           2080        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            66          dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            66          dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2)            0           dense_5[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 19,318\n",
      "Trainable params: 19,318\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 49)                6321      \n",
      "=================================================================\n",
      "Total params: 16,855\n",
      "Trainable params: 16,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              [(None, 2), (None, 2), (N 19318     \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 49)                16855     \n",
      "=================================================================\n",
      "Total params: 36,173\n",
      "Trainable params: 36,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_2.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 110.2526 - val_loss: 63.7273\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 75.7278 - val_loss: 54.2889\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 65.0122 - val_loss: 48.8208\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 59.3450 - val_loss: 42.7259\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 54.6853 - val_loss: 39.1223\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 52.0227 - val_loss: 36.3632\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 51.2737 - val_loss: 35.8809\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 51.0503 - val_loss: 35.4381\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 128us/step - loss: 50.9646 - val_loss: 35.2374\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 91us/step - loss: 50.7937 - val_loss: 35.0939\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 50.7490 - val_loss: 35.1181\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 50.7297 - val_loss: 35.0680\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 50.7450 - val_loss: 34.9899\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 50.6436 - val_loss: 34.9433\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 50.6474 - val_loss: 34.8013\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 50.6933 - val_loss: 35.0006\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.6678 - val_loss: 34.8878\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 50.6606 - val_loss: 34.9237\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6744 - val_loss: 34.8954\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.5784 - val_loss: 34.8801\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6275 - val_loss: 35.0321\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.6078 - val_loss: 34.7880\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 50.7062 - val_loss: 34.8673\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.5986 - val_loss: 34.7996\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 50.6081 - val_loss: 34.9103\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.5714 - val_loss: 34.8604\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.6806 - val_loss: 35.0407\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.5471 - val_loss: 34.8797\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.5408 - val_loss: 34.9036\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.7144 - val_loss: 34.8775\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6678 - val_loss: 35.0534\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.6531 - val_loss: 34.8103\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 50.5998 - val_loss: 34.9317\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 50.6432 - val_loss: 34.8409\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.5691 - val_loss: 34.8072\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.6616 - val_loss: 34.8025\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 50.6094 - val_loss: 34.8345\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6360 - val_loss: 34.8017\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 50.6307 - val_loss: 34.8607\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 50.6131 - val_loss: 34.8288\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 50.6168 - val_loss: 34.8396\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.6051 - val_loss: 34.8169\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 50.5973 - val_loss: 34.7993\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.6099 - val_loss: 34.8111\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.5758 - val_loss: 34.8112\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 50.5916 - val_loss: 34.7931\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 50.5892 - val_loss: 34.9146\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 50.6284 - val_loss: 34.9233\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 50.6285 - val_loss: 34.8858\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 50.6277 - val_loss: 34.8553\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.5889 - val_loss: 34.7520\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 50.6371 - val_loss: 34.7500\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6320 - val_loss: 34.8704\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.5909 - val_loss: 34.8497\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 50.5204 - val_loss: 34.8645\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 50.6889 - val_loss: 34.8338\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6775 - val_loss: 34.8765\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.5924 - val_loss: 34.8272\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6103 - val_loss: 34.8150\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 50.6062 - val_loss: 34.7923\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6083 - val_loss: 34.7552\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.5687 - val_loss: 34.8081\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6497 - val_loss: 34.7826\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6190 - val_loss: 34.8208\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.6291 - val_loss: 34.7766\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.5997 - val_loss: 34.8293\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6162 - val_loss: 34.8186\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.5960 - val_loss: 34.7956\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6113 - val_loss: 34.7794\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6512 - val_loss: 34.7486\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.6380 - val_loss: 34.8188\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.5993 - val_loss: 34.7908\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 50.5831 - val_loss: 34.7708\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 50.6008 - val_loss: 34.8825\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 50.6055 - val_loss: 34.7571\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 50.6126 - val_loss: 34.8904\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.5950 - val_loss: 34.7895\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 50.6182 - val_loss: 34.7888\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 50.6132 - val_loss: 34.8274\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 50.6528 - val_loss: 34.7754\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.6213 - val_loss: 34.7902\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 50.5947 - val_loss: 34.7859\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.5968 - val_loss: 34.7712\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6079 - val_loss: 34.7945\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6312 - val_loss: 34.7786\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.6150 - val_loss: 34.8661\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 50.6118 - val_loss: 34.7875\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.5780 - val_loss: 34.8162\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6032 - val_loss: 34.8278\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6092 - val_loss: 34.7954\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6348 - val_loss: 34.8490\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.6361 - val_loss: 34.8112\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.5935 - val_loss: 34.8138\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.6031 - val_loss: 34.8178\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.6151 - val_loss: 34.8514\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 50.5836 - val_loss: 34.7966\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 50.5773 - val_loss: 34.7774\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 50.6117 - val_loss: 34.8453\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 50.6061 - val_loss: 34.9006\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 50.5901 - val_loss: 34.8404\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 VAE\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 ABOD\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  5 INLIERS :  360 FB\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 HBOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 IF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  4 INLIERS :  361 KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  1 INLIERS :  364 Average KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 OCSVM\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 SOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  4 INLIERS :  361 LOF\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 49)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 49)           2450        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          6400        dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 64)           8256        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64)           0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 32)           2080        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 32)           0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 2)            66          dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 2)            66          dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 2)            0           dense_16[0][0]                   \n",
      "                                                                 dense_17[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 19,318\n",
      "Trainable params: 19,318\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 49)                6321      \n",
      "=================================================================\n",
      "Total params: 16,855\n",
      "Trainable params: 16,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_5 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_5.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "model_4 (Model)              [(None, 2), (None, 2), (N 19318     \n",
      "_________________________________________________________________\n",
      "model_5 (Model)              (None, 49)                16855     \n",
      "=================================================================\n",
      "Total params: 36,173\n",
      "Trainable params: 36,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 115.6865 - val_loss: 97.2798\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 74.6734 - val_loss: 80.1766\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 62.4721 - val_loss: 72.4613\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 56.0319 - val_loss: 66.1907\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 52.1393 - val_loss: 64.9868\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 49.7378 - val_loss: 63.7084\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 48.6258 - val_loss: 62.4465\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 48.0614 - val_loss: 62.8726\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 48.1575 - val_loss: 62.7695\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 96us/step - loss: 48.0397 - val_loss: 62.7034\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 47.8700 - val_loss: 62.5331\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.7309 - val_loss: 62.3843\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.6679 - val_loss: 62.5965\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.7413 - val_loss: 62.5574\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.5624 - val_loss: 62.5868\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.6325 - val_loss: 62.4676\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5094 - val_loss: 62.0870\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.5181 - val_loss: 62.3003\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.5711 - val_loss: 62.3523\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5661 - val_loss: 62.4440\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.6282 - val_loss: 62.1087\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.6279 - val_loss: 62.0242\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5574 - val_loss: 62.2474\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5500 - val_loss: 62.3486\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 47.6235 - val_loss: 62.1290\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5233 - val_loss: 62.0905\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5760 - val_loss: 62.3034\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5493 - val_loss: 62.2048\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.5365 - val_loss: 62.0891\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5217 - val_loss: 62.1484\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5273 - val_loss: 62.2562\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.5447 - val_loss: 62.2148\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5277 - val_loss: 62.1540\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5803 - val_loss: 62.2125\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5390 - val_loss: 62.1168\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5300 - val_loss: 62.2279\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.5492 - val_loss: 62.2180\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.5145 - val_loss: 62.1108\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.5218 - val_loss: 62.2036\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.5331 - val_loss: 62.2328\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5175 - val_loss: 62.2059\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5000 - val_loss: 62.2183\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.5176 - val_loss: 62.1607\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5220 - val_loss: 62.1741\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5028 - val_loss: 62.1814\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.5154 - val_loss: 62.1863\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.5443 - val_loss: 62.1967\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.5294 - val_loss: 62.2253\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.5258 - val_loss: 62.2008\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.4899 - val_loss: 62.1878\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.5176 - val_loss: 62.2443\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5225 - val_loss: 62.1926\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5178 - val_loss: 62.2248\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.5004 - val_loss: 62.1299\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.5170 - val_loss: 62.1630\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.5177 - val_loss: 62.2066\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5304 - val_loss: 62.2102\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5274 - val_loss: 62.1599\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5449 - val_loss: 62.1671\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.5447 - val_loss: 62.1623\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.5113 - val_loss: 62.2301\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.4947 - val_loss: 62.2170\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.5186 - val_loss: 62.2087\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5050 - val_loss: 62.2534\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.4960 - val_loss: 62.2253\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.5058 - val_loss: 62.2024\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.5150 - val_loss: 62.1462\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5330 - val_loss: 62.1912\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.5277 - val_loss: 62.1895\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5051 - val_loss: 62.1693\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.5159 - val_loss: 62.1666\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5281 - val_loss: 62.2006\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.5244 - val_loss: 62.1799\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.4950 - val_loss: 62.1923\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.5019 - val_loss: 62.2175\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.5005 - val_loss: 62.2013\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5177 - val_loss: 62.1603\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5111 - val_loss: 62.1921\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.4983 - val_loss: 62.2194\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5063 - val_loss: 62.2217\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5003 - val_loss: 62.1614\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.5180 - val_loss: 62.1732\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5285 - val_loss: 62.1775\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.5038 - val_loss: 62.1975\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.5342 - val_loss: 62.2811\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.5185 - val_loss: 62.2097\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.5209 - val_loss: 62.2050\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.5122 - val_loss: 62.1972\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 47.5182 - val_loss: 62.2024\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.5018 - val_loss: 62.1945\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.5092 - val_loss: 62.2217\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5090 - val_loss: 62.2194\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5177 - val_loss: 62.2208\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5125 - val_loss: 62.2050\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5078 - val_loss: 62.2116\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.5247 - val_loss: 62.1865\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.5160 - val_loss: 62.1841\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.4945 - val_loss: 62.1943\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.5070 - val_loss: 62.1848\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.5186 - val_loss: 62.1999\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 VAE\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  10 INLIERS :  355 ABOD\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  9 INLIERS :  356 FB\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 HBOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 IF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  9 INLIERS :  356 KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  3 INLIERS :  362 Average KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 OCSVM\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 SOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  7 INLIERS :  358 LOF\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 49)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 49)           2450        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 128)          6400        dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 128)          0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 64)           8256        dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 64)           0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 32)           2080        dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 32)           0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 2)            66          dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 2)            66          dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 2)            0           dense_27[0][0]                   \n",
      "                                                                 dense_28[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 19,318\n",
      "Trainable params: 19,318\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 49)                6321      \n",
      "=================================================================\n",
      "Total params: 16,855\n",
      "Trainable params: 16,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_8.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "model_7 (Model)              [(None, 2), (None, 2), (N 19318     \n",
      "_________________________________________________________________\n",
      "model_8 (Model)              (None, 49)                16855     \n",
      "=================================================================\n",
      "Total params: 36,173\n",
      "Trainable params: 36,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 116.9634 - val_loss: 92.1571\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 76.2211 - val_loss: 72.7482\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 64.5876 - val_loss: 63.6729\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 57.6575 - val_loss: 56.4555\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 52.3453 - val_loss: 53.6078\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 50.2322 - val_loss: 53.6118\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 49.4841 - val_loss: 52.9865\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 49.0427 - val_loss: 52.7283\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 49.0424 - val_loss: 52.2201\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 48.9040 - val_loss: 51.9962\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 48.9723 - val_loss: 52.0015\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.7906 - val_loss: 52.1064\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.9500 - val_loss: 52.1703\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.7455 - val_loss: 52.0208\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.8772 - val_loss: 51.9732\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.8385 - val_loss: 52.0467\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7801 - val_loss: 51.9862\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7546 - val_loss: 52.0591\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.7395 - val_loss: 52.0104\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7528 - val_loss: 52.0782\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6991 - val_loss: 52.0325\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.7188 - val_loss: 52.0210\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.7123 - val_loss: 52.0477\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6937 - val_loss: 51.9856\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7293 - val_loss: 51.9998\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.7088 - val_loss: 51.9661\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.7025 - val_loss: 52.0445\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6954 - val_loss: 51.9790\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6775 - val_loss: 52.0069\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7010 - val_loss: 51.9785\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6684 - val_loss: 51.9701\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6681 - val_loss: 52.0186\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6632 - val_loss: 51.9724\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6597 - val_loss: 52.0534\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6476 - val_loss: 52.0004\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6789 - val_loss: 52.0021\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6656 - val_loss: 51.9202\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6632 - val_loss: 52.0147\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.6721 - val_loss: 51.9910\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6567 - val_loss: 52.0503\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6990 - val_loss: 51.9645\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 48.6906 - val_loss: 51.9864\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.6791 - val_loss: 52.0234\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6695 - val_loss: 51.9740\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6652 - val_loss: 51.9570\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6695 - val_loss: 51.9548\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6806 - val_loss: 51.9782\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6744 - val_loss: 52.0050\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.6575 - val_loss: 52.0221\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6911 - val_loss: 51.9939\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 48.6761 - val_loss: 51.9586\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6639 - val_loss: 51.9650\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6722 - val_loss: 51.9990\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6665 - val_loss: 51.9768\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6744 - val_loss: 51.9462\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6553 - val_loss: 51.9392\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6823 - val_loss: 51.9768\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6617 - val_loss: 52.0070\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6688 - val_loss: 51.9750\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6610 - val_loss: 51.9994\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6680 - val_loss: 51.9939\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6498 - val_loss: 51.9699\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6600 - val_loss: 51.9850\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.6974 - val_loss: 51.9711\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.6378 - val_loss: 51.9982\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6833 - val_loss: 51.9860\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6573 - val_loss: 51.9669\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6924 - val_loss: 52.0369\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6534 - val_loss: 51.9993\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6705 - val_loss: 52.0281\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6407 - val_loss: 51.9814\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 48.6587 - val_loss: 52.0340\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6890 - val_loss: 52.0448\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6739 - val_loss: 51.9765\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 48.6533 - val_loss: 52.0120\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.6733 - val_loss: 51.9855\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6872 - val_loss: 51.9732\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6719 - val_loss: 51.9694\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6660 - val_loss: 51.9862\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.6768 - val_loss: 51.9843\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.6561 - val_loss: 51.9585\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.6657 - val_loss: 51.9724\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6786 - val_loss: 52.0016\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6811 - val_loss: 51.9935\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.6673 - val_loss: 51.9665\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.6638 - val_loss: 51.9637\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.6607 - val_loss: 51.9778\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6881 - val_loss: 51.9780\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6795 - val_loss: 51.9611\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.6542 - val_loss: 51.9617\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6736 - val_loss: 51.9754\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6651 - val_loss: 51.9712\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6718 - val_loss: 51.9690\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6753 - val_loss: 51.9584\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.6729 - val_loss: 51.9573\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.6622 - val_loss: 51.9597\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6638 - val_loss: 51.9656\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6724 - val_loss: 51.9595\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6763 - val_loss: 51.9575\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6590 - val_loss: 51.9716\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  12 INLIERS :  353 VAE\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  16 INLIERS :  349 ABOD\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  12 INLIERS :  353 FB\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 HBOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 IF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  12 INLIERS :  353 KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  3 INLIERS :  362 Average KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 OCSVM\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 SOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  12 INLIERS :  353 LOF\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 49)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 49)           2450        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 128)          6400        dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 128)          0           dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 64)           8256        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 64)           0           dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 32)           2080        dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 32)           0           dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 2)            66          dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 2)            66          dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 2)            0           dense_38[0][0]                   \n",
      "                                                                 dense_39[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 19,318\n",
      "Trainable params: 19,318\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 49)                6321      \n",
      "=================================================================\n",
      "Total params: 16,855\n",
      "Trainable params: 16,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_11 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_11.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "model_10 (Model)             [(None, 2), (None, 2), (N 19318     \n",
      "_________________________________________________________________\n",
      "model_11 (Model)             (None, 49)                16855     \n",
      "=================================================================\n",
      "Total params: 36,173\n",
      "Trainable params: 36,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 119.3733 - val_loss: 84.8888\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 108us/step - loss: 77.5501 - val_loss: 71.9424\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 64.7327 - val_loss: 63.6456\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 57.6957 - val_loss: 53.4078\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 53.2723 - val_loss: 48.5177\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 51.1150 - val_loss: 46.1451\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 50.3431 - val_loss: 45.7137\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 50.3564 - val_loss: 44.8708\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 127us/step - loss: 50.1384 - val_loss: 44.6523\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 94us/step - loss: 49.9507 - val_loss: 44.7652\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 49.7070 - val_loss: 44.7577\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.7720 - val_loss: 44.8419\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.7485 - val_loss: 44.5888\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5790 - val_loss: 44.6973\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.7076 - val_loss: 44.5213\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.6713 - val_loss: 44.6395\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.6379 - val_loss: 44.5472\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.6807 - val_loss: 44.4832\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.6344 - val_loss: 44.5185\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5899 - val_loss: 44.4981\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5324 - val_loss: 44.4313\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.4920 - val_loss: 44.5559\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.5590 - val_loss: 44.6348\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.6223 - val_loss: 44.6967\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5205 - val_loss: 44.4144\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.5560 - val_loss: 44.8822\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.5058 - val_loss: 44.5663\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5459 - val_loss: 44.5271\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.4868 - val_loss: 44.5399\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5806 - val_loss: 44.4334\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5182 - val_loss: 44.6700\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.5593 - val_loss: 44.6150\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5224 - val_loss: 44.4109\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5455 - val_loss: 44.6391\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.4971 - val_loss: 44.6420\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5446 - val_loss: 44.5334\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.6025 - val_loss: 44.3940\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.6260 - val_loss: 44.4360\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.4303 - val_loss: 44.2648\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5682 - val_loss: 44.3320\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5712 - val_loss: 44.3146\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5910 - val_loss: 44.4015\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.5219 - val_loss: 44.2148\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5392 - val_loss: 44.4619\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5864 - val_loss: 44.4645\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.5278 - val_loss: 44.3881\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5416 - val_loss: 44.4667\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5807 - val_loss: 44.6009\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5688 - val_loss: 44.3259\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5310 - val_loss: 44.1865\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5740 - val_loss: 44.4297\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5208 - val_loss: 44.4186\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5834 - val_loss: 44.2693\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.4982 - val_loss: 44.2957\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5616 - val_loss: 44.2787\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5643 - val_loss: 44.2643\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5176 - val_loss: 44.2149\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5380 - val_loss: 44.4511\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5302 - val_loss: 44.3622\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5555 - val_loss: 44.3635\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.4828 - val_loss: 44.2922\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5305 - val_loss: 44.4064\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5101 - val_loss: 44.6354\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5945 - val_loss: 44.5614\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 49.5282 - val_loss: 44.4343\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5579 - val_loss: 44.4072\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5149 - val_loss: 44.4636\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5327 - val_loss: 44.3923\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5406 - val_loss: 44.4383\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.4974 - val_loss: 44.3657\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5524 - val_loss: 44.3032\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.4972 - val_loss: 44.5052\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5129 - val_loss: 44.4874\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5407 - val_loss: 44.2912\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5320 - val_loss: 44.4169\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.4904 - val_loss: 44.4221\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5482 - val_loss: 44.7615\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5512 - val_loss: 44.5703\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5335 - val_loss: 44.4573\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5478 - val_loss: 44.5683\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5445 - val_loss: 44.3092\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5586 - val_loss: 44.4289\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5471 - val_loss: 44.3529\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5322 - val_loss: 44.3783\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5390 - val_loss: 44.3699\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.6074 - val_loss: 44.3657\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5316 - val_loss: 44.2994\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.4716 - val_loss: 44.4763\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5961 - val_loss: 44.3338\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5167 - val_loss: 44.5901\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5389 - val_loss: 44.4259\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.5394 - val_loss: 44.5394\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.5349 - val_loss: 44.5591\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.5437 - val_loss: 44.7116\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.4981 - val_loss: 44.4835\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.5507 - val_loss: 44.4988\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.4086 - val_loss: 44.6606\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.4526 - val_loss: 44.8312\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.6621 - val_loss: 44.7166\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.5621 - val_loss: 44.2817\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  14 INLIERS :  351 VAE\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  21 INLIERS :  344 ABOD\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  14 INLIERS :  351 FB\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 HBOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 IF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  15 INLIERS :  350 KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  3 INLIERS :  362 Average KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 OCSVM\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 SOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  16 INLIERS :  349 LOF\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 49)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 49)           2450        input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 128)          6400        dense_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 128)          0           dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 64)           8256        dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 64)           0           dense_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 32)           2080        dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 32)           0           dense_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 2)            66          dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 2)            66          dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 2)            0           dense_49[0][0]                   \n",
      "                                                                 dense_50[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 19,318\n",
      "Trainable params: 19,318\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 49)                6321      \n",
      "=================================================================\n",
      "Total params: 16,855\n",
      "Trainable params: 16,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_14 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_14.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "model_13 (Model)             [(None, 2), (None, 2), (N 19318     \n",
      "_________________________________________________________________\n",
      "model_14 (Model)             (None, 49)                16855     \n",
      "=================================================================\n",
      "Total params: 36,173\n",
      "Trainable params: 36,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 116.7092 - val_loss: 95.1480\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 75.7121 - val_loss: 77.7229\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 62.3644 - val_loss: 67.4819\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 111us/step - loss: 54.7099 - val_loss: 60.1346\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 51.7269 - val_loss: 57.8874\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 49.7466 - val_loss: 56.2500\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 49.3628 - val_loss: 54.9568\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 48.9972 - val_loss: 54.4973\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 48.8849 - val_loss: 54.6839\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 93us/step - loss: 48.6767 - val_loss: 54.2463\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 48.6536 - val_loss: 53.6376\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 48.5311 - val_loss: 54.5047\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.5610 - val_loss: 54.4146\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.6619 - val_loss: 54.1852\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.5450 - val_loss: 54.0791\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.5735 - val_loss: 53.9859\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.4491 - val_loss: 54.1166\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.5387 - val_loss: 54.1283\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.3920 - val_loss: 54.1701\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4831 - val_loss: 54.0207\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.4863 - val_loss: 54.6522\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.4389 - val_loss: 54.2766\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.5144 - val_loss: 54.1243\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4961 - val_loss: 53.9373\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4484 - val_loss: 54.0657\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.4051 - val_loss: 54.0335\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.4701 - val_loss: 54.0294\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4949 - val_loss: 53.9900\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.4794 - val_loss: 54.0876\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4978 - val_loss: 53.9431\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.5193 - val_loss: 53.9147\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.5371 - val_loss: 53.9564\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.4741 - val_loss: 53.8024\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.4503 - val_loss: 53.8563\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.4831 - val_loss: 53.8164\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.3952 - val_loss: 53.9975\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.4637 - val_loss: 54.0575\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4497 - val_loss: 53.9173\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.4432 - val_loss: 53.9906\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4954 - val_loss: 53.8948\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.4367 - val_loss: 53.8443\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4089 - val_loss: 54.0292\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4553 - val_loss: 53.9827\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4351 - val_loss: 53.9652\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4536 - val_loss: 54.0334\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.4363 - val_loss: 54.0160\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.3310 - val_loss: 53.8535\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.4784 - val_loss: 54.2380\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.5040 - val_loss: 53.9854\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.3956 - val_loss: 53.9945\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4915 - val_loss: 54.0226\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.5318 - val_loss: 53.7572\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.4383 - val_loss: 53.9563\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.5045 - val_loss: 53.8836\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.5180 - val_loss: 53.7924\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.5097 - val_loss: 53.8905\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.5212 - val_loss: 53.8644\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.4564 - val_loss: 53.8089\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4698 - val_loss: 53.8423\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.4381 - val_loss: 53.8239\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4339 - val_loss: 53.8343\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.4519 - val_loss: 53.8271\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.4314 - val_loss: 53.8551\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.4737 - val_loss: 53.8457\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.4908 - val_loss: 53.7959\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.4397 - val_loss: 53.8154\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.5000 - val_loss: 53.9230\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4694 - val_loss: 53.8294\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4579 - val_loss: 53.9071\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4616 - val_loss: 53.9255\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4677 - val_loss: 53.8634\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.4574 - val_loss: 53.8898\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.4517 - val_loss: 53.9411\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4749 - val_loss: 53.9363\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4768 - val_loss: 53.8431\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4434 - val_loss: 53.9288\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4692 - val_loss: 53.8842\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4583 - val_loss: 53.8333\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4698 - val_loss: 53.8917\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.4708 - val_loss: 53.8437\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4461 - val_loss: 53.8740\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4695 - val_loss: 53.8936\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4530 - val_loss: 53.8611\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4675 - val_loss: 53.8859\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.4522 - val_loss: 53.8472\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.4457 - val_loss: 53.7892\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.4649 - val_loss: 53.8653\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4821 - val_loss: 53.7927\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4480 - val_loss: 53.8481\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4350 - val_loss: 53.8194\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4742 - val_loss: 53.8651\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4634 - val_loss: 53.8652\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4563 - val_loss: 53.9026\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.4392 - val_loss: 53.8199\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.4467 - val_loss: 53.8264\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.4558 - val_loss: 53.8672\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4268 - val_loss: 53.8801\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.4664 - val_loss: 53.8627\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4917 - val_loss: 53.8733\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.4656 - val_loss: 53.8072\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 VAE\n"
     ]
    }
   ],
   "source": [
    "# time the code\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "# empty list to store outliers and inliners\n",
    "df_outliers = []\n",
    "scores_pred_lst = []\n",
    "proba_lst = []\n",
    "predict_rank_lst = []\n",
    "\n",
    "# iterate over different outlier fractions \n",
    "outliers_fraction = [0.01,0.02,0.03,0.04,0.05]\n",
    "random_state = 123 # set random state\n",
    "\n",
    "for fraction in outliers_fraction:\n",
    "    \n",
    "    # Define 13 outlier detection tools to be compared\n",
    "    classifiers = {#'AE' : AutoEncoder(contamination=outliers_fraction, random_state=random_state), # check why error\n",
    "               'ABOD': ABOD(contamination = fraction),\n",
    "              'CBLOF':CBLOF(check_estimator=False, random_state=random_state), # getting 0 \n",
    "              'FB':FeatureBagging(LOF(n_neighbors=35),contamination=fraction,check_estimator=False,random_state=random_state),\n",
    "              'HBOS': HBOS(contamination=fraction),\n",
    "              'IF': IForest(contamination=fraction,random_state=random_state),\n",
    "              'KNN': KNN(contamination=fraction),\n",
    "              'Average KNN': KNN(method='mean',contamination=fraction),\n",
    "              'OCSVM' : OCSVM(contamination=fraction),\n",
    "              'SOS' : SOS(contamination=fraction),\n",
    "              'LOF': LOF(n_neighbors=35, contamination = fraction),\n",
    "              #'LOCI': LOCI(contamination = fraction), # takes a super long time + don't understand\n",
    "              'VAE' : VAE(contamination=fraction, random_state=random_state) \n",
    "              }\n",
    "    \n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        clf.fit(X)\n",
    "        # predict raw anomaly score\n",
    "        scores_pred = clf.decision_function(X) * -1 # WHY? * -1\n",
    "\n",
    "        # prediction of a datapoint category outlier or inlier\n",
    "        y_pred = clf.predict(X)\n",
    "\n",
    "        # probability of outlier \n",
    "        probability = clf.predict_proba(X, method = 'linear')\n",
    "\n",
    "        # predict rank\n",
    "        predict_rank = clf._predict_rank(X)\n",
    "\n",
    "        n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "        n_outliers = np.count_nonzero(y_pred == 1)\n",
    "\n",
    "        # copy of dataframe\n",
    "        dfx = X.copy()\n",
    "\n",
    "        dfx[clf_name] = y_pred.tolist()\n",
    "        #print(clf_name,'\\n',i,clf,'\\n',classifiers,'\\n',classifiers.items())\n",
    "        print('Outlier fraction: ', fraction)\n",
    "        print('OUTLIERS : ',n_outliers,'INLIERS : ',n_inliers, clf_name)\n",
    "        #print('scores_pred: ', scores_pred)\n",
    "        scores_pred_lst.append(scores_pred)\n",
    "        df_outliers.append(dfx[clf_name]) \n",
    "        proba_lst.append(probability)\n",
    "        predict_rank_lst.append(predict_rank)\n",
    "t1 = time.time()\n",
    "total = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run:  44.76075267791748 s\n",
      "Nbr of lists 55 (5 outlier fractions and 11 models)\n"
     ]
    }
   ],
   "source": [
    "print('Time to run: ',total,'s')\n",
    "print('Nbr of lists',len(df_outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ranking table\n",
    "#frequencies_df_price = frequencies_df\n",
    "# save the outliers lists uniquely for later\n",
    "prices_df = df_outliers\n",
    "# probability lists\n",
    "prices_proba = proba_lst\n",
    "# rank lists\n",
    "prices_rank_lst = predict_rank_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Seperate models per contamination\n",
    "- Change if number of models change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the outliers identified by models into lists by fraction \n",
    "# we know that it starts with 0.01 and how many models\n",
    "frac_oo1 = df_outliers[0:11]\n",
    "frac_oo2 = df_outliers[11:22]\n",
    "frac_oo3 = df_outliers[22:33]\n",
    "frac_oo4 = df_outliers[33:44]\n",
    "frac_oo5 = df_outliers[44:]\n",
    "\n",
    "# subset data so we only get dates where models identified them as outliers\n",
    "def outlier_subset_function(outliers_lst):\n",
    "# convert lists into df and transpose it\n",
    "    btc_outliers = np.transpose(pd.DataFrame(outliers_lst))#.reset_index()\n",
    "    \n",
    "    outlier_dates = [] # this stores outlier dates\n",
    "    #lenght = len(outlier_dates)\n",
    "    for col in btc_outliers.columns:\n",
    "        outliers = btc_outliers[btc_outliers[col] == 1] # subset outlier data\n",
    "        outlier_dates.append(outliers.index.tolist()) # append the string/date \n",
    "        \n",
    "        #lenght2 = len(outlier_dates)\n",
    "        #nbr_dates = \n",
    "    return outlier_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function to extract anomolous dates for each model and for each fraction\n",
    "frac_oo1_outliers = outlier_subset_function(frac_oo1)\n",
    "frac_oo2_outliers = outlier_subset_function(frac_oo2)\n",
    "frac_oo3_outliers = outlier_subset_function(frac_oo3)\n",
    "frac_oo4_outliers = outlier_subset_function(frac_oo4)\n",
    "frac_oo5_outliers = outlier_subset_function(frac_oo5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- depending on the following lenght, the number of lists to be joined in the function below is modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nbr of list = nbr of models\n",
    "len(frac_oo1_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Get Unique Anomaly Dates per Contamination fraction\n",
    "- purpuse of count_weight_outliers function is to find unique anomaly dates for a fraction \n",
    "\n",
    "**NOTE:** \n",
    "1. Nbr of lists in count_weight_outliers function has to be adjusted depending on how many models you are using\n",
    "2. Weight_per_fraction col enumerator (division) needs to be changed depending on the number of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_weight_outliers(outlier_dates): \n",
    "    # we have 11 anomaly model lists, joined them into one \n",
    "    # this function only does it for 1 fraction i.e. 0.01\n",
    "    join =outlier_dates[0] + outlier_dates[1] + outlier_dates[2] + outlier_dates[3] + outlier_dates[4] +\\\n",
    "        outlier_dates[5] + outlier_dates[6] + outlier_dates[7] + outlier_dates[8] + outlier_dates[9] + outlier_dates[10]\n",
    "    # find unique dates and count their occurance\n",
    "    (unique, counts) = np.unique(join, return_counts=True)\n",
    "    # convert this into an array from tuple\n",
    "    frequencies = np.asarray((unique, counts)).T\n",
    "    # now make it a dataframe\n",
    "    frequencies_df = pd.DataFrame(frequencies)\n",
    "    # name the columns so you can rever to them more easily \n",
    "    frequencies_df.columns = ['Date','Count_Anomaly_Date']\n",
    "    # convert rank to integer type \n",
    "    frequencies_df.Count_Anomaly_Date = frequencies_df.Count_Anomaly_Date.astype(int)\n",
    "    # how many times 11 models identified date as an anomaly for 1 fraction\n",
    "        # again nbr of models for each fraction is 11, divide how many models identified the anomaly for specific date\n",
    "        # with the number of total models = 11 \n",
    "    frequencies_df['Weight_per_fraction'] = frequencies_df['Count_Anomaly_Date']/11 # ALTER THIS if nbr models changes\n",
    "    # sort the dataframe by count of models that identified a specific date as anomaly\n",
    "    frequencies_df = frequencies_df.sort_values(by='Count_Anomaly_Date', ascending = False)\n",
    "    return frequencies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **dates_per_fraction function** makes a dataframe to display number of anomaly dates per ALL fraction   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction 0.01 unique dates:  38\n",
      "Outlier fraction 0.02 unique dates:  42\n",
      "Outlier fraction 0.03 unique dates:  48\n",
      "Outlier fraction 0.04 unique dates:  52\n",
      "Outlier fraction 0.05 unique dates:  59\n"
     ]
    }
   ],
   "source": [
    "def dates_per_fraction(l1,l2,l3,l4,l5):\n",
    "    # because we want to know nbr of unique anomalies per dates for all fraction we concat all on axis = 1\n",
    "    d1 = pd.concat([count_weight_outliers(l1),count_weight_outliers(l2),count_weight_outliers(l3),\n",
    "                    count_weight_outliers(l4),count_weight_outliers(l5)],axis=1)\n",
    "    # As all columns now have the same names due to our count_weight_outliers I rename the columns\n",
    "    d1.columns = ['001','Count1','W1','002','Count2','W2','003','Count3','W3','004','Count4','W4','005',\n",
    "                'Count5','W5']\n",
    "    \n",
    "    # to nicely display all anomaly date fractions in 1 output we convert compute value_counts for the date\n",
    "    # these are already unique dates so this returns nbr 1 for each date \n",
    "    d01 = d1['001'].value_counts()\n",
    "    d02 = d1['002'].value_counts()\n",
    "    d03 = d1['003'].value_counts()\n",
    "    d04 = d1['004'].value_counts()\n",
    "    d05 = d1['005'].value_counts()\n",
    "    # concat these value counts now i.e. when one fraction has a values it will display 1 \n",
    "        # when other doesnt it will display NaN\n",
    "    l = pd.concat([d01,d02,d03,d04,d05],axis=1, sort=True)\n",
    "    # to replace NaN with 0 when one fraction doesn't have that anomaly date we groupe it by count\n",
    "        # the other way would be to replace NaN with 0\n",
    "    l = l.groupby(l.index)['001','002','003','004','005'].agg('count')\n",
    "    \n",
    "    #lw = l.merge(d1[['001','W1']],left_on=l.index, right_on=d1['001'])\n",
    "    #lw = l.merge(d1[['002','W2']],left_on=l.index, right_on=d1['002'])\n",
    "    \n",
    "    # here we sum up all the counts to get the total number of anomalies per fraction\n",
    "    print('Outlier fraction 0.01 unique dates: ',l['001'].sum())\n",
    "    print('Outlier fraction 0.02 unique dates: ',l['002'].sum())\n",
    "    print('Outlier fraction 0.03 unique dates: ',l['003'].sum())\n",
    "    print('Outlier fraction 0.04 unique dates: ',l['004'].sum())\n",
    "    print('Outlier fraction 0.05 unique dates: ',l['005'].sum())\n",
    "    # but we also want to see the df for all fraction so we return l\n",
    "    return #l\n",
    "\n",
    "dates_per_fraction(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Anomaly Likelihood\n",
    "- Assumption1: if all contamination levels vote for a date the date is more likely to be an anomaly\n",
    "- Assumption2: if more models vote for a date the date is more likely to be an anomaly \n",
    "- So if both assumtions are true we could say date is even more likely to be an anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all fraction anomalous dates found merged \n",
    "def date_ranking(df1, df2, df3, df4, df5):\n",
    "    # concat all the fractions into 1 dataframe on rows (axis = 0)\n",
    "    # here we have duplicate values as some fractions/models identified same dates \n",
    "    concat = pd.concat([count_weight_outliers(df1),count_weight_outliers(df2),\n",
    "            count_weight_outliers(df3),count_weight_outliers(df4), count_weight_outliers(df5)], \n",
    "            axis = 0)\n",
    "    concat.columns = ['Date','ModelAnomaly_Vote','Weight_per_fraction']\n",
    "    # count_weight_outliers function provides unique dates per contamination level\n",
    "        # so if we group/value_counts on date, the date can be counted 5 times max as we have 1%-5% contamination\n",
    "    # here we answer how many times a date was selected in ALL fractions (0.01 - 0.05) \n",
    "    allFractions_dateAnomaly = pd.DataFrame(concat.Date.value_counts()).reset_index()\n",
    "    allFractions_dateAnomaly.columns = ['Date','AllContaminations_Vote']\n",
    " \n",
    "    # how many models voted in all fractions combined \n",
    "    # sum of nbr of models that selected a date as an anomaly (in all contamination levels)\n",
    "    model_vote = pd.DataFrame(concat.groupby(\"Date\")['ModelAnomaly_Vote'].agg('sum')).reset_index()\n",
    "    \n",
    "    # sum the number of models that voted for a date (in all contamination levels)\n",
    "    sum_ModelVotes = concat['ModelAnomaly_Vote'].sum()\n",
    "    \n",
    "    # merge it with anomalyCount_Fractions df so that dates match \n",
    "    merged_inner = pd.merge(left=allFractions_dateAnomaly, right=model_vote, on='Date')\n",
    "    \n",
    "    # model vote weight - if more models voted for a date we could assume its more likely an anomaly\n",
    "    # model votes for 1 date / total model anomaly votes (665) in all contamination levels\n",
    "    merged_inner['weight_anomaly_models'] = merged_inner.ModelAnomaly_Vote/sum_ModelVotes\n",
    "    \n",
    "    # Sort based on highest contamination anomaly count and highest model anomaly vote\n",
    "    merged_inner = merged_inner.sort_values([\"AllContaminations_Vote\", \"ModelAnomaly_Vote\"], ascending = (False, False))\n",
    "    return merged_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Anomaly Dates:  73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AllContaminations_Vote</th>\n",
       "      <th>ModelAnomaly_Vote</th>\n",
       "      <th>weight_anomaly_models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>0.054135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.048120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>0.046617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.045113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>0.043609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>0.036090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.033083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>0.031579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>0.031579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.030075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.024060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.024060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.022556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.022556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.021053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.021053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-11-15</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.018045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018-12-20</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.018045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.016541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-08-08</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.016541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.016541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-11-14</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.016541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.015038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-11-18</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.015038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-03-10</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.015038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-07-24</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.015038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.013534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-11-20</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.012030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.012030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-05-11</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.010526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-11-16</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.010526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-11-17</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.009023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-10-11</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.009023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-10-15</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.009023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.009023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-21</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-22</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-04-15</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-06-10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-11-24</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2018-12-17</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.016541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2018-12-22</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.012030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2018-05-10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2018-12-23</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2018-07-17</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2018-11-13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2018-10-12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.009023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2018-08-27</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2018-03-06</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2018-03-30</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2018-11-09</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2018-08-22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2018-07-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2018-04-03</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2018-08-28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2018-06-22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  AllContaminations_Vote  ModelAnomaly_Vote  \\\n",
       "11  2018-01-16                       5                 36   \n",
       "43  2018-01-17                       5                 32   \n",
       "25  2018-01-06                       5                 31   \n",
       "10  2018-01-01                       5                 30   \n",
       "17  2018-01-05                       5                 29   \n",
       "37  2018-01-07                       5                 24   \n",
       "15  2018-01-08                       5                 22   \n",
       "36  2018-01-02                       5                 21   \n",
       "40  2018-01-03                       5                 21   \n",
       "35  2018-01-04                       5                 20   \n",
       "3   2018-01-09                       5                 16   \n",
       "21  2018-02-06                       5                 16   \n",
       "13  2018-01-14                       5                 15   \n",
       "23  2018-01-11                       5                 15   \n",
       "34  2018-02-05                       5                 14   \n",
       "41  2018-01-10                       5                 14   \n",
       "18  2018-11-15                       5                 12   \n",
       "30  2018-12-20                       5                 12   \n",
       "6   2018-01-12                       5                 11   \n",
       "12  2018-08-08                       5                 11   \n",
       "19  2018-02-02                       5                 11   \n",
       "24  2018-11-14                       5                 11   \n",
       "0   2018-01-13                       5                 10   \n",
       "26  2018-11-18                       5                 10   \n",
       "28  2018-03-10                       5                 10   \n",
       "29  2018-07-24                       5                 10   \n",
       "33  2018-12-24                       5                  9   \n",
       "7   2018-11-20                       5                  8   \n",
       "42  2018-01-15                       5                  8   \n",
       "4   2018-05-11                       5                  7   \n",
       "27  2018-11-16                       5                  7   \n",
       "9   2018-11-17                       5                  6   \n",
       "20  2018-10-11                       5                  6   \n",
       "22  2018-10-15                       5                  6   \n",
       "39  2018-01-20                       5                  6   \n",
       "1   2018-11-21                       5                  5   \n",
       "2   2018-11-22                       5                  5   \n",
       "5   2018-04-15                       5                  5   \n",
       "8   2018-06-12                       5                  5   \n",
       "14  2018-11-19                       5                  5   \n",
       "16  2018-06-13                       5                  5   \n",
       "31  2018-06-10                       5                  5   \n",
       "32  2018-11-24                       5                  5   \n",
       "38  2018-12-17                       5                  5   \n",
       "48  2018-01-18                       4                 11   \n",
       "46  2018-12-22                       4                  8   \n",
       "44  2018-01-21                       4                  4   \n",
       "45  2018-04-25                       4                  4   \n",
       "47  2018-05-10                       4                  4   \n",
       "49  2018-02-01                       4                  4   \n",
       "50  2018-09-21                       4                  4   \n",
       "51  2018-12-23                       3                  5   \n",
       "52  2018-07-17                       3                  5   \n",
       "55  2018-09-19                       3                  4   \n",
       "53  2018-11-13                       3                  3   \n",
       "54  2018-01-22                       3                  3   \n",
       "56  2018-10-12                       3                  3   \n",
       "59  2018-02-04                       2                  6   \n",
       "60  2018-07-18                       2                  4   \n",
       "64  2018-08-27                       2                  3   \n",
       "57  2018-09-20                       2                  2   \n",
       "58  2018-01-19                       2                  2   \n",
       "61  2018-03-06                       2                  2   \n",
       "62  2018-03-30                       2                  2   \n",
       "63  2018-11-09                       2                  2   \n",
       "65  2018-08-22                       1                  1   \n",
       "66  2018-01-28                       1                  1   \n",
       "67  2018-01-23                       1                  1   \n",
       "68  2018-01-30                       1                  1   \n",
       "69  2018-07-02                       1                  1   \n",
       "70  2018-04-03                       1                  1   \n",
       "71  2018-08-28                       1                  1   \n",
       "72  2018-06-22                       1                  1   \n",
       "\n",
       "    weight_anomaly_models  \n",
       "11               0.054135  \n",
       "43               0.048120  \n",
       "25               0.046617  \n",
       "10               0.045113  \n",
       "17               0.043609  \n",
       "37               0.036090  \n",
       "15               0.033083  \n",
       "36               0.031579  \n",
       "40               0.031579  \n",
       "35               0.030075  \n",
       "3                0.024060  \n",
       "21               0.024060  \n",
       "13               0.022556  \n",
       "23               0.022556  \n",
       "34               0.021053  \n",
       "41               0.021053  \n",
       "18               0.018045  \n",
       "30               0.018045  \n",
       "6                0.016541  \n",
       "12               0.016541  \n",
       "19               0.016541  \n",
       "24               0.016541  \n",
       "0                0.015038  \n",
       "26               0.015038  \n",
       "28               0.015038  \n",
       "29               0.015038  \n",
       "33               0.013534  \n",
       "7                0.012030  \n",
       "42               0.012030  \n",
       "4                0.010526  \n",
       "27               0.010526  \n",
       "9                0.009023  \n",
       "20               0.009023  \n",
       "22               0.009023  \n",
       "39               0.009023  \n",
       "1                0.007519  \n",
       "2                0.007519  \n",
       "5                0.007519  \n",
       "8                0.007519  \n",
       "14               0.007519  \n",
       "16               0.007519  \n",
       "31               0.007519  \n",
       "32               0.007519  \n",
       "38               0.007519  \n",
       "48               0.016541  \n",
       "46               0.012030  \n",
       "44               0.006015  \n",
       "45               0.006015  \n",
       "47               0.006015  \n",
       "49               0.006015  \n",
       "50               0.006015  \n",
       "51               0.007519  \n",
       "52               0.007519  \n",
       "55               0.006015  \n",
       "53               0.004511  \n",
       "54               0.004511  \n",
       "56               0.004511  \n",
       "59               0.009023  \n",
       "60               0.006015  \n",
       "64               0.004511  \n",
       "57               0.003008  \n",
       "58               0.003008  \n",
       "61               0.003008  \n",
       "62               0.003008  \n",
       "63               0.003008  \n",
       "65               0.001504  \n",
       "66               0.001504  \n",
       "67               0.001504  \n",
       "68               0.001504  \n",
       "69               0.001504  \n",
       "70               0.001504  \n",
       "71               0.001504  \n",
       "72               0.001504  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_anomalies = date_ranking(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)\n",
    "print('Unique Anomaly Dates: ',len(price_anomalies.Date.unique()))\n",
    "price_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Borderline probability analysis\n",
    "- e.g. the probability was 51% that date is an anomaly --> whats the realtionships\n",
    "    - I think it's similar to predict() where I get labels not ratios (based on threshold it labels?)\n",
    "    - predict_proba(): Predict the probability of a sample being outlier using the fitted detector.\n",
    "    - simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. \n",
    "    - Description of labeling formula vs probabilty:\n",
    "    https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#BaseDetector.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.26869892e-07, 9.99999773e-01],\n",
       "       [1.22951083e-04, 9.99877049e-01],\n",
       "       [3.55243856e-04, 9.99644756e-01],\n",
       "       [4.31716397e-04, 9.99568284e-01],\n",
       "       [3.90444856e-05, 9.99960956e-01],\n",
       "       [1.49117160e-05, 9.99985088e-01],\n",
       "       [8.06483994e-05, 9.99919352e-01],\n",
       "       [3.57676071e-05, 9.99964232e-01],\n",
       "       [2.57806667e-05, 9.99974219e-01],\n",
       "       [1.64204572e-05, 9.99983580e-01]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=100)\n",
    "prices_proba[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Blockhain Dataset \n",
    "- **Note**\n",
    "    - no feature engineering added to this dataset\n",
    "    - as we defined functions above here we only need to call them on a new dataset \n",
    "    - exeption is model code so we can adjust the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>active_addresses</th>\n",
       "      <th>average_transaction_value</th>\n",
       "      <th>block_height</th>\n",
       "      <th>block_size</th>\n",
       "      <th>block_time</th>\n",
       "      <th>current_supply</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>hashrate</th>\n",
       "      <th>large_transaction_count</th>\n",
       "      <th>new_addresses</th>\n",
       "      <th>transaction_count</th>\n",
       "      <th>transaction_count_all_time</th>\n",
       "      <th>unique_addresses_all_time</th>\n",
       "      <th>zero_balance_addresses_all_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.312659</td>\n",
       "      <td>0.224696</td>\n",
       "      <td>0.513188</td>\n",
       "      <td>0.577511</td>\n",
       "      <td>0.325309</td>\n",
       "      <td>0.513188</td>\n",
       "      <td>0.550890</td>\n",
       "      <td>0.476739</td>\n",
       "      <td>0.170867</td>\n",
       "      <td>0.224342</td>\n",
       "      <td>0.303136</td>\n",
       "      <td>0.482607</td>\n",
       "      <td>0.507523</td>\n",
       "      <td>0.518616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.171182</td>\n",
       "      <td>0.136495</td>\n",
       "      <td>0.290541</td>\n",
       "      <td>0.223873</td>\n",
       "      <td>0.162468</td>\n",
       "      <td>0.290539</td>\n",
       "      <td>0.301799</td>\n",
       "      <td>0.243572</td>\n",
       "      <td>0.152479</td>\n",
       "      <td>0.133257</td>\n",
       "      <td>0.152195</td>\n",
       "      <td>0.275745</td>\n",
       "      <td>0.276457</td>\n",
       "      <td>0.275090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.212106</td>\n",
       "      <td>0.132698</td>\n",
       "      <td>0.262062</td>\n",
       "      <td>0.417905</td>\n",
       "      <td>0.210205</td>\n",
       "      <td>0.262065</td>\n",
       "      <td>0.287124</td>\n",
       "      <td>0.268642</td>\n",
       "      <td>0.098229</td>\n",
       "      <td>0.151384</td>\n",
       "      <td>0.198545</td>\n",
       "      <td>0.246273</td>\n",
       "      <td>0.271502</td>\n",
       "      <td>0.300481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.279694</td>\n",
       "      <td>0.196649</td>\n",
       "      <td>0.517795</td>\n",
       "      <td>0.610013</td>\n",
       "      <td>0.302476</td>\n",
       "      <td>0.517790</td>\n",
       "      <td>0.575492</td>\n",
       "      <td>0.476230</td>\n",
       "      <td>0.134256</td>\n",
       "      <td>0.212049</td>\n",
       "      <td>0.281559</td>\n",
       "      <td>0.464341</td>\n",
       "      <td>0.496377</td>\n",
       "      <td>0.509794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.361409</td>\n",
       "      <td>0.287395</td>\n",
       "      <td>0.770601</td>\n",
       "      <td>0.771129</td>\n",
       "      <td>0.417515</td>\n",
       "      <td>0.770599</td>\n",
       "      <td>0.855096</td>\n",
       "      <td>0.684369</td>\n",
       "      <td>0.189775</td>\n",
       "      <td>0.265077</td>\n",
       "      <td>0.394768</td>\n",
       "      <td>0.709336</td>\n",
       "      <td>0.739602</td>\n",
       "      <td>0.748531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       active_addresses  average_transaction_value  block_height  block_size  \\\n",
       "count        365.000000                 365.000000    365.000000  365.000000   \n",
       "mean           0.312659                   0.224696      0.513188    0.577511   \n",
       "std            0.171182                   0.136495      0.290541    0.223873   \n",
       "min            0.000000                   0.000000      0.000000    0.000000   \n",
       "25%            0.212106                   0.132698      0.262062    0.417905   \n",
       "50%            0.279694                   0.196649      0.517795    0.610013   \n",
       "75%            0.361409                   0.287395      0.770601    0.771129   \n",
       "max            1.000000                   1.000000      1.000000    1.000000   \n",
       "\n",
       "       block_time  current_supply  difficulty    hashrate  \\\n",
       "count  365.000000      365.000000  365.000000  365.000000   \n",
       "mean     0.325309        0.513188    0.550890    0.476739   \n",
       "std      0.162468        0.290539    0.301799    0.243572   \n",
       "min      0.000000        0.000000    0.000000    0.000000   \n",
       "25%      0.210205        0.262065    0.287124    0.268642   \n",
       "50%      0.302476        0.517790    0.575492    0.476230   \n",
       "75%      0.417515        0.770599    0.855096    0.684369   \n",
       "max      1.000000        1.000000    1.000000    1.000000   \n",
       "\n",
       "       large_transaction_count  new_addresses  transaction_count  \\\n",
       "count               365.000000     365.000000         365.000000   \n",
       "mean                  0.170867       0.224342           0.303136   \n",
       "std                   0.152479       0.133257           0.152195   \n",
       "min                   0.000000       0.000000           0.000000   \n",
       "25%                   0.098229       0.151384           0.198545   \n",
       "50%                   0.134256       0.212049           0.281559   \n",
       "75%                   0.189775       0.265077           0.394768   \n",
       "max                   1.000000       1.000000           1.000000   \n",
       "\n",
       "       transaction_count_all_time  unique_addresses_all_time  \\\n",
       "count                  365.000000                 365.000000   \n",
       "mean                     0.482607                   0.507523   \n",
       "std                      0.275745                   0.276457   \n",
       "min                      0.000000                   0.000000   \n",
       "25%                      0.246273                   0.271502   \n",
       "50%                      0.464341                   0.496377   \n",
       "75%                      0.709336                   0.739602   \n",
       "max                      1.000000                   1.000000   \n",
       "\n",
       "       zero_balance_addresses_all_time  \n",
       "count                       365.000000  \n",
       "mean                          0.518616  \n",
       "std                           0.275090  \n",
       "min                           0.000000  \n",
       "25%                           0.300481  \n",
       "50%                           0.509794  \n",
       "75%                           0.748531  \n",
       "max                           1.000000  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = pd.read_csv('blockchain18.csv', index_col = 0)\n",
    "X = scale(block)\n",
    "print(X.shape)\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Run Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adjusted neurons for VAE (not enogh features for default setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  6 INLIERS :  359 ABOD\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 FB\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 HBOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 IF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  1 INLIERS :  364 Average KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 OCSVM\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 SOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 LOF\n",
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 14)           210         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 56)           840         dense_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 56)           0           dense_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 28)           1596        dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 28)           0           dense_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 14)           406         dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 14)           0           dense_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 2)            30          dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_61 (Dense)                (None, 2)            30          dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 2)            0           dense_60[0][0]                   \n",
      "                                                                 dense_61[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,112\n",
      "Trainable params: 3,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 14)                42        \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 28)                420       \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 56)                1624      \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 14)                798       \n",
      "=================================================================\n",
      "Total params: 2,890\n",
      "Trainable params: 2,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_17 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_17.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "model_16 (Model)             [(None, 2), (None, 2), (N 3112      \n",
      "_________________________________________________________________\n",
      "model_17 (Model)             (None, 14)                2890      \n",
      "=================================================================\n",
      "Total params: 6,002\n",
      "Trainable params: 6,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 40.3974 - val_loss: 26.6272\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 31.0483 - val_loss: 22.5096\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 98us/step - loss: 25.9433 - val_loss: 20.1392\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 98us/step - loss: 22.9851 - val_loss: 18.5904\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 21.0763 - val_loss: 17.4334\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 98us/step - loss: 19.3359 - val_loss: 16.4390\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 18.0911 - val_loss: 15.3092\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 16.8412 - val_loss: 14.6946\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 100us/step - loss: 16.1111 - val_loss: 14.2125\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 94us/step - loss: 15.4727 - val_loss: 13.8208\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 15.2404 - val_loss: 13.8524\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 14.9930 - val_loss: 13.6443\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.8829 - val_loss: 13.6684\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.7688 - val_loss: 13.5642\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.6336 - val_loss: 13.5332\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.5456 - val_loss: 13.4550\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.4813 - val_loss: 13.4362\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.3816 - val_loss: 13.4123\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.3396 - val_loss: 13.3701\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.3159 - val_loss: 13.3432\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.3011 - val_loss: 13.3531\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.2879 - val_loss: 13.2545\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.2759 - val_loss: 13.3371\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.2524 - val_loss: 13.2834\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.2099 - val_loss: 13.2779\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.2120 - val_loss: 13.2866\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1893 - val_loss: 13.2890\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.1985 - val_loss: 13.2635\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1838 - val_loss: 13.2681\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.1706 - val_loss: 13.2600\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1705 - val_loss: 13.2893\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1433 - val_loss: 13.2626\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1682 - val_loss: 13.2588\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1346 - val_loss: 13.2537\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1476 - val_loss: 13.2545\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1293 - val_loss: 13.2523\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1227 - val_loss: 13.2361\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.1290 - val_loss: 13.2385\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1243 - val_loss: 13.2486\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1284 - val_loss: 13.2564\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1205 - val_loss: 13.2494\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.1198 - val_loss: 13.2374\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1284 - val_loss: 13.2414\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1154 - val_loss: 13.2436\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1127 - val_loss: 13.2475\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1110 - val_loss: 13.2601\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1109 - val_loss: 13.2324\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.1055 - val_loss: 13.2375\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1155 - val_loss: 13.2402\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1059 - val_loss: 13.2385\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1062 - val_loss: 13.2423\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.1196 - val_loss: 13.2334\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1040 - val_loss: 13.2427\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1052 - val_loss: 13.2350\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1073 - val_loss: 13.2363\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1043 - val_loss: 13.2348\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.1033 - val_loss: 13.2368\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1051 - val_loss: 13.2374\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1008 - val_loss: 13.2340\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0953 - val_loss: 13.2317\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0922 - val_loss: 13.2394\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0948 - val_loss: 13.2377\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1011 - val_loss: 13.2315\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1045 - val_loss: 13.2381\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0970 - val_loss: 13.2297\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0962 - val_loss: 13.2379\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0950 - val_loss: 13.2365\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0936 - val_loss: 13.2347\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0941 - val_loss: 13.2343\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0892 - val_loss: 13.2313\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0862 - val_loss: 13.2358\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0944 - val_loss: 13.2359\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0928 - val_loss: 13.2372\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0959 - val_loss: 13.2293\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0851 - val_loss: 13.2390\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0921 - val_loss: 13.2334\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0974 - val_loss: 13.2296\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0940 - val_loss: 13.2376\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0933 - val_loss: 13.2339\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0920 - val_loss: 13.2368\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0959 - val_loss: 13.2347\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0927 - val_loss: 13.2320\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0914 - val_loss: 13.2318\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0917 - val_loss: 13.2351\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0899 - val_loss: 13.2336\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0877 - val_loss: 13.2367\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0899 - val_loss: 13.2296\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0876 - val_loss: 13.2349\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0896 - val_loss: 13.2320\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0900 - val_loss: 13.2320\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0940 - val_loss: 13.2317\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0923 - val_loss: 13.2317\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0924 - val_loss: 13.2356\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0869 - val_loss: 13.2348\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0931 - val_loss: 13.2324\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0877 - val_loss: 13.2340\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0881 - val_loss: 13.2337\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0921 - val_loss: 13.2328\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0850 - val_loss: 13.2329\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0902 - val_loss: 13.2337\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 VAE\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 ABOD\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  7 INLIERS :  358 FB\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 HBOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 IF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  7 INLIERS :  358 KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  2 INLIERS :  363 Average KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 OCSVM\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 SOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  6 INLIERS :  359 LOF\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_67 (Dense)                (None, 14)           210         input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_68 (Dense)                (None, 56)           840         dense_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 56)           0           dense_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_69 (Dense)                (None, 28)           1596        dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 28)           0           dense_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_70 (Dense)                (None, 14)           406         dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 14)           0           dense_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_71 (Dense)                (None, 2)            30          dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_72 (Dense)                (None, 2)            30          dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 2)            0           dense_71[0][0]                   \n",
      "                                                                 dense_72[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,112\n",
      "Trainable params: 3,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 14)                42        \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 28)                420       \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 56)                1624      \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 14)                798       \n",
      "=================================================================\n",
      "Total params: 2,890\n",
      "Trainable params: 2,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_20 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_20.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "model_19 (Model)             [(None, 2), (None, 2), (N 3112      \n",
      "_________________________________________________________________\n",
      "model_20 (Model)             (None, 14)                2890      \n",
      "=================================================================\n",
      "Total params: 6,002\n",
      "Trainable params: 6,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 32.0172 - val_loss: 28.3121\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 26.0356 - val_loss: 24.3058\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 22.3637 - val_loss: 21.9316\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 20.5444 - val_loss: 19.8366\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 18.8708 - val_loss: 18.9579\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 17.7799 - val_loss: 17.7887\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 16.8489 - val_loss: 16.6864\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 16.0490 - val_loss: 15.4448\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 15.2906 - val_loss: 15.1032\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 14.9518 - val_loss: 14.9913\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 14.7433 - val_loss: 14.5961\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.5819 - val_loss: 14.5707\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.3834 - val_loss: 14.4202\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.3252 - val_loss: 14.2583\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.2258 - val_loss: 14.2071\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1826 - val_loss: 14.1976\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 14.1693 - val_loss: 14.1578\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1481 - val_loss: 14.1090\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1237 - val_loss: 14.0581\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0743 - val_loss: 14.0888\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0817 - val_loss: 14.0359\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1015 - val_loss: 13.9926\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0508 - val_loss: 14.0213\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0726 - val_loss: 14.0448\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0643 - val_loss: 14.0661\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0742 - val_loss: 14.0788\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0445 - val_loss: 13.9882\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0469 - val_loss: 13.9655\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0458 - val_loss: 14.0280\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0298 - val_loss: 14.0139\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.0509 - val_loss: 14.0328\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0684 - val_loss: 13.9876\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0401 - val_loss: 13.9875\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0349 - val_loss: 13.9818\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0208 - val_loss: 13.9790\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0537 - val_loss: 13.9544\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0386 - val_loss: 13.9985\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0176 - val_loss: 13.9666\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0234 - val_loss: 13.9430\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0396 - val_loss: 13.9694\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0315 - val_loss: 13.9432\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0149 - val_loss: 13.9615\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0155 - val_loss: 13.9688\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0212 - val_loss: 13.9771\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0019 - val_loss: 13.9444\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0128 - val_loss: 13.9639\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0143 - val_loss: 13.9481\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0176 - val_loss: 13.9570\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0141 - val_loss: 13.9752\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0366 - val_loss: 13.9638\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0229 - val_loss: 13.9678\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0322 - val_loss: 13.9538\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0181 - val_loss: 13.9528\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0116 - val_loss: 13.9641\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0117 - val_loss: 13.9759\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0167 - val_loss: 13.9650\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0297 - val_loss: 13.9487\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0127 - val_loss: 13.9599\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0177 - val_loss: 13.9522\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0223 - val_loss: 13.9600\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0186 - val_loss: 13.9587\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0122 - val_loss: 13.9568\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0113 - val_loss: 13.9535\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0097 - val_loss: 13.9500\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0136 - val_loss: 13.9615\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0116 - val_loss: 13.9423\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0132 - val_loss: 13.9446\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0136 - val_loss: 13.9431\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0109 - val_loss: 13.9425\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0196 - val_loss: 13.9464\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0123 - val_loss: 13.9475\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0134 - val_loss: 13.9490\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0107 - val_loss: 13.9509\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0211 - val_loss: 13.9540\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0064 - val_loss: 13.9503\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0159 - val_loss: 13.9465\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0105 - val_loss: 13.9545\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0088 - val_loss: 13.9523\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0126 - val_loss: 13.9501\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.0148 - val_loss: 13.9445\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0033 - val_loss: 13.9502\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0069 - val_loss: 13.9488\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0069 - val_loss: 13.9520\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0040 - val_loss: 13.9429\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0018 - val_loss: 13.9506\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0084 - val_loss: 13.9466\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9956 - val_loss: 13.9503\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9909 - val_loss: 13.9478\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.0148 - val_loss: 13.9512\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0105 - val_loss: 13.9420\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0058 - val_loss: 13.9476\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0080 - val_loss: 13.9431\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0010 - val_loss: 13.9482\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0055 - val_loss: 13.9383\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0131 - val_loss: 13.9455\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0118 - val_loss: 13.9466\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0092 - val_loss: 13.9519\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0116 - val_loss: 13.9420\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0087 - val_loss: 13.9396\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0063 - val_loss: 13.9466\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 VAE\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  15 INLIERS :  350 ABOD\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 FB\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 HBOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 IF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  8 INLIERS :  357 KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  2 INLIERS :  363 Average KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 OCSVM\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 SOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 LOF\n",
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_78 (Dense)                (None, 14)           210         input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_79 (Dense)                (None, 56)           840         dense_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 56)           0           dense_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_80 (Dense)                (None, 28)           1596        dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 28)           0           dense_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_81 (Dense)                (None, 14)           406         dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 14)           0           dense_81[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_82 (Dense)                (None, 2)            30          dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_83 (Dense)                (None, 2)            30          dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 2)            0           dense_82[0][0]                   \n",
      "                                                                 dense_83[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,112\n",
      "Trainable params: 3,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 14)                42        \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 28)                420       \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 56)                1624      \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 14)                798       \n",
      "=================================================================\n",
      "Total params: 2,890\n",
      "Trainable params: 2,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_23 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_23.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "model_22 (Model)             [(None, 2), (None, 2), (N 3112      \n",
      "_________________________________________________________________\n",
      "model_23 (Model)             (None, 14)                2890      \n",
      "=================================================================\n",
      "Total params: 6,002\n",
      "Trainable params: 6,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 53.5444 - val_loss: 47.3427\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 39.9502 - val_loss: 38.2187\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 100us/step - loss: 32.1495 - val_loss: 32.3934\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 110us/step - loss: 26.6569 - val_loss: 27.7682\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 23.2397 - val_loss: 24.5720\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 98us/step - loss: 20.7393 - val_loss: 22.2440\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 114us/step - loss: 18.7001 - val_loss: 20.3494\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 100us/step - loss: 17.1535 - val_loss: 19.0329\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 16.2181 - val_loss: 18.2099\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 15.6211 - val_loss: 18.0136\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 15.2181 - val_loss: 17.5849\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.8280 - val_loss: 17.2334\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.7067 - val_loss: 16.9732\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.4921 - val_loss: 16.9014\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 14.2918 - val_loss: 16.6825\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.2072 - val_loss: 16.6473\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1624 - val_loss: 16.4651\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.1680 - val_loss: 16.4265\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.0944 - val_loss: 16.3940\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.0113 - val_loss: 16.3054\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.9597 - val_loss: 16.2918\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9452 - val_loss: 16.2346\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 13.9396 - val_loss: 16.2182\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.8941 - val_loss: 16.1713\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9156 - val_loss: 16.1824\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8770 - val_loss: 16.1431\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.8369 - val_loss: 16.1394\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8560 - val_loss: 16.1048\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8492 - val_loss: 16.1053\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 13.8431 - val_loss: 16.0901\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8353 - val_loss: 16.0783\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.8526 - val_loss: 16.0309\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 13.8307 - val_loss: 16.0932\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.8185 - val_loss: 16.0793\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8261 - val_loss: 16.0660\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.7984 - val_loss: 16.0395\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8064 - val_loss: 16.0256\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.7899 - val_loss: 16.0274\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.8111 - val_loss: 16.0336\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.7902 - val_loss: 16.0146\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8101 - val_loss: 16.0198\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.7849 - val_loss: 16.0008\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8014 - val_loss: 15.9909\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.7983 - val_loss: 16.0412\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.7883 - val_loss: 15.9941\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8018 - val_loss: 16.0313\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8001 - val_loss: 16.0143\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.7786 - val_loss: 15.9765\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8054 - val_loss: 16.0009\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8219 - val_loss: 15.9754\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.7876 - val_loss: 16.0079\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7987 - val_loss: 16.0146\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8176 - val_loss: 15.9967\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8161 - val_loss: 15.9756\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7874 - val_loss: 15.9858\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8021 - val_loss: 15.9829\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8131 - val_loss: 15.9687\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7937 - val_loss: 15.9912\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7960 - val_loss: 15.9796\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7859 - val_loss: 15.9636\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.7902 - val_loss: 15.9741\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.7853 - val_loss: 15.9702\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7872 - val_loss: 15.9707\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7849 - val_loss: 15.9731\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7991 - val_loss: 15.9736\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7902 - val_loss: 15.9599\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.7936 - val_loss: 15.9646\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7987 - val_loss: 15.9781\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7863 - val_loss: 15.9698\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7822 - val_loss: 15.9600\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7876 - val_loss: 15.9701\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7902 - val_loss: 15.9625\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7859 - val_loss: 15.9629\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8036 - val_loss: 15.9638\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.7837 - val_loss: 15.9565\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7984 - val_loss: 15.9607\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.7834 - val_loss: 15.9646\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7927 - val_loss: 15.9583\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7941 - val_loss: 15.9677\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.7840 - val_loss: 15.9538\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7881 - val_loss: 15.9497\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7783 - val_loss: 15.9586\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7989 - val_loss: 15.9434\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.7869 - val_loss: 15.9503\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7933 - val_loss: 15.9435\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7845 - val_loss: 15.9597\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7942 - val_loss: 15.9587\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7899 - val_loss: 15.9468\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7897 - val_loss: 15.9586\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7819 - val_loss: 15.9499\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7827 - val_loss: 15.9489\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7955 - val_loss: 15.9493\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7958 - val_loss: 15.9526\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7906 - val_loss: 15.9487\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7949 - val_loss: 15.9569\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7917 - val_loss: 15.9530\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7853 - val_loss: 15.9450\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.7927 - val_loss: 15.9464\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7947 - val_loss: 15.9464\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7831 - val_loss: 15.9479\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 VAE\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  18 INLIERS :  347 ABOD\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  14 INLIERS :  351 FB\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 HBOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 IF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  11 INLIERS :  354 KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  2 INLIERS :  363 Average KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 OCSVM\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 SOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  14 INLIERS :  351 LOF\n",
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_89 (Dense)                (None, 14)           210         input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_90 (Dense)                (None, 56)           840         dense_89[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 56)           0           dense_90[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_91 (Dense)                (None, 28)           1596        dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 28)           0           dense_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_92 (Dense)                (None, 14)           406         dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 14)           0           dense_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_93 (Dense)                (None, 2)            30          dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_94 (Dense)                (None, 2)            30          dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 2)            0           dense_93[0][0]                   \n",
      "                                                                 dense_94[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,112\n",
      "Trainable params: 3,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 14)                42        \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 28)                420       \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 56)                1624      \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 14)                798       \n",
      "=================================================================\n",
      "Total params: 2,890\n",
      "Trainable params: 2,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_26 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_26.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "model_25 (Model)             [(None, 2), (None, 2), (N 3112      \n",
      "_________________________________________________________________\n",
      "model_26 (Model)             (None, 14)                2890      \n",
      "=================================================================\n",
      "Total params: 6,002\n",
      "Trainable params: 6,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 38.5952 - val_loss: 30.5038\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 29.2928 - val_loss: 25.7112\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 24.8438 - val_loss: 22.8390\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 22.1102 - val_loss: 21.0136\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 100us/step - loss: 20.3165 - val_loss: 19.4516\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 18.8771 - val_loss: 18.1331\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 17.5724 - val_loss: 16.8515\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 16.6330 - val_loss: 16.2881\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 100us/step - loss: 15.9769 - val_loss: 15.3996\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 15.4260 - val_loss: 15.0169\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 15.1732 - val_loss: 15.1016\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.8513 - val_loss: 14.8092\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.6913 - val_loss: 14.7648\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.6079 - val_loss: 14.5002\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.4878 - val_loss: 14.3198\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.4732 - val_loss: 14.3823\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.3567 - val_loss: 14.3314\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.3249 - val_loss: 14.2785\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.2465 - val_loss: 14.3987\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.1899 - val_loss: 14.1597\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.2166 - val_loss: 14.2764\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1671 - val_loss: 14.2226\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1320 - val_loss: 14.2030\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1473 - val_loss: 14.1994\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0915 - val_loss: 14.1694\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0945 - val_loss: 14.1251\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0794 - val_loss: 14.1367\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0935 - val_loss: 14.1091\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0738 - val_loss: 14.1013\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0778 - val_loss: 14.1131\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0578 - val_loss: 14.0912\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0474 - val_loss: 14.1319\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0329 - val_loss: 14.1317\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.0348 - val_loss: 14.1009\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.0361 - val_loss: 14.1068\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0265 - val_loss: 14.0763\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0051 - val_loss: 14.1185\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0380 - val_loss: 14.0459\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0319 - val_loss: 14.0738\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0128 - val_loss: 14.0756\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0254 - val_loss: 14.0684\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0261 - val_loss: 14.0593\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0106 - val_loss: 14.0948\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0253 - val_loss: 14.0657\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0105 - val_loss: 14.0646\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0108 - val_loss: 14.0545\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9960 - val_loss: 14.0549\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0257 - val_loss: 14.0648\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0089 - val_loss: 14.0816\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0073 - val_loss: 14.0805\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0005 - val_loss: 14.0578\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0130 - val_loss: 14.0759\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0008 - val_loss: 14.0700\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9958 - val_loss: 14.0671\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0068 - val_loss: 14.0719\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0024 - val_loss: 14.0735\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0064 - val_loss: 14.0638\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9997 - val_loss: 14.0742\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0131 - val_loss: 14.0579\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0033 - val_loss: 14.0574\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9902 - val_loss: 14.0638\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0026 - val_loss: 14.0796\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9926 - val_loss: 14.0524\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0136 - val_loss: 14.0562\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9973 - val_loss: 14.0612\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9974 - val_loss: 14.0592\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9992 - val_loss: 14.0578\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9927 - val_loss: 14.0558\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9927 - val_loss: 14.0479\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0080 - val_loss: 14.0529\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9942 - val_loss: 14.0472\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0018 - val_loss: 14.0612\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0110 - val_loss: 14.0483\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9962 - val_loss: 14.0591\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0047 - val_loss: 14.0559\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9963 - val_loss: 14.0582\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9911 - val_loss: 14.0491\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0039 - val_loss: 14.0522\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0089 - val_loss: 14.0655\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0009 - val_loss: 14.0541\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9966 - val_loss: 14.0475\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9972 - val_loss: 14.0628\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9969 - val_loss: 14.0597\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0047 - val_loss: 14.0539\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9977 - val_loss: 14.0524\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9891 - val_loss: 14.0492\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9942 - val_loss: 14.0518\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9909 - val_loss: 14.0558\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9990 - val_loss: 14.0524\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9932 - val_loss: 14.0544\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9965 - val_loss: 14.0524\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9988 - val_loss: 14.0564\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.9950 - val_loss: 14.0653\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9984 - val_loss: 14.0551\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9913 - val_loss: 14.0517\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9952 - val_loss: 14.0539\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9978 - val_loss: 14.0590\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9981 - val_loss: 14.0564\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9888 - val_loss: 14.0515\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9962 - val_loss: 14.0553\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 VAE\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  24 INLIERS :  341 ABOD\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  16 INLIERS :  349 FB\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 HBOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 IF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  15 INLIERS :  350 KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  4 INLIERS :  361 Average KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 OCSVM\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 SOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  17 INLIERS :  348 LOF\n",
      "Model: \"model_28\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_100 (Dense)               (None, 14)           210         input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 56)           840         dense_100[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 56)           0           dense_101[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_102 (Dense)               (None, 28)           1596        dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 28)           0           dense_102[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_103 (Dense)               (None, 14)           406         dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 14)           0           dense_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_104 (Dense)               (None, 2)            30          dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_105 (Dense)               (None, 2)            30          dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 2)            0           dense_104[0][0]                  \n",
      "                                                                 dense_105[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,112\n",
      "Trainable params: 3,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 14)                42        \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 28)                420       \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 56)                1624      \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 14)                798       \n",
      "=================================================================\n",
      "Total params: 2,890\n",
      "Trainable params: 2,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_29 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_29.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "model_28 (Model)             [(None, 2), (None, 2), (N 3112      \n",
      "_________________________________________________________________\n",
      "model_29 (Model)             (None, 14)                2890      \n",
      "=================================================================\n",
      "Total params: 6,002\n",
      "Trainable params: 6,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 40.2016 - val_loss: 28.7130\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 29.7806 - val_loss: 24.2665\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 24.8499 - val_loss: 21.6128\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 21.9079 - val_loss: 19.7674\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 19.8607 - val_loss: 18.3967\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 18.2304 - val_loss: 17.0516\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 16.9427 - val_loss: 16.4219\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 15.7305 - val_loss: 15.8460\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 100us/step - loss: 15.1833 - val_loss: 15.1736\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 14.8874 - val_loss: 15.1224\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 14.5655 - val_loss: 15.1064\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.3993 - val_loss: 14.8495\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.3552 - val_loss: 14.6568\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.2235 - val_loss: 14.7575\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.2024 - val_loss: 14.5635\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1666 - val_loss: 14.5982\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1245 - val_loss: 14.4352\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0994 - val_loss: 14.4264\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0649 - val_loss: 14.4567\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0413 - val_loss: 14.5336\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.0493 - val_loss: 14.4468\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0415 - val_loss: 14.4984\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0147 - val_loss: 14.4823\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.0234 - val_loss: 14.4487\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0138 - val_loss: 14.4402\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9821 - val_loss: 14.4390\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9853 - val_loss: 14.4415\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.9960 - val_loss: 14.4197\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9725 - val_loss: 14.4279\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9578 - val_loss: 14.3936\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9752 - val_loss: 14.4374\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0009 - val_loss: 14.4283\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9703 - val_loss: 14.4626\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9655 - val_loss: 14.4355\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9873 - val_loss: 14.4354\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9828 - val_loss: 14.4416\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9833 - val_loss: 14.4198\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9594 - val_loss: 14.4333\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9621 - val_loss: 14.4027\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9730 - val_loss: 14.4279\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9881 - val_loss: 14.4326\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.9653 - val_loss: 14.4194\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9820 - val_loss: 14.4217\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9672 - val_loss: 14.4183\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9550 - val_loss: 14.4239\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9682 - val_loss: 14.4517\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9553 - val_loss: 14.4275\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9744 - val_loss: 14.4236\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9635 - val_loss: 14.4292\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9629 - val_loss: 14.4195\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9529 - val_loss: 14.4185\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9605 - val_loss: 14.4304\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9656 - val_loss: 14.4313\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9580 - val_loss: 14.4153\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9539 - val_loss: 14.4081\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9626 - val_loss: 14.4159\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9595 - val_loss: 14.4099\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9675 - val_loss: 14.4274\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9533 - val_loss: 14.3974\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9727 - val_loss: 14.4140\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9682 - val_loss: 14.4201\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9568 - val_loss: 14.4236\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9520 - val_loss: 14.4053\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9550 - val_loss: 14.4281\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9602 - val_loss: 14.4231\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9552 - val_loss: 14.4064\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9608 - val_loss: 14.4236\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9628 - val_loss: 14.4234\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9620 - val_loss: 14.4118\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9633 - val_loss: 14.4227\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9683 - val_loss: 14.4031\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.9498 - val_loss: 14.4142\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9535 - val_loss: 14.4152\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9577 - val_loss: 14.4093\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9584 - val_loss: 14.4105\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9485 - val_loss: 14.4102\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9637 - val_loss: 14.4186\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9597 - val_loss: 14.4065\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9696 - val_loss: 14.4075\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9541 - val_loss: 14.4085\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9583 - val_loss: 14.4134\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9513 - val_loss: 14.4209\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9638 - val_loss: 14.4142\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9594 - val_loss: 14.4047\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9582 - val_loss: 14.4077\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9544 - val_loss: 14.4083\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9483 - val_loss: 14.4118\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9454 - val_loss: 14.4048\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9514 - val_loss: 14.4083\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9638 - val_loss: 14.4044\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9607 - val_loss: 14.4103\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9587 - val_loss: 14.4147\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9547 - val_loss: 14.4023\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9542 - val_loss: 14.4154\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9562 - val_loss: 14.4114\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9532 - val_loss: 14.4047\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9647 - val_loss: 14.4023\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9604 - val_loss: 14.4041\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9545 - val_loss: 14.4082\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9550 - val_loss: 14.4094\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 VAE\n"
     ]
    }
   ],
   "source": [
    "# time the code\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "# empty list to store outliers and inliners\n",
    "df_outliers = []\n",
    "scores_pred_lst = []\n",
    "proba_lst = []\n",
    "predict_rank_lst = []\n",
    "\n",
    "for fraction in outliers_fraction:\n",
    "    # Define 13 outlier detection tools to be compared\n",
    "    classifiers = {#'AE' : AutoEncoder(contamination=outliers_fraction, random_state=random_state), # check why error\n",
    "               'ABOD': ABOD(contamination = fraction),\n",
    "              'CBLOF':CBLOF(check_estimator=False, random_state=random_state), # getting 0 \n",
    "              'FB':FeatureBagging(LOF(n_neighbors=35),contamination=fraction,check_estimator=False,random_state=random_state),\n",
    "              'HBOS': HBOS(contamination=fraction),\n",
    "              'IF': IForest(contamination=fraction,random_state=random_state),\n",
    "              'KNN': KNN(contamination=fraction),\n",
    "              'Average KNN': KNN(method='mean',contamination=fraction),\n",
    "              'OCSVM' : OCSVM(contamination=fraction),\n",
    "              'SOS' : SOS(contamination=fraction),\n",
    "              'LOF': LOF(n_neighbors=35, contamination = fraction),\n",
    "              #'LOCI': LOCI(contamination = fraction), # takes a super long time + don't understand\n",
    "              'VAE' : VAE(contamination=fraction, random_state=random_state,encoder_neurons=[56, 28, 14],\n",
    "                         decoder_neurons=[14,28,56]) # default nbr of neurons too much for blockchain dataset\n",
    "              }\n",
    "\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        clf.fit(X)\n",
    "        # predict raw anomaly score\n",
    "        scores_pred = clf.decision_function(X) * -1 # WHY? * -1\n",
    "\n",
    "        # prediction of a datapoint category outlier or inlier\n",
    "        y_pred = clf.predict(X)\n",
    "\n",
    "        # probability of outlier \n",
    "        probability = clf.predict_proba(X, method = 'linear')\n",
    "\n",
    "        # predict rank\n",
    "        predict_rank = clf._predict_rank(X)\n",
    "\n",
    "        n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "        n_outliers = np.count_nonzero(y_pred == 1)\n",
    "\n",
    "        # copy of dataframe\n",
    "        dfx = X.copy()\n",
    "\n",
    "        dfx[clf_name] = y_pred.tolist()\n",
    "        #print(clf_name,'\\n',i,clf,'\\n',classifiers,'\\n',classifiers.items())\n",
    "        print('Outlier fraction: ', fraction)\n",
    "        print('OUTLIERS : ',n_outliers,'INLIERS : ',n_inliers, clf_name)\n",
    "        #print('scores_pred: ', scores_pred)\n",
    "        scores_pred_lst.append(scores_pred)\n",
    "        df_outliers.append(dfx[clf_name]) \n",
    "        proba_lst.append(probability)\n",
    "        predict_rank_lst.append(predict_rank)\n",
    "t1 = time.time()\n",
    "total = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run:  36.129899978637695 s\n",
      "Nbr of lists:  55\n"
     ]
    }
   ],
   "source": [
    "print('Time to run: ',total,'s')\n",
    "print('Nbr of lists: ',len(df_outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the outliers lists uniquely for later\n",
    "block_df = df_outliers\n",
    "# probability lists\n",
    "block_proba = proba_lst\n",
    "# rank lists\n",
    "block_rank = predict_rank_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Seperate models per contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_oo1 = df_outliers[0:11]\n",
    "frac_oo2 = df_outliers[11:22]\n",
    "frac_oo3 = df_outliers[22:33]\n",
    "frac_oo4 = df_outliers[33:44]\n",
    "frac_oo5 = df_outliers[44:]\n",
    "# use the function to extract the dates\n",
    "frac_oo1_outliers = outlier_subset_function(frac_oo1)\n",
    "frac_oo2_outliers = outlier_subset_function(frac_oo2)\n",
    "frac_oo3_outliers = outlier_subset_function(frac_oo3)\n",
    "frac_oo4_outliers = outlier_subset_function(frac_oo4)\n",
    "frac_oo5_outliers = outlier_subset_function(frac_oo5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Get Unique Anomaly Dates per Contamination fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction 0.01 unique dates:  38\n",
      "Outlier fraction 0.02 unique dates:  42\n",
      "Outlier fraction 0.03 unique dates:  48\n",
      "Outlier fraction 0.04 unique dates:  52\n",
      "Outlier fraction 0.05 unique dates:  59\n"
     ]
    }
   ],
   "source": [
    "dates_per_fraction(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Anomaly Likelihood\n",
    "- Assumption1: if all contamination levels vote for a date the date is more likely to be an anomaly\n",
    "- Assumption2: if more models vote for a date the date is more likely to be an anomaly\n",
    "- So if both assumtions are true we could say date is even more likely to be an anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Anomaly Dates:  59\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AllContaminations_Vote</th>\n",
       "      <th>ModelAnomaly_Vote</th>\n",
       "      <th>weight_anomaly_models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>0.068768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>5</td>\n",
       "      <td>38</td>\n",
       "      <td>0.054441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>0.051576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>0.051576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>0.044413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>0.040115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>0.038682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>0.038682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>0.037249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>0.037249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>0.032951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.031519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>0.030086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.025788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.025788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-10-14</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.022923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.022923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.022923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-04-24</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.022923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.021490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.018625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.014327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.014327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.014327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-12-23</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.012894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.012894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.011461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.011461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-12-22</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.010029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-12-25</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-04-07</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-07-12</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-11-29</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-05-11</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-11-21</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2018-06-23</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.012894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2018-06-04</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.012894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2018-06-10</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2018-07-13</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2018-10-03</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2018-03-13</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2018-01-27</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2018-06-27</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2018-03-03</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2018-04-15</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2018-04-19</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2018-06-03</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2018-10-12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2018-07-14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2018-04-14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2018-07-22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  AllContaminations_Vote  ModelAnomaly_Vote  \\\n",
       "32  2018-01-04                       5                 48   \n",
       "36  2018-03-17                       5                 38   \n",
       "15  2018-01-05                       5                 36   \n",
       "26  2018-01-03                       5                 36   \n",
       "30  2018-01-06                       5                 31   \n",
       "1   2018-01-09                       5                 28   \n",
       "17  2018-01-28                       5                 27   \n",
       "27  2018-01-07                       5                 27   \n",
       "22  2018-01-11                       5                 26   \n",
       "37  2018-01-02                       5                 26   \n",
       "35  2018-01-10                       5                 23   \n",
       "9   2018-01-31                       5                 22   \n",
       "14  2018-01-08                       5                 21   \n",
       "7   2018-01-12                       5                 18   \n",
       "8   2018-01-16                       5                 18   \n",
       "6   2018-10-14                       5                 16   \n",
       "21  2018-02-25                       5                 16   \n",
       "24  2018-01-17                       5                 16   \n",
       "25  2018-04-24                       5                 16   \n",
       "5   2018-01-18                       5                 15   \n",
       "10  2018-01-01                       5                 13   \n",
       "0   2018-11-30                       5                 10   \n",
       "3   2018-12-30                       5                 10   \n",
       "34  2018-01-15                       5                 10   \n",
       "13  2018-12-23                       5                  9   \n",
       "20  2018-01-13                       5                  9   \n",
       "11  2018-04-25                       5                  8   \n",
       "31  2018-01-19                       5                  8   \n",
       "28  2018-12-22                       5                  7   \n",
       "2   2018-12-27                       5                  5   \n",
       "4   2018-04-08                       5                  5   \n",
       "12  2018-12-25                       5                  5   \n",
       "16  2018-04-07                       5                  5   \n",
       "18  2018-07-12                       5                  5   \n",
       "19  2018-11-29                       5                  5   \n",
       "23  2018-05-11                       5                  5   \n",
       "29  2018-11-21                       5                  5   \n",
       "33  2018-04-01                       5                  5   \n",
       "39  2018-06-23                       4                  9   \n",
       "41  2018-06-04                       4                  9   \n",
       "40  2018-06-10                       4                  5   \n",
       "38  2018-07-13                       4                  4   \n",
       "42  2018-10-03                       3                  5   \n",
       "45  2018-03-13                       3                  5   \n",
       "43  2018-07-03                       3                  3   \n",
       "44  2018-09-21                       3                  3   \n",
       "46  2018-01-27                       3                  3   \n",
       "47  2018-06-27                       3                  3   \n",
       "50  2018-03-03                       2                  3   \n",
       "48  2018-07-18                       2                  2   \n",
       "49  2018-04-15                       2                  2   \n",
       "51  2018-04-19                       2                  2   \n",
       "52  2018-06-03                       1                  1   \n",
       "53  2018-01-21                       1                  1   \n",
       "54  2018-10-12                       1                  1   \n",
       "55  2018-07-14                       1                  1   \n",
       "56  2018-04-14                       1                  1   \n",
       "57  2018-07-22                       1                  1   \n",
       "58  2018-01-20                       1                  1   \n",
       "\n",
       "    weight_anomaly_models  \n",
       "32               0.068768  \n",
       "36               0.054441  \n",
       "15               0.051576  \n",
       "26               0.051576  \n",
       "30               0.044413  \n",
       "1                0.040115  \n",
       "17               0.038682  \n",
       "27               0.038682  \n",
       "22               0.037249  \n",
       "37               0.037249  \n",
       "35               0.032951  \n",
       "9                0.031519  \n",
       "14               0.030086  \n",
       "7                0.025788  \n",
       "8                0.025788  \n",
       "6                0.022923  \n",
       "21               0.022923  \n",
       "24               0.022923  \n",
       "25               0.022923  \n",
       "5                0.021490  \n",
       "10               0.018625  \n",
       "0                0.014327  \n",
       "3                0.014327  \n",
       "34               0.014327  \n",
       "13               0.012894  \n",
       "20               0.012894  \n",
       "11               0.011461  \n",
       "31               0.011461  \n",
       "28               0.010029  \n",
       "2                0.007163  \n",
       "4                0.007163  \n",
       "12               0.007163  \n",
       "16               0.007163  \n",
       "18               0.007163  \n",
       "19               0.007163  \n",
       "23               0.007163  \n",
       "29               0.007163  \n",
       "33               0.007163  \n",
       "39               0.012894  \n",
       "41               0.012894  \n",
       "40               0.007163  \n",
       "38               0.005731  \n",
       "42               0.007163  \n",
       "45               0.007163  \n",
       "43               0.004298  \n",
       "44               0.004298  \n",
       "46               0.004298  \n",
       "47               0.004298  \n",
       "50               0.004298  \n",
       "48               0.002865  \n",
       "49               0.002865  \n",
       "51               0.002865  \n",
       "52               0.001433  \n",
       "53               0.001433  \n",
       "54               0.001433  \n",
       "55               0.001433  \n",
       "56               0.001433  \n",
       "57               0.001433  \n",
       "58               0.001433  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_anomalies = date_ranking(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)\n",
    "print('Unique Anomaly Dates: ',len(block_anomalies.Date.unique()))\n",
    "block_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Borderline probability analysis\n",
    "e.g. the probability was 51% that date is an anomaly --> whats the realtionships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.25234864e-03, 9.96747651e-01],\n",
       "       [4.31609633e-03, 9.95683904e-01],\n",
       "       [1.80495468e-03, 9.98195045e-01],\n",
       "       [9.37913245e-05, 9.99906209e-01],\n",
       "       [2.13447781e-04, 9.99786552e-01],\n",
       "       [1.72016711e-02, 9.82798329e-01],\n",
       "       [4.45059874e-02, 9.55494013e-01],\n",
       "       [7.78463923e-02, 9.22153608e-01],\n",
       "       [7.70073252e-02, 9.22992675e-01],\n",
       "       [4.39499633e-02, 9.56050037e-01]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_proba[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Dataset Test \n",
    "- drop code_repo_contributors as it only has 0 values (**for 2018 BTC Dataset**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>posts</th>\n",
       "      <th>followers</th>\n",
       "      <th>points</th>\n",
       "      <th>overview_page_views</th>\n",
       "      <th>analysis_page_views</th>\n",
       "      <th>markets_page_views</th>\n",
       "      <th>charts_page_views</th>\n",
       "      <th>trades_page_views</th>\n",
       "      <th>forum_page_views</th>\n",
       "      <th>...</th>\n",
       "      <th>reddit_posts_per_day</th>\n",
       "      <th>reddit_comments_per_hour</th>\n",
       "      <th>reddit_comments_per_day</th>\n",
       "      <th>code_repo_stars</th>\n",
       "      <th>code_repo_forks</th>\n",
       "      <th>code_repo_subscribers</th>\n",
       "      <th>code_repo_open_pull_issues</th>\n",
       "      <th>code_repo_closed_pull_issues</th>\n",
       "      <th>code_repo_open_issues</th>\n",
       "      <th>code_repo_closed_issues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.553743</td>\n",
       "      <td>0.599464</td>\n",
       "      <td>0.667312</td>\n",
       "      <td>0.588562</td>\n",
       "      <td>0.658026</td>\n",
       "      <td>0.676191</td>\n",
       "      <td>0.710861</td>\n",
       "      <td>0.688210</td>\n",
       "      <td>0.666003</td>\n",
       "      <td>0.609329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278695</td>\n",
       "      <td>0.196713</td>\n",
       "      <td>0.196708</td>\n",
       "      <td>0.648723</td>\n",
       "      <td>0.680061</td>\n",
       "      <td>0.737169</td>\n",
       "      <td>0.414537</td>\n",
       "      <td>0.500259</td>\n",
       "      <td>0.401739</td>\n",
       "      <td>0.542089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.284817</td>\n",
       "      <td>0.275271</td>\n",
       "      <td>0.214088</td>\n",
       "      <td>0.269895</td>\n",
       "      <td>0.260020</td>\n",
       "      <td>0.261618</td>\n",
       "      <td>0.241494</td>\n",
       "      <td>0.259187</td>\n",
       "      <td>0.268737</td>\n",
       "      <td>0.279112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169271</td>\n",
       "      <td>0.163744</td>\n",
       "      <td>0.163743</td>\n",
       "      <td>0.265907</td>\n",
       "      <td>0.266401</td>\n",
       "      <td>0.255689</td>\n",
       "      <td>0.215412</td>\n",
       "      <td>0.301799</td>\n",
       "      <td>0.244852</td>\n",
       "      <td>0.291751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.332275</td>\n",
       "      <td>0.409717</td>\n",
       "      <td>0.566065</td>\n",
       "      <td>0.397166</td>\n",
       "      <td>0.501349</td>\n",
       "      <td>0.514024</td>\n",
       "      <td>0.595820</td>\n",
       "      <td>0.574112</td>\n",
       "      <td>0.511107</td>\n",
       "      <td>0.420929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175227</td>\n",
       "      <td>0.090028</td>\n",
       "      <td>0.090025</td>\n",
       "      <td>0.471224</td>\n",
       "      <td>0.511847</td>\n",
       "      <td>0.601688</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.239695</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.327409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.574602</td>\n",
       "      <td>0.630516</td>\n",
       "      <td>0.705386</td>\n",
       "      <td>0.616027</td>\n",
       "      <td>0.721745</td>\n",
       "      <td>0.738860</td>\n",
       "      <td>0.777052</td>\n",
       "      <td>0.774184</td>\n",
       "      <td>0.727668</td>\n",
       "      <td>0.663521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260633</td>\n",
       "      <td>0.149065</td>\n",
       "      <td>0.149058</td>\n",
       "      <td>0.697656</td>\n",
       "      <td>0.740560</td>\n",
       "      <td>0.814346</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.491935</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>0.547240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.848427</td>\n",
       "      <td>0.802549</td>\n",
       "      <td>0.827863</td>\n",
       "      <td>0.872159</td>\n",
       "      <td>0.896440</td>\n",
       "      <td>0.892082</td>\n",
       "      <td>0.887800</td>\n",
       "      <td>0.891561</td>\n",
       "      <td>0.851346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331805</td>\n",
       "      <td>0.240735</td>\n",
       "      <td>0.240726</td>\n",
       "      <td>0.875716</td>\n",
       "      <td>0.915828</td>\n",
       "      <td>0.947679</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.773297</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.781104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         comments       posts   followers      points  overview_page_views  \\\n",
       "count  365.000000  365.000000  365.000000  365.000000           365.000000   \n",
       "mean     0.553743    0.599464    0.667312    0.588562             0.658026   \n",
       "std      0.284817    0.275271    0.214088    0.269895             0.260020   \n",
       "min      0.000000    0.000000    0.000000    0.000000             0.000000   \n",
       "25%      0.332275    0.409717    0.566065    0.397166             0.501349   \n",
       "50%      0.574602    0.630516    0.705386    0.616027             0.721745   \n",
       "75%      0.817308    0.848427    0.802549    0.827863             0.872159   \n",
       "max      1.000000    1.000000    1.000000    1.000000             1.000000   \n",
       "\n",
       "       analysis_page_views  markets_page_views  charts_page_views  \\\n",
       "count           365.000000          365.000000         365.000000   \n",
       "mean              0.676191            0.710861           0.688210   \n",
       "std               0.261618            0.241494           0.259187   \n",
       "min               0.000000            0.000000           0.000000   \n",
       "25%               0.514024            0.595820           0.574112   \n",
       "50%               0.738860            0.777052           0.774184   \n",
       "75%               0.896440            0.892082           0.887800   \n",
       "max               1.000000            1.000000           1.000000   \n",
       "\n",
       "       trades_page_views  forum_page_views  ...  reddit_posts_per_day  \\\n",
       "count         365.000000        365.000000  ...            365.000000   \n",
       "mean            0.666003          0.609329  ...              0.278695   \n",
       "std             0.268737          0.279112  ...              0.169271   \n",
       "min             0.000000          0.000000  ...              0.000000   \n",
       "25%             0.511107          0.420929  ...              0.175227   \n",
       "50%             0.727668          0.663521  ...              0.260633   \n",
       "75%             0.891561          0.851346  ...              0.331805   \n",
       "max             1.000000          1.000000  ...              1.000000   \n",
       "\n",
       "       reddit_comments_per_hour  reddit_comments_per_day  code_repo_stars  \\\n",
       "count                365.000000               365.000000       365.000000   \n",
       "mean                   0.196713                 0.196708         0.648723   \n",
       "std                    0.163744                 0.163743         0.265907   \n",
       "min                    0.000000                 0.000000         0.000000   \n",
       "25%                    0.090028                 0.090025         0.471224   \n",
       "50%                    0.149065                 0.149058         0.697656   \n",
       "75%                    0.240735                 0.240726         0.875716   \n",
       "max                    1.000000                 1.000000         1.000000   \n",
       "\n",
       "       code_repo_forks  code_repo_subscribers  code_repo_open_pull_issues  \\\n",
       "count       365.000000             365.000000                  365.000000   \n",
       "mean          0.680061               0.737169                    0.414537   \n",
       "std           0.266401               0.255689                    0.215412   \n",
       "min           0.000000               0.000000                    0.000000   \n",
       "25%           0.511847               0.601688                    0.258824   \n",
       "50%           0.740560               0.814346                    0.400000   \n",
       "75%           0.915828               0.947679                    0.541176   \n",
       "max           1.000000               1.000000                    1.000000   \n",
       "\n",
       "       code_repo_closed_pull_issues  code_repo_open_issues  \\\n",
       "count                    365.000000             365.000000   \n",
       "mean                       0.500259               0.401739   \n",
       "std                        0.301799               0.244852   \n",
       "min                        0.000000               0.000000   \n",
       "25%                        0.239695               0.211538   \n",
       "50%                        0.491935               0.365385   \n",
       "75%                        0.773297               0.602564   \n",
       "max                        1.000000               1.000000   \n",
       "\n",
       "       code_repo_closed_issues  \n",
       "count               365.000000  \n",
       "mean                  0.542089  \n",
       "std                   0.291751  \n",
       "min                   0.000000  \n",
       "25%                   0.327409  \n",
       "50%                   0.547240  \n",
       "75%                   0.781104  \n",
       "max                   1.000000  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social = pd.read_csv(\"social18.csv\", index_col = 0)\n",
    "social = social.drop('code_repo_contributors', axis = 1)\n",
    "X = scale(social)\n",
    "print(X.shape)\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Run Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SOS gives Runtime error for some reason "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  0 INLIERS :  365 ABOD\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 FB\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 HBOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 IF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  2 INLIERS :  363 Average KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 OCSVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:375: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:376: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  0 INLIERS :  365 SOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 LOF\n",
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_111 (Dense)               (None, 32)           1056        input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_112 (Dense)               (None, 128)          4224        dense_111[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 128)          0           dense_112[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_113 (Dense)               (None, 64)           8256        dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 64)           0           dense_113[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_114 (Dense)               (None, 32)           2080        dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 32)           0           dense_114[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_115 (Dense)               (None, 2)            66          dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_116 (Dense)               (None, 2)            66          dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 2)            0           dense_115[0][0]                  \n",
      "                                                                 dense_116[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,748\n",
      "Trainable params: 15,748\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 14,662\n",
      "Trainable params: 14,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_32 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_32.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "model_31 (Model)             [(None, 2), (None, 2), (N 15748     \n",
      "_________________________________________________________________\n",
      "model_32 (Model)             (None, 32)                14662     \n",
      "=================================================================\n",
      "Total params: 30,410\n",
      "Trainable params: 30,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 69.5191 - val_loss: 72.8724\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 48.3263 - val_loss: 63.0692\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 41.0711 - val_loss: 57.7011\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 36.6616 - val_loss: 52.8783\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 118us/step - loss: 33.9150 - val_loss: 50.2850\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 32.1641 - val_loss: 48.9801\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 108us/step - loss: 31.0415 - val_loss: 48.4249\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 115us/step - loss: 31.0625 - val_loss: 48.2441\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 30.7564 - val_loss: 47.9079\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 93us/step - loss: 30.5414 - val_loss: 47.6442\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 100us/step - loss: 30.5744 - val_loss: 47.8736\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 30.5647 - val_loss: 47.7125\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.5972 - val_loss: 47.3003\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 30.5042 - val_loss: 47.2303\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 30.4215 - val_loss: 47.2861\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 30.4738 - val_loss: 47.2998\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.4411 - val_loss: 47.2964\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.5433 - val_loss: 47.2378\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 30.5042 - val_loss: 47.1676\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.4331 - val_loss: 47.2108\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.3830 - val_loss: 47.1582\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 30.4060 - val_loss: 47.2235\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 30.4557 - val_loss: 47.1137\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 30.4904 - val_loss: 47.2744\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.3979 - val_loss: 47.1350\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.3250 - val_loss: 47.1121\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.4763 - val_loss: 47.1102\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.4223 - val_loss: 47.1706\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.4092 - val_loss: 47.1570\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.4356 - val_loss: 47.1888\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.4272 - val_loss: 46.9772\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 30.4349 - val_loss: 47.1707\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.4333 - val_loss: 47.0801\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.4192 - val_loss: 47.0596\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.4447 - val_loss: 47.0891\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 30.4546 - val_loss: 47.1025\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 30.4077 - val_loss: 47.0666\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.4305 - val_loss: 47.0630\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 30.4001 - val_loss: 47.0770\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.4121 - val_loss: 47.0861\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4002 - val_loss: 47.0932\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 30.3954 - val_loss: 47.0715\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4110 - val_loss: 47.0575\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4085 - val_loss: 47.0575\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.3812 - val_loss: 47.0750\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.3963 - val_loss: 47.0518\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.3994 - val_loss: 47.0733\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.4123 - val_loss: 47.1115\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4096 - val_loss: 47.1198\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.3791 - val_loss: 47.0223\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4060 - val_loss: 47.1429\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.4319 - val_loss: 47.0848\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.4183 - val_loss: 46.9808\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4331 - val_loss: 47.0051\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.4138 - val_loss: 47.0375\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4004 - val_loss: 47.0525\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.4279 - val_loss: 46.9812\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.4314 - val_loss: 46.9827\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.3930 - val_loss: 47.0440\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.3965 - val_loss: 47.0123\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.3958 - val_loss: 47.0573\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.3846 - val_loss: 47.0259\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.4039 - val_loss: 46.9734\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.4114 - val_loss: 47.0804\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.4153 - val_loss: 46.9583\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4303 - val_loss: 47.0278\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.3988 - val_loss: 46.9832\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 30.4099 - val_loss: 46.9920\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 30.4050 - val_loss: 47.0137\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.4236 - val_loss: 46.9997\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.4118 - val_loss: 47.0077\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.3861 - val_loss: 47.0367\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4110 - val_loss: 46.9992\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.4087 - val_loss: 46.9998\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.4036 - val_loss: 47.0235\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4185 - val_loss: 47.0317\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4065 - val_loss: 46.9924\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 30.4171 - val_loss: 47.0173\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.3950 - val_loss: 47.0075\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.3971 - val_loss: 47.0898\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.3909 - val_loss: 47.0101\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.4105 - val_loss: 46.9937\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.3887 - val_loss: 46.9794\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.4179 - val_loss: 47.0648\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.4112 - val_loss: 46.9775\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4122 - val_loss: 47.0850\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.4100 - val_loss: 47.0387\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.4113 - val_loss: 47.0246\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4265 - val_loss: 47.0333\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.4162 - val_loss: 46.9969\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4138 - val_loss: 47.0168\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 30.4114 - val_loss: 47.0176\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.4114 - val_loss: 47.0430\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4006 - val_loss: 46.9993\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 30.4030 - val_loss: 47.0037\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.4080 - val_loss: 47.0026\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 30.4032 - val_loss: 47.0174\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.4054 - val_loss: 47.0148\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 30.4097 - val_loss: 47.0110\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 30.4053 - val_loss: 47.0258\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 VAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  0 INLIERS :  365 ABOD\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 FB\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 HBOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 IF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  5 INLIERS :  360 KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  3 INLIERS :  362 Average KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 OCSVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:375: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:376: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  0 INLIERS :  365 SOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 LOF\n",
      "Model: \"model_34\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_122 (Dense)               (None, 32)           1056        input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_123 (Dense)               (None, 128)          4224        dense_122[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 128)          0           dense_123[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_124 (Dense)               (None, 64)           8256        dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 64)           0           dense_124[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_125 (Dense)               (None, 32)           2080        dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 32)           0           dense_125[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_126 (Dense)               (None, 2)            66          dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_127 (Dense)               (None, 2)            66          dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 2)            0           dense_126[0][0]                  \n",
      "                                                                 dense_127[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,748\n",
      "Trainable params: 15,748\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 14,662\n",
      "Trainable params: 14,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_35 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_35.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "model_34 (Model)             [(None, 2), (None, 2), (N 15748     \n",
      "_________________________________________________________________\n",
      "model_35 (Model)             (None, 32)                14662     \n",
      "=================================================================\n",
      "Total params: 30,410\n",
      "Trainable params: 30,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 70.0694 - val_loss: 55.3176\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 50.3605 - val_loss: 47.3080\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 43.0063 - val_loss: 41.0118\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 38.4318 - val_loss: 37.4985\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 131us/step - loss: 35.8095 - val_loss: 35.3908\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 33.8039 - val_loss: 33.4832\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 33.4713 - val_loss: 32.9869\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 126us/step - loss: 32.9798 - val_loss: 33.1729\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 32.7076 - val_loss: 32.8546\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 32.5054 - val_loss: 32.9439\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 32.4927 - val_loss: 32.7040\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 32.3028 - val_loss: 32.5527\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.1874 - val_loss: 32.3993\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.2376 - val_loss: 32.4785\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.1849 - val_loss: 32.4419\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.1466 - val_loss: 32.4877\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.1167 - val_loss: 32.4134\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0631 - val_loss: 32.4355\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1068 - val_loss: 32.3879\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0528 - val_loss: 32.3019\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0513 - val_loss: 32.3205\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 32.0137 - val_loss: 32.2695\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0026 - val_loss: 32.2729\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9848 - val_loss: 32.3935\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.9952 - val_loss: 32.3134\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 31.9948 - val_loss: 32.3504\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9815 - val_loss: 32.3979\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0352 - val_loss: 32.3319\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9929 - val_loss: 32.3642\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0184 - val_loss: 32.3215\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9638 - val_loss: 32.3519\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9904 - val_loss: 32.2951\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.9686 - val_loss: 32.3166\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9934 - val_loss: 32.3838\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9892 - val_loss: 32.3103\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9977 - val_loss: 32.2658\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9858 - val_loss: 32.2867\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9933 - val_loss: 32.3122\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0064 - val_loss: 32.2857\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9985 - val_loss: 32.2820\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9911 - val_loss: 32.3408\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.9549 - val_loss: 32.2912\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9781 - val_loss: 32.2949\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9929 - val_loss: 32.2738\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9648 - val_loss: 32.2928\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.9766 - val_loss: 32.2671\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9721 - val_loss: 32.2685\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.9494 - val_loss: 32.2847\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9707 - val_loss: 32.3136\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9911 - val_loss: 32.2992\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9886 - val_loss: 32.3216\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9664 - val_loss: 32.2657\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9606 - val_loss: 32.2607\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9885 - val_loss: 32.2743\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9804 - val_loss: 32.2492\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.9857 - val_loss: 32.2795\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.9873 - val_loss: 32.2961\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9470 - val_loss: 32.2758\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9611 - val_loss: 32.2877\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9731 - val_loss: 32.2923\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.9842 - val_loss: 32.2733\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.9731 - val_loss: 32.3045\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.9781 - val_loss: 32.2876\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9649 - val_loss: 32.3203\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9865 - val_loss: 32.2911\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9652 - val_loss: 32.2751\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9794 - val_loss: 32.3792\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.9753 - val_loss: 32.2152\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9696 - val_loss: 32.3274\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9618 - val_loss: 32.2775\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9713 - val_loss: 32.3038\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9587 - val_loss: 32.3229\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9769 - val_loss: 32.3444\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.9609 - val_loss: 32.3013\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9778 - val_loss: 32.3373\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9777 - val_loss: 32.2820\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.9845 - val_loss: 32.3170\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.9704 - val_loss: 32.3211\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9478 - val_loss: 32.2789\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.9738 - val_loss: 32.2879\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9725 - val_loss: 32.3188\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9690 - val_loss: 32.3197\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.9791 - val_loss: 32.3012\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9808 - val_loss: 32.2996\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9792 - val_loss: 32.3250\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9398 - val_loss: 32.3173\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0143 - val_loss: 32.2831\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9731 - val_loss: 32.2965\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.9717 - val_loss: 32.2536\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9810 - val_loss: 32.2722\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9692 - val_loss: 32.2833\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.9772 - val_loss: 32.2649\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9539 - val_loss: 32.2768\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 31.9638 - val_loss: 32.2726\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9551 - val_loss: 32.3166\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9767 - val_loss: 32.3163\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9752 - val_loss: 32.2705\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9738 - val_loss: 32.3065\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9709 - val_loss: 32.2826\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9640 - val_loss: 32.2981\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 VAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  0 INLIERS :  365 ABOD\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  10 INLIERS :  355 FB\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 HBOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 IF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  5 INLIERS :  360 KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  4 INLIERS :  361 Average KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 OCSVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:375: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:376: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  0 INLIERS :  365 SOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  10 INLIERS :  355 LOF\n",
      "Model: \"model_37\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_133 (Dense)               (None, 32)           1056        input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_134 (Dense)               (None, 128)          4224        dense_133[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 128)          0           dense_134[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_135 (Dense)               (None, 64)           8256        dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 64)           0           dense_135[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_136 (Dense)               (None, 32)           2080        dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 32)           0           dense_136[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_137 (Dense)               (None, 2)            66          dropout_75[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_138 (Dense)               (None, 2)            66          dropout_75[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 2)            0           dense_137[0][0]                  \n",
      "                                                                 dense_138[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,748\n",
      "Trainable params: 15,748\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 14,662\n",
      "Trainable params: 14,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_38 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_38.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "model_37 (Model)             [(None, 2), (None, 2), (N 15748     \n",
      "_________________________________________________________________\n",
      "model_38 (Model)             (None, 32)                14662     \n",
      "=================================================================\n",
      "Total params: 30,410\n",
      "Trainable params: 30,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 91.0615 - val_loss: 63.7348\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 58.4954 - val_loss: 48.0679\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 46.8741 - val_loss: 40.8695\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 40.5615 - val_loss: 37.1483\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 36.6819 - val_loss: 34.6458\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 114us/step - loss: 34.6199 - val_loss: 33.6060\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 33.7629 - val_loss: 33.0427\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 114us/step - loss: 33.2879 - val_loss: 32.6294\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 32.9390 - val_loss: 32.4111\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 32.7957 - val_loss: 32.2865\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.5892 - val_loss: 32.2523\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.5327 - val_loss: 32.1481\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.4204 - val_loss: 32.0733\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.3764 - val_loss: 32.0845\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.3455 - val_loss: 32.0260\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.2959 - val_loss: 31.9357\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.2463 - val_loss: 31.9264\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.2488 - val_loss: 31.8913\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.1988 - val_loss: 31.8804\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.1503 - val_loss: 31.7840\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.1483 - val_loss: 31.7789\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 32.1171 - val_loss: 31.7964\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.1179 - val_loss: 31.7888\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.1159 - val_loss: 31.7587\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.1313 - val_loss: 31.7216\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0902 - val_loss: 31.7527\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.1070 - val_loss: 31.7411\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1025 - val_loss: 31.7046\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0832 - val_loss: 31.7035\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0613 - val_loss: 31.6888\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0699 - val_loss: 31.6869\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0786 - val_loss: 31.6894\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.0590 - val_loss: 31.6964\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0872 - val_loss: 31.6938\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0789 - val_loss: 31.6819\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.0539 - val_loss: 31.6778\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0490 - val_loss: 31.6846\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0495 - val_loss: 31.6768\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0502 - val_loss: 31.6790\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0526 - val_loss: 31.6604\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0540 - val_loss: 31.6899\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0461 - val_loss: 31.6667\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0432 - val_loss: 31.6508\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0410 - val_loss: 31.6461\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0546 - val_loss: 31.6708\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0524 - val_loss: 31.6584\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0379 - val_loss: 31.6760\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0604 - val_loss: 31.6608\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0590 - val_loss: 31.6632\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0496 - val_loss: 31.6619\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0420 - val_loss: 31.6564\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0420 - val_loss: 31.6629\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0473 - val_loss: 31.6540\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0470 - val_loss: 31.6745\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0477 - val_loss: 31.6666\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0528 - val_loss: 31.6655\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0412 - val_loss: 31.6570\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0471 - val_loss: 31.6607\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0432 - val_loss: 31.6563\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0445 - val_loss: 31.6614\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0435 - val_loss: 31.6620\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0457 - val_loss: 31.6551\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0428 - val_loss: 31.6570\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0458 - val_loss: 31.6598\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.0437 - val_loss: 31.6568\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.0406 - val_loss: 31.6576\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0458 - val_loss: 31.6581\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0432 - val_loss: 31.6593\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0369 - val_loss: 31.6652\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0418 - val_loss: 31.6591\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0384 - val_loss: 31.6581\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0399 - val_loss: 31.6569\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0394 - val_loss: 31.6641\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0424 - val_loss: 31.6611\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0376 - val_loss: 31.6599\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0429 - val_loss: 31.6586\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0386 - val_loss: 31.6595\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0425 - val_loss: 31.6589\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0387 - val_loss: 31.6595\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0453 - val_loss: 31.6587\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0371 - val_loss: 31.6608\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0385 - val_loss: 31.6593\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0379 - val_loss: 31.6620\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0408 - val_loss: 31.6591\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0390 - val_loss: 31.6596\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0387 - val_loss: 31.6581\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0358 - val_loss: 31.6597\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.0407 - val_loss: 31.6612\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0397 - val_loss: 31.6581\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0401 - val_loss: 31.6577\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0407 - val_loss: 31.6614\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0437 - val_loss: 31.6606\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0415 - val_loss: 31.6580\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0388 - val_loss: 31.6608\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0403 - val_loss: 31.6577\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0409 - val_loss: 31.6594\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0394 - val_loss: 31.6595\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0396 - val_loss: 31.6601\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0395 - val_loss: 31.6588\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0397 - val_loss: 31.6611\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 VAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  0 INLIERS :  365 ABOD\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  14 INLIERS :  351 FB\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 HBOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 IF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  10 INLIERS :  355 KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  7 INLIERS :  358 Average KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 OCSVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:375: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:376: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  0 INLIERS :  365 SOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  14 INLIERS :  351 LOF\n",
      "Model: \"model_40\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_144 (Dense)               (None, 32)           1056        input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_145 (Dense)               (None, 128)          4224        dense_144[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 128)          0           dense_145[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_146 (Dense)               (None, 64)           8256        dropout_79[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 64)           0           dense_146[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_147 (Dense)               (None, 32)           2080        dropout_80[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (None, 32)           0           dense_147[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_148 (Dense)               (None, 2)            66          dropout_81[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_149 (Dense)               (None, 2)            66          dropout_81[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 2)            0           dense_148[0][0]                  \n",
      "                                                                 dense_149[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,748\n",
      "Trainable params: 15,748\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 14,662\n",
      "Trainable params: 14,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_41 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_41.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "model_40 (Model)             [(None, 2), (None, 2), (N 15748     \n",
      "_________________________________________________________________\n",
      "model_41 (Model)             (None, 32)                14662     \n",
      "=================================================================\n",
      "Total params: 30,410\n",
      "Trainable params: 30,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 88.1687 - val_loss: 56.1447\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 110us/step - loss: 58.3120 - val_loss: 46.0852\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 108us/step - loss: 48.2714 - val_loss: 40.8210\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 118us/step - loss: 42.3328 - val_loss: 37.5394\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 38.7462 - val_loss: 34.6189\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 36.1458 - val_loss: 32.6626\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 117us/step - loss: 34.5606 - val_loss: 32.3493\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 33.1107 - val_loss: 32.2558\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 33.5175 - val_loss: 32.0603\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 97us/step - loss: 33.1037 - val_loss: 31.9330\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 32.8589 - val_loss: 31.7046\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.7053 - val_loss: 31.7713\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.5294 - val_loss: 31.6137\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.4534 - val_loss: 31.6574\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.3859 - val_loss: 31.6874\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.2517 - val_loss: 31.6705\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.2310 - val_loss: 31.6262\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 32.2175 - val_loss: 31.7294\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.1745 - val_loss: 31.4597\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0959 - val_loss: 31.6298\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1353 - val_loss: 31.6767\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.1417 - val_loss: 31.6064\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 32.1617 - val_loss: 31.5250\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1737 - val_loss: 31.5223\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1006 - val_loss: 31.6462\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.1448 - val_loss: 31.5414\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0926 - val_loss: 31.5996\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.0772 - val_loss: 31.5743\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0523 - val_loss: 31.6143\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0495 - val_loss: 31.5471\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0817 - val_loss: 31.5902\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0669 - val_loss: 31.5631\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0729 - val_loss: 31.5439\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0544 - val_loss: 31.5760\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0524 - val_loss: 31.5363\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0429 - val_loss: 31.5540\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0571 - val_loss: 31.5422\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0902 - val_loss: 31.5219\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0612 - val_loss: 31.5665\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0564 - val_loss: 31.5400\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0672 - val_loss: 31.5247\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0624 - val_loss: 31.5452\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0491 - val_loss: 31.6006\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.0277 - val_loss: 31.5638\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0489 - val_loss: 31.5721\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0568 - val_loss: 31.5303\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0475 - val_loss: 31.5518\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0477 - val_loss: 31.5584\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0406 - val_loss: 31.5563\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0406 - val_loss: 31.5591\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.0500 - val_loss: 31.6205\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0835 - val_loss: 31.5334\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0327 - val_loss: 31.5584\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0597 - val_loss: 31.5899\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0515 - val_loss: 31.4998\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0632 - val_loss: 31.5898\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0533 - val_loss: 31.5354\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0403 - val_loss: 31.5705\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0437 - val_loss: 31.5851\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0043 - val_loss: 31.6309\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0491 - val_loss: 31.4383\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0749 - val_loss: 31.4354\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0677 - val_loss: 31.5462\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0327 - val_loss: 31.6072\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1118 - val_loss: 31.6008\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0722 - val_loss: 31.6024\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0438 - val_loss: 31.5439\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0542 - val_loss: 31.5340\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0704 - val_loss: 31.5859\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0494 - val_loss: 31.5716\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0498 - val_loss: 31.6085\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0820 - val_loss: 31.5440\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0583 - val_loss: 31.5383\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0629 - val_loss: 31.5760\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0489 - val_loss: 31.5616\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0578 - val_loss: 31.5643\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0526 - val_loss: 31.5678\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0376 - val_loss: 31.5675\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0739 - val_loss: 31.5490\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0680 - val_loss: 31.5479\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0456 - val_loss: 31.5532\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0578 - val_loss: 31.5592\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0522 - val_loss: 31.5576\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0635 - val_loss: 31.5349\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0583 - val_loss: 31.5445\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0459 - val_loss: 31.5298\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0440 - val_loss: 31.5450\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0555 - val_loss: 31.5460\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0417 - val_loss: 31.5430\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0646 - val_loss: 31.5633\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0596 - val_loss: 31.5468\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0463 - val_loss: 31.5625\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0618 - val_loss: 31.5513\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 32.0487 - val_loss: 31.5417\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0482 - val_loss: 31.5433\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0635 - val_loss: 31.5454\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.0605 - val_loss: 31.5539\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.0554 - val_loss: 31.5467\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0572 - val_loss: 31.5483\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.0567 - val_loss: 31.5373\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 VAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  0 INLIERS :  365 ABOD\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  18 INLIERS :  347 FB\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 HBOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 IF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  12 INLIERS :  353 KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  7 INLIERS :  358 Average KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 OCSVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:375: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:376: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  0 INLIERS :  365 SOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  18 INLIERS :  347 LOF\n",
      "Model: \"model_43\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_155 (Dense)               (None, 32)           1056        input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_156 (Dense)               (None, 128)          4224        dense_155[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 128)          0           dense_156[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_157 (Dense)               (None, 64)           8256        dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)            (None, 64)           0           dense_157[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_158 (Dense)               (None, 32)           2080        dropout_86[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)            (None, 32)           0           dense_158[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_159 (Dense)               (None, 2)            66          dropout_87[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_160 (Dense)               (None, 2)            66          dropout_87[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 2)            0           dense_159[0][0]                  \n",
      "                                                                 dense_160[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,748\n",
      "Trainable params: 15,748\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_30 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 14,662\n",
      "Trainable params: 14,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_44 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_44.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "model_43 (Model)             [(None, 2), (None, 2), (N 15748     \n",
      "_________________________________________________________________\n",
      "model_44 (Model)             (None, 32)                14662     \n",
      "=================================================================\n",
      "Total params: 30,410\n",
      "Trainable params: 30,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 67.6537 - val_loss: 54.4223\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 49.7374 - val_loss: 47.4624\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 43.0107 - val_loss: 43.7667\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 38.6557 - val_loss: 39.3858\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 35.6455 - val_loss: 35.8428\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 33.8585 - val_loss: 34.6694\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 32.9722 - val_loss: 33.9952\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 32.6857 - val_loss: 33.7935\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 125us/step - loss: 32.4096 - val_loss: 33.7642\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 94us/step - loss: 32.2600 - val_loss: 34.0288\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 32.2394 - val_loss: 33.6562\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 32.0699 - val_loss: 33.8446\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 32.0968 - val_loss: 33.4883\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0217 - val_loss: 33.7737\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.9482 - val_loss: 33.6732\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.0790 - val_loss: 33.7481\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 31.9521 - val_loss: 33.5783\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.9697 - val_loss: 33.5901\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 31.8466 - val_loss: 33.6206\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8613 - val_loss: 33.5725\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8544 - val_loss: 33.5766\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8439 - val_loss: 33.5571\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8177 - val_loss: 33.5189\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8776 - val_loss: 33.6027\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8958 - val_loss: 33.4094\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 31.8773 - val_loss: 33.5281\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8891 - val_loss: 33.4952\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8733 - val_loss: 33.5164\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8315 - val_loss: 33.4594\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.8365 - val_loss: 33.4542\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8547 - val_loss: 33.4833\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.8354 - val_loss: 33.4759\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8827 - val_loss: 33.5160\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 31.8366 - val_loss: 33.5279\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.8210 - val_loss: 33.5834\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8467 - val_loss: 33.5412\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8394 - val_loss: 33.4370\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8616 - val_loss: 33.5254\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8681 - val_loss: 33.5248\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8437 - val_loss: 33.4789\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8397 - val_loss: 33.4886\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8336 - val_loss: 33.4897\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8193 - val_loss: 33.4718\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8345 - val_loss: 33.6182\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8391 - val_loss: 33.5417\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8356 - val_loss: 33.5174\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8165 - val_loss: 33.5214\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8120 - val_loss: 33.5239\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.8807 - val_loss: 33.4777\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8409 - val_loss: 33.4962\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8260 - val_loss: 33.5308\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8479 - val_loss: 33.4905\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8294 - val_loss: 33.5422\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8533 - val_loss: 33.5179\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8519 - val_loss: 33.5023\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8311 - val_loss: 33.4838\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8360 - val_loss: 33.4565\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8314 - val_loss: 33.5019\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8131 - val_loss: 33.4324\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8476 - val_loss: 33.4591\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8134 - val_loss: 33.5013\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8443 - val_loss: 33.5234\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8113 - val_loss: 33.4439\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8126 - val_loss: 33.4197\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 31.8447 - val_loss: 33.5109\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.7970 - val_loss: 33.5942\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8043 - val_loss: 33.4870\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8546 - val_loss: 33.4254\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.7909 - val_loss: 33.4880\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.7990 - val_loss: 33.6683\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8373 - val_loss: 33.6639\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8639 - val_loss: 33.6078\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8231 - val_loss: 33.9014\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8165 - val_loss: 33.8548\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8886 - val_loss: 33.5623\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.7956 - val_loss: 33.3809\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.7655 - val_loss: 33.7157\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8784 - val_loss: 33.7077\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8551 - val_loss: 33.4320\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8089 - val_loss: 33.4867\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8145 - val_loss: 33.4999\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.7884 - val_loss: 33.6359\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.7467 - val_loss: 33.9419\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 31.8014 - val_loss: 33.5582\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9258 - val_loss: 33.5635\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.7999 - val_loss: 33.5845\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8721 - val_loss: 33.5811\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8242 - val_loss: 33.5862\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8098 - val_loss: 33.4439\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8338 - val_loss: 33.4927\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.7757 - val_loss: 33.6481\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.7953 - val_loss: 33.8023\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.7765 - val_loss: 33.6505\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8566 - val_loss: 33.7311\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8133 - val_loss: 33.6862\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 31.8516 - val_loss: 33.4438\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8159 - val_loss: 33.6093\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8117 - val_loss: 33.6426\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8230 - val_loss: 33.5765\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.7771 - val_loss: 33.6227\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  18 INLIERS :  347 VAE\n"
     ]
    }
   ],
   "source": [
    "# time the code\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "# empty list to store outliers and inliners\n",
    "df_outliers = []\n",
    "scores_pred_lst = []\n",
    "proba_lst = []\n",
    "predict_rank_lst = []\n",
    "\n",
    "# iterate over different outlier fractions \n",
    "outliers_fraction = [0.01,0.02,0.03,0.04,0.05]\n",
    "for fraction in outliers_fraction:\n",
    "    # Define 13 outlier detection tools to be compared\n",
    "    classifiers = {#'AE' : AutoEncoder(contamination=outliers_fraction, random_state=random_state), # check why error\n",
    "               'ABOD': ABOD(contamination = fraction),\n",
    "              'CBLOF':CBLOF(check_estimator=False, random_state=random_state), # getting 0 \n",
    "              'FB':FeatureBagging(LOF(n_neighbors=35),contamination=fraction,check_estimator=False,random_state=random_state),\n",
    "              'HBOS': HBOS(contamination=fraction),\n",
    "              'IF': IForest(contamination=fraction,random_state=random_state),\n",
    "              'KNN': KNN(contamination=fraction),\n",
    "              'Average KNN': KNN(method='mean',contamination=fraction),\n",
    "              'OCSVM' : OCSVM(contamination=fraction),\n",
    "              'SOS' : SOS(contamination=fraction),\n",
    "              'LOF': LOF(n_neighbors=35, contamination = fraction),\n",
    "              #'LOCI': LOCI(contamination = fraction), # takes a super long time + don't understand\n",
    "              'VAE' : VAE(contamination=fraction, random_state=random_state) # default nbr of neurons too much for blockchain dataset\n",
    "              }\n",
    "\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        clf.fit(X)\n",
    "        # predict raw anomaly score\n",
    "        scores_pred = clf.decision_function(X) * -1 # WHY? * -1\n",
    "\n",
    "        # prediction of a datapoint category outlier or inlier\n",
    "        y_pred = clf.predict(X)\n",
    "\n",
    "        # probability of outlier \n",
    "        probability = clf.predict_proba(X, method = 'linear')\n",
    "\n",
    "        # predict rank\n",
    "        predict_rank = clf._predict_rank(X)\n",
    "\n",
    "        n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "        n_outliers = np.count_nonzero(y_pred == 1)\n",
    "\n",
    "        # copy of dataframe\n",
    "        dfx = X.copy()\n",
    "\n",
    "        dfx[clf_name] = y_pred.tolist()\n",
    "        #print(clf_name,'\\n',i,clf,'\\n',classifiers,'\\n',classifiers.items())\n",
    "        print('Outlier fraction: ', fraction)\n",
    "        print('OUTLIERS : ',n_outliers,'INLIERS : ',n_inliers, clf_name)\n",
    "        #print('scores_pred: ', scores_pred)\n",
    "        scores_pred_lst.append(scores_pred)\n",
    "        df_outliers.append(dfx[clf_name]) \n",
    "        proba_lst.append(probability)\n",
    "        predict_rank_lst.append(predict_rank)\n",
    "t1 = time.time()\n",
    "total = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run:  53.36793041229248 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Time to run: ',total,'s')\n",
    "# we have 50 lists (or essentially 5 with 10 models)\n",
    "len(df_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the outliers lists uniquely for later\n",
    "block_df_s = df_outliers\n",
    "# probability lists\n",
    "block_proba_s = proba_lst\n",
    "# rank lists\n",
    "block_rank_s = predict_rank_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Seperate models per contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack lists based on outlier fraction e.g. first 10 = 0.01 outlier fraction\n",
    "frac_oo1 = df_outliers[0:11]\n",
    "frac_oo2 = df_outliers[11:22]\n",
    "frac_oo3 = df_outliers[22:33]\n",
    "frac_oo4 = df_outliers[33:44]\n",
    "frac_oo5 = df_outliers[44:]\n",
    "\n",
    "# use the function to extract the dates\n",
    "frac_oo1_outliers = outlier_subset_function(frac_oo1)\n",
    "frac_oo2_outliers = outlier_subset_function(frac_oo2)\n",
    "frac_oo3_outliers = outlier_subset_function(frac_oo3)\n",
    "frac_oo4_outliers = outlier_subset_function(frac_oo4)\n",
    "frac_oo5_outliers = outlier_subset_function(frac_oo5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Get Unique Anomaly Dates per Contamination fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction 0.01 unique dates:  37\n",
      "Outlier fraction 0.02 unique dates:  38\n",
      "Outlier fraction 0.03 unique dates:  40\n",
      "Outlier fraction 0.04 unique dates:  44\n",
      "Outlier fraction 0.05 unique dates:  48\n"
     ]
    }
   ],
   "source": [
    "dates_per_fraction(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Anomaly Likelihood\n",
    "- Assumption1: if all contamination levels vote for a date the date is more likely to be an anomaly\n",
    "- Assumption2: if more models vote for a date the date is more likely to be an anomaly\n",
    "- So if both assumtions are true we could say date is even more likely to be an anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Anomaly Dates:  48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AllContaminations_Vote</th>\n",
       "      <th>ModelAnomaly_Vote</th>\n",
       "      <th>weight_anomaly_models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>0.060870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>0.053913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.052174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.038261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>0.036522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.034783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-10</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.034783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.033043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.033043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-11-24</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.033043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.033043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.031304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.027826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.026087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.026087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.024348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.024348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.022609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.022609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.020870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.020870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.019130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.015652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.013913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.012174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.012174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.012174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.012174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.010435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.015652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2018-10-25</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2018-04-23</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2018-10-28</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2018-08-20</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2018-10-24</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2018-10-27</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2018-10-29</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2018-10-23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2018-02-14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  AllContaminations_Vote  ModelAnomaly_Vote  \\\n",
       "4   2018-04-08                       5                 35   \n",
       "36  2018-01-16                       5                 31   \n",
       "30  2018-01-17                       5                 30   \n",
       "27  2018-01-02                       5                 25   \n",
       "17  2018-01-05                       5                 23   \n",
       "20  2018-01-01                       5                 22   \n",
       "16  2018-01-09                       5                 21   \n",
       "1   2018-02-01                       5                 20   \n",
       "2   2018-09-10                       5                 20   \n",
       "3   2018-01-06                       5                 19   \n",
       "19  2018-01-07                       5                 19   \n",
       "29  2018-11-24                       5                 19   \n",
       "32  2018-04-09                       5                 19   \n",
       "8   2018-01-03                       5                 18   \n",
       "6   2018-01-08                       5                 16   \n",
       "5   2018-12-31                       5                 15   \n",
       "34  2018-01-04                       5                 15   \n",
       "10  2018-01-10                       5                 14   \n",
       "18  2018-12-30                       5                 14   \n",
       "11  2018-12-28                       5                 13   \n",
       "26  2018-12-29                       5                 13   \n",
       "15  2018-01-11                       5                 12   \n",
       "24  2018-01-12                       5                 12   \n",
       "13  2018-11-26                       5                 11   \n",
       "9   2018-01-15                       5                  9   \n",
       "21  2018-01-19                       5                  8   \n",
       "7   2018-01-20                       5                  7   \n",
       "12  2018-01-13                       5                  7   \n",
       "28  2018-01-22                       5                  7   \n",
       "31  2018-01-14                       5                  7   \n",
       "23  2018-01-18                       5                  6   \n",
       "0   2018-01-30                       5                  5   \n",
       "14  2018-01-24                       5                  5   \n",
       "22  2018-01-31                       5                  5   \n",
       "25  2018-01-23                       5                  5   \n",
       "33  2018-01-21                       5                  5   \n",
       "35  2018-01-25                       5                  5   \n",
       "37  2018-12-27                       4                  9   \n",
       "38  2018-10-26                       3                  5   \n",
       "39  2018-10-25                       3                  5   \n",
       "42  2018-04-23                       2                  4   \n",
       "40  2018-10-28                       2                  3   \n",
       "41  2018-08-20                       2                  3   \n",
       "43  2018-10-24                       2                  3   \n",
       "45  2018-10-27                       1                  2   \n",
       "47  2018-10-29                       1                  2   \n",
       "44  2018-10-23                       1                  1   \n",
       "46  2018-02-14                       1                  1   \n",
       "\n",
       "    weight_anomaly_models  \n",
       "4                0.060870  \n",
       "36               0.053913  \n",
       "30               0.052174  \n",
       "27               0.043478  \n",
       "17               0.040000  \n",
       "20               0.038261  \n",
       "16               0.036522  \n",
       "1                0.034783  \n",
       "2                0.034783  \n",
       "3                0.033043  \n",
       "19               0.033043  \n",
       "29               0.033043  \n",
       "32               0.033043  \n",
       "8                0.031304  \n",
       "6                0.027826  \n",
       "5                0.026087  \n",
       "34               0.026087  \n",
       "10               0.024348  \n",
       "18               0.024348  \n",
       "11               0.022609  \n",
       "26               0.022609  \n",
       "15               0.020870  \n",
       "24               0.020870  \n",
       "13               0.019130  \n",
       "9                0.015652  \n",
       "21               0.013913  \n",
       "7                0.012174  \n",
       "12               0.012174  \n",
       "28               0.012174  \n",
       "31               0.012174  \n",
       "23               0.010435  \n",
       "0                0.008696  \n",
       "14               0.008696  \n",
       "22               0.008696  \n",
       "25               0.008696  \n",
       "33               0.008696  \n",
       "35               0.008696  \n",
       "37               0.015652  \n",
       "38               0.008696  \n",
       "39               0.008696  \n",
       "42               0.006957  \n",
       "40               0.005217  \n",
       "41               0.005217  \n",
       "43               0.005217  \n",
       "45               0.003478  \n",
       "47               0.003478  \n",
       "44               0.001739  \n",
       "46               0.001739  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_anomalies = date_ranking(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)\n",
    "print('Unique Anomaly Dates: ',len(social_anomalies.Date.unique()))\n",
    "social_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Borderline probability analysis\n",
    "e.g. the probability was 51% that date is an anomaly --> whats the realtionships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.50292904e-03, 9.95497071e-01],\n",
       "       [1.12099410e-03, 9.98879006e-01],\n",
       "       [5.23235301e-03, 9.94767647e-01],\n",
       "       [7.57957939e-04, 9.99242042e-01],\n",
       "       [1.78144570e-04, 9.99821855e-01],\n",
       "       [6.24011819e-03, 9.93759882e-01],\n",
       "       [2.34677593e-03, 9.97653224e-01],\n",
       "       [1.40997280e-02, 9.85900272e-01],\n",
       "       [8.00851819e-04, 9.99199148e-01],\n",
       "       [1.54262402e-01, 8.45737598e-01]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_proba_s[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Same anomalies in identified all 3 datasets\n",
    "- inner join (merge) anomaly likelihood dataframes for all three datasets \n",
    "- we get dates that were identified in all three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr of common date anomalies across three datasets:  20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contamination_avg</th>\n",
       "      <th>modelVote_sum</th>\n",
       "      <th>modelVote_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>29.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-16</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>28.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>83</td>\n",
       "      <td>27.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>81</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-17</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>78</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>75</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>72</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>70</td>\n",
       "      <td>23.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>65</td>\n",
       "      <td>21.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>65</td>\n",
       "      <td>21.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>59</td>\n",
       "      <td>19.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-11</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>53</td>\n",
       "      <td>17.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>51</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-12</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>13.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-15</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>27</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-13</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>26</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-18</th>\n",
       "      <td>4.666667</td>\n",
       "      <td>32</td>\n",
       "      <td>10.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-20</th>\n",
       "      <td>3.666667</td>\n",
       "      <td>14</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-21</th>\n",
       "      <td>3.333333</td>\n",
       "      <td>10</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            contamination_avg  modelVote_sum  modelVote_avg\n",
       "Date                                                       \n",
       "2018-01-05           5.000000             88      29.333333\n",
       "2018-01-16           5.000000             85      28.333333\n",
       "2018-01-04           5.000000             83      27.666667\n",
       "2018-01-06           5.000000             81      27.000000\n",
       "2018-01-17           5.000000             78      26.000000\n",
       "2018-01-03           5.000000             75      25.000000\n",
       "2018-01-02           5.000000             72      24.000000\n",
       "2018-01-07           5.000000             70      23.333333\n",
       "2018-01-01           5.000000             65      21.666667\n",
       "2018-01-09           5.000000             65      21.666667\n",
       "2018-01-08           5.000000             59      19.666667\n",
       "2018-01-11           5.000000             53      17.666667\n",
       "2018-01-10           5.000000             51      17.000000\n",
       "2018-01-12           5.000000             41      13.666667\n",
       "2018-01-15           5.000000             27       9.000000\n",
       "2018-01-13           5.000000             26       8.666667\n",
       "2018-01-18           4.666667             32      10.666667\n",
       "2018-01-19           4.000000             18       6.000000\n",
       "2018-01-20           3.666667             14       4.666667\n",
       "2018-01-21           3.333333             10       3.333333"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge 3 dataframes on date \n",
    "three_dataset_anomalies = pd.merge(pd.merge(price_anomalies,block_anomalies,on='Date'),social_anomalies,on='Date')\n",
    "# rename the columns so its easier to read/refer\n",
    "three_dataset_anomalies.columns = ['Date', 'AllContaminations_Vote_Price', 'ModelAnomaly_Vote_Price',\n",
    "       'weight_anomaly_models_Price', 'AllContaminations_Vote_Block',\n",
    "       'ModelAnomaly_Vote_Block', 'weight_anomaly_models_Block',\n",
    "       'AllContaminations_Vote_Social', 'ModelAnomaly_Vote_Social', 'weight_anomaly_models_Social']\n",
    "\n",
    "# sum row-wise per date (for 3-datasets): mean contamination, sum and mean model votes\n",
    "contamination_avg = pd.DataFrame(three_dataset_anomalies[['AllContaminations_Vote_Price','AllContaminations_Vote_Block',\n",
    "                                      'AllContaminations_Vote_Social']].mean(axis=1))\n",
    "modelVote_sum = pd.DataFrame(three_dataset_anomalies[['ModelAnomaly_Vote_Price','ModelAnomaly_Vote_Block',\n",
    "                                      'ModelAnomaly_Vote_Social']].sum(axis=1))\n",
    "modelVote_avg = pd.DataFrame(three_dataset_anomalies[['ModelAnomaly_Vote_Price','ModelAnomaly_Vote_Block',\n",
    "                                      'ModelAnomaly_Vote_Social']].mean(axis=1))\n",
    "\n",
    "# concat these 3 calculations, rename columns respectively, and sort it descending\n",
    "three_dataset_anomalies_calc = pd.concat([contamination_avg,modelVote_sum,modelVote_avg],axis = 1)\n",
    "three_dataset_anomalies_calc.columns = ['contamination_avg','modelVote_sum','modelVote_avg']\n",
    "three_dataset_anomalies_calc = three_dataset_anomalies_calc.set_index(three_dataset_anomalies.Date)\n",
    "three_dataset_anomalies_calc = three_dataset_anomalies_calc.sort_values(by=['contamination_avg','modelVote_sum']\n",
    "                                                                            ,ascending = (False, False))\n",
    "\n",
    "print('Nbr of common date anomalies across three datasets: ', len(three_dataset_anomalies.Date))\n",
    "three_dataset_anomalies_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Anomalies that were identified in 2 Datasets \n",
    "- here we will only make lists of dates to plot later on close price\n",
    "- there are 3 possible combinations\n",
    "    1. Price, Blockchain\n",
    "    2. Price, Social\n",
    "    3. Blockchain, Social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_block = list(set(list(price_anomalies.Date)) & set(list(block_anomalies.Date)))\n",
    "price_social = list(set(list(price_anomalies.Date)) & set(list(social_anomalies.Date)))\n",
    "block_social = list(set(list(block_anomalies.Date)) & set(list(social_anomalies.Date)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Find corresponding close price for anomaly dates\n",
    "- Find it for: same dates across 3 datasets(4.1), and combinations of 2 (4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert a list of dates and get it back with close prices\n",
    "def close_prices_anomalies(anomaly_dates):\n",
    "    close_anomalies = [] # collects close price for anomolous dates\n",
    "    # iterate over anomaly dates\n",
    "    for dates in anomaly_dates:\n",
    "        # get close price for date that mathces the anomaly date\n",
    "        close_loc = price.loc[str(dates),'close']\n",
    "        # append it to the list\n",
    "        close_anomalies.append(close_loc)\n",
    "    # make close prices and dates into a series and then concat them together\n",
    "    close_anomalies = pd.Series(close_anomalies)\n",
    "    dates1 = pd.Series(anomaly_dates) # put that in parameter\n",
    "    \n",
    "    close_anomalies_df = pd.concat([dates1,close_anomalies], axis = 1)\n",
    "    close_anomalies_df.columns = ['Date','Close'] # rename columns\n",
    "    # make Date the index (needed for plotting alter)\n",
    "    close_anomalies_df = close_anomalies_df.set_index('Date') \n",
    "    # convert dates into datetime\n",
    "    close_anomalies_df.index = pd.to_datetime(close_anomalies_df.index)\n",
    "    return close_anomalies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the functions and save resutls in the variable\n",
    "three_datasets_close = close_prices_anomalies(three_dataset_anomalies_calc.index)\n",
    "price_block_close = close_prices_anomalies(price_block)\n",
    "price_social_close = close_prices_anomalies(price_social)\n",
    "block_social_close = close_prices_anomalies(block_social)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Close price and annotate anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filter for same dates \n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "# Converting the index as date\n",
    "price.index = pd.to_datetime(price.index)\n",
    "#common = \\\n",
    "#   set.intersection(set(price.index), set(three_datasets_close.index))\n",
    "#filter1 = price[price.index.isin(common)]\n",
    "#filter1 = filter1[['close']]\n",
    "\n",
    "# Alter size for the plot\n",
    "plt.subplots(dpi=100,figsize=(16,6))\n",
    "# plot all close price data\n",
    "plt.plot(price.index, price.close,  alpha=0.5)\n",
    "# set x-axis label and specific size\n",
    "plt.xlabel('Date',size=16)\n",
    "# set y-axis label and specific size\n",
    "plt.ylabel('Close Price',size=16)\n",
    "# set plot title with specific size\n",
    "plt.title('Unique Anomalies between all Models',size=16)\n",
    "\n",
    "# plot anomalies \n",
    "plt.scatter(three_datasets_close.index, three_datasets_close.Close, color=\"red\", marker = 'H')\n",
    "plt.scatter(price_block_close.index, price_block_close.Close, color=\"blue\", marker = 'p')\n",
    "plt.scatter(price_social_close.index, price_social_close.Close, color=\"green\", marker = 'v')\n",
    "plt.scatter(block_social_close.index, block_social_close.Close, color=\"pink\", marker = 'X')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection on all 3 datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365, 95)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volumefrom</th>\n",
       "      <th>volumeto</th>\n",
       "      <th>CCI</th>\n",
       "      <th>ichimoku_leadSpanA</th>\n",
       "      <th>ichimoku_leadSpanB</th>\n",
       "      <th>MACD</th>\n",
       "      <th>...</th>\n",
       "      <th>reddit_posts_per_day</th>\n",
       "      <th>reddit_comments_per_hour</th>\n",
       "      <th>reddit_comments_per_day</th>\n",
       "      <th>code_repo_stars</th>\n",
       "      <th>code_repo_forks</th>\n",
       "      <th>code_repo_subscribers</th>\n",
       "      <th>code_repo_open_pull_issues</th>\n",
       "      <th>code_repo_closed_pull_issues</th>\n",
       "      <th>code_repo_open_issues</th>\n",
       "      <th>code_repo_closed_issues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.309506</td>\n",
       "      <td>0.323185</td>\n",
       "      <td>0.314349</td>\n",
       "      <td>0.311447</td>\n",
       "      <td>0.158511</td>\n",
       "      <td>0.165547</td>\n",
       "      <td>0.610031</td>\n",
       "      <td>0.360129</td>\n",
       "      <td>0.336818</td>\n",
       "      <td>0.567335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278695</td>\n",
       "      <td>0.196713</td>\n",
       "      <td>0.196708</td>\n",
       "      <td>0.648723</td>\n",
       "      <td>0.680061</td>\n",
       "      <td>0.737169</td>\n",
       "      <td>0.414537</td>\n",
       "      <td>0.500259</td>\n",
       "      <td>0.401739</td>\n",
       "      <td>0.542089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.173618</td>\n",
       "      <td>0.181712</td>\n",
       "      <td>0.171755</td>\n",
       "      <td>0.174608</td>\n",
       "      <td>0.120752</td>\n",
       "      <td>0.150214</td>\n",
       "      <td>0.147846</td>\n",
       "      <td>0.207782</td>\n",
       "      <td>0.236865</td>\n",
       "      <td>0.167418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169271</td>\n",
       "      <td>0.163744</td>\n",
       "      <td>0.163743</td>\n",
       "      <td>0.265907</td>\n",
       "      <td>0.266401</td>\n",
       "      <td>0.255689</td>\n",
       "      <td>0.215412</td>\n",
       "      <td>0.301799</td>\n",
       "      <td>0.244852</td>\n",
       "      <td>0.291751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.225885</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.236921</td>\n",
       "      <td>0.226146</td>\n",
       "      <td>0.077413</td>\n",
       "      <td>0.061453</td>\n",
       "      <td>0.505965</td>\n",
       "      <td>0.254712</td>\n",
       "      <td>0.192137</td>\n",
       "      <td>0.470921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175227</td>\n",
       "      <td>0.090028</td>\n",
       "      <td>0.090025</td>\n",
       "      <td>0.471224</td>\n",
       "      <td>0.511847</td>\n",
       "      <td>0.601688</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.239695</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.327409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.264223</td>\n",
       "      <td>0.276441</td>\n",
       "      <td>0.267946</td>\n",
       "      <td>0.264287</td>\n",
       "      <td>0.124178</td>\n",
       "      <td>0.117303</td>\n",
       "      <td>0.592183</td>\n",
       "      <td>0.305660</td>\n",
       "      <td>0.234630</td>\n",
       "      <td>0.598206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260633</td>\n",
       "      <td>0.149065</td>\n",
       "      <td>0.149058</td>\n",
       "      <td>0.697656</td>\n",
       "      <td>0.740560</td>\n",
       "      <td>0.814346</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.491935</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>0.547240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.386708</td>\n",
       "      <td>0.406506</td>\n",
       "      <td>0.391708</td>\n",
       "      <td>0.390215</td>\n",
       "      <td>0.211678</td>\n",
       "      <td>0.220504</td>\n",
       "      <td>0.726902</td>\n",
       "      <td>0.458545</td>\n",
       "      <td>0.416687</td>\n",
       "      <td>0.650943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331805</td>\n",
       "      <td>0.240735</td>\n",
       "      <td>0.240726</td>\n",
       "      <td>0.875716</td>\n",
       "      <td>0.915828</td>\n",
       "      <td>0.947679</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.773297</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.781104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            close        high         low        open  volumefrom    volumeto  \\\n",
       "count  365.000000  365.000000  365.000000  365.000000  365.000000  365.000000   \n",
       "mean     0.309506    0.323185    0.314349    0.311447    0.158511    0.165547   \n",
       "std      0.173618    0.181712    0.171755    0.174608    0.120752    0.150214   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.225885    0.230769    0.236921    0.226146    0.077413    0.061453   \n",
       "50%      0.264223    0.276441    0.267946    0.264287    0.124178    0.117303   \n",
       "75%      0.386708    0.406506    0.391708    0.390215    0.211678    0.220504   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "              CCI  ichimoku_leadSpanA  ichimoku_leadSpanB        MACD  ...  \\\n",
       "count  365.000000          365.000000          365.000000  365.000000  ...   \n",
       "mean     0.610031            0.360129            0.336818    0.567335  ...   \n",
       "std      0.147846            0.207782            0.236865    0.167418  ...   \n",
       "min      0.000000            0.000000            0.000000    0.000000  ...   \n",
       "25%      0.505965            0.254712            0.192137    0.470921  ...   \n",
       "50%      0.592183            0.305660            0.234630    0.598206  ...   \n",
       "75%      0.726902            0.458545            0.416687    0.650943  ...   \n",
       "max      1.000000            1.000000            1.000000    1.000000  ...   \n",
       "\n",
       "       reddit_posts_per_day  reddit_comments_per_hour  \\\n",
       "count            365.000000                365.000000   \n",
       "mean               0.278695                  0.196713   \n",
       "std                0.169271                  0.163744   \n",
       "min                0.000000                  0.000000   \n",
       "25%                0.175227                  0.090028   \n",
       "50%                0.260633                  0.149065   \n",
       "75%                0.331805                  0.240735   \n",
       "max                1.000000                  1.000000   \n",
       "\n",
       "       reddit_comments_per_day  code_repo_stars  code_repo_forks  \\\n",
       "count               365.000000       365.000000       365.000000   \n",
       "mean                  0.196708         0.648723         0.680061   \n",
       "std                   0.163743         0.265907         0.266401   \n",
       "min                   0.000000         0.000000         0.000000   \n",
       "25%                   0.090025         0.471224         0.511847   \n",
       "50%                   0.149058         0.697656         0.740560   \n",
       "75%                   0.240726         0.875716         0.915828   \n",
       "max                   1.000000         1.000000         1.000000   \n",
       "\n",
       "       code_repo_subscribers  code_repo_open_pull_issues  \\\n",
       "count             365.000000                  365.000000   \n",
       "mean                0.737169                    0.414537   \n",
       "std                 0.255689                    0.215412   \n",
       "min                 0.000000                    0.000000   \n",
       "25%                 0.601688                    0.258824   \n",
       "50%                 0.814346                    0.400000   \n",
       "75%                 0.947679                    0.541176   \n",
       "max                 1.000000                    1.000000   \n",
       "\n",
       "       code_repo_closed_pull_issues  code_repo_open_issues  \\\n",
       "count                    365.000000             365.000000   \n",
       "mean                       0.500259               0.401739   \n",
       "std                        0.301799               0.244852   \n",
       "min                        0.000000               0.000000   \n",
       "25%                        0.239695               0.211538   \n",
       "50%                        0.491935               0.365385   \n",
       "75%                        0.773297               0.602564   \n",
       "max                        1.000000               1.000000   \n",
       "\n",
       "       code_repo_closed_issues  \n",
       "count               365.000000  \n",
       "mean                  0.542089  \n",
       "std                   0.291751  \n",
       "min                   0.000000  \n",
       "25%                   0.327409  \n",
       "50%                   0.547240  \n",
       "75%                   0.781104  \n",
       "max                   1.000000  \n",
       "\n",
       "[8 rows x 95 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the 3 datasets \n",
    "merged_df = pd.concat([price,block,social], axis = 1)\n",
    "# scale them \n",
    "X = scale(merged_df)\n",
    "print(X.shape)\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  5 INLIERS :  360 ABOD\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 FB\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 HBOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 IF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  2 INLIERS :  363 Average KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 OCSVM\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 SOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 LOF\n",
      "Model: \"model_46\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_31 (InputLayer)           (None, 95)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_166 (Dense)               (None, 95)           9120        input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_167 (Dense)               (None, 128)          12288       dense_166[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)            (None, 128)          0           dense_167[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_168 (Dense)               (None, 64)           8256        dropout_91[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_92 (Dropout)            (None, 64)           0           dense_168[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_169 (Dense)               (None, 32)           2080        dropout_92[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)            (None, 32)           0           dense_169[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_170 (Dense)               (None, 2)            66          dropout_93[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_171 (Dense)               (None, 2)            66          dropout_93[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 2)            0           dense_170[0][0]                  \n",
      "                                                                 dense_171[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,876\n",
      "Trainable params: 31,876\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_173 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_174 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_176 (Dense)            (None, 95)                12255     \n",
      "=================================================================\n",
      "Total params: 22,789\n",
      "Trainable params: 22,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_31 (InputLayer)        (None, 95)                0         \n",
      "_________________________________________________________________\n",
      "model_46 (Model)             [(None, 2), (None, 2), (N 31876     \n",
      "_________________________________________________________________\n",
      "model_47 (Model)             (None, 95)                22789     \n",
      "=================================================================\n",
      "Total params: 54,665\n",
      "Trainable params: 54,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_47 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_47.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 201.1140 - val_loss: 168.9893\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 128.0586 - val_loss: 149.0081\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 113.4524 - val_loss: 140.0210\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 102.9605 - val_loss: 132.6620\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 94.8861 - val_loss: 128.7056\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 115us/step - loss: 92.5830 - val_loss: 127.7743\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 91.7504 - val_loss: 127.8181\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 108us/step - loss: 91.5240 - val_loss: 127.8586\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 91.4562 - val_loss: 127.6834\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 95us/step - loss: 91.4653 - val_loss: 127.7330\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 91us/step - loss: 91.4540 - val_loss: 127.7397\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 91.4146 - val_loss: 127.5565\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 91.4460 - val_loss: 127.7416\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 91.3811 - val_loss: 127.6641\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 91.4037 - val_loss: 127.7113\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 91.3729 - val_loss: 127.6871\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 91.3846 - val_loss: 127.7210\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3930 - val_loss: 127.6495\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 91.3455 - val_loss: 127.7318\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 91.3625 - val_loss: 127.6272\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3765 - val_loss: 127.7882\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3349 - val_loss: 127.7085\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3755 - val_loss: 127.8091\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3473 - val_loss: 127.6696\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3516 - val_loss: 127.7104\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 91.3560 - val_loss: 127.7498\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.4019 - val_loss: 127.6366\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 91.3313 - val_loss: 127.6986\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 91.3309 - val_loss: 127.6266\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3438 - val_loss: 127.6070\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 91.3559 - val_loss: 127.5783\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3221 - val_loss: 127.7324\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 91.3474 - val_loss: 127.6715\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 91.3757 - val_loss: 127.6589\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3698 - val_loss: 127.7019\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 91.3464 - val_loss: 127.5919\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3764 - val_loss: 127.6404\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3616 - val_loss: 127.7219\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 91.3209 - val_loss: 127.7053\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3381 - val_loss: 127.6031\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3516 - val_loss: 127.7416\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 91.3760 - val_loss: 127.6507\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3409 - val_loss: 127.6534\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 91.3353 - val_loss: 127.6943\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3308 - val_loss: 127.6528\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3474 - val_loss: 127.6697\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3068 - val_loss: 127.6425\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3264 - val_loss: 127.6332\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 91.3344 - val_loss: 127.6392\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 91.4110 - val_loss: 127.7332\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3166 - val_loss: 127.6219\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3414 - val_loss: 127.7287\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 91.3391 - val_loss: 127.8429\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 91.3248 - val_loss: 127.6996\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 91.4285 - val_loss: 127.5956\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.2959 - val_loss: 127.6855\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 91.3490 - val_loss: 127.6459\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3131 - val_loss: 127.7140\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 91.3828 - val_loss: 127.6200\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3151 - val_loss: 127.7943\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3352 - val_loss: 127.6876\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3016 - val_loss: 127.7766\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 91.3428 - val_loss: 127.8659\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 91.3269 - val_loss: 127.8387\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3252 - val_loss: 127.7484\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 91.2758 - val_loss: 127.8619\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 91.3725 - val_loss: 128.0477\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3719 - val_loss: 127.8517\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3098 - val_loss: 127.9117\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3401 - val_loss: 127.7143\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 91.3583 - val_loss: 127.7797\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 91.3376 - val_loss: 127.7421\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3422 - val_loss: 127.7328\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 91.3934 - val_loss: 127.6170\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 91.3621 - val_loss: 127.7743\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 91.3320 - val_loss: 127.6019\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 91.3144 - val_loss: 127.6315\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3146 - val_loss: 127.5552\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3328 - val_loss: 127.7138\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3510 - val_loss: 127.6916\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3137 - val_loss: 127.6844\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3258 - val_loss: 127.6350\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3564 - val_loss: 127.6932\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 91.3267 - val_loss: 127.7539\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3380 - val_loss: 127.7860\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 91.2994 - val_loss: 127.6910\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 91.3110 - val_loss: 127.8196\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 91.4023 - val_loss: 128.3270\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.4184 - val_loss: 127.7071\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 91.3523 - val_loss: 127.8839\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 91.3478 - val_loss: 127.6556\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 91.3385 - val_loss: 127.7605\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 91.3222 - val_loss: 127.7940\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 91.3407 - val_loss: 127.6260\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3009 - val_loss: 127.8897\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.3812 - val_loss: 127.8331\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 91.3271 - val_loss: 127.5661\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 91.3295 - val_loss: 127.7374\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 91.2917 - val_loss: 127.7497\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 91.3069 - val_loss: 127.7713\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  5 INLIERS :  360 VAE\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  10 INLIERS :  355 ABOD\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  7 INLIERS :  358 FB\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 HBOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 IF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  5 INLIERS :  360 KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  3 INLIERS :  362 Average KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 OCSVM\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 SOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  6 INLIERS :  359 LOF\n",
      "Model: \"model_49\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           (None, 95)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_177 (Dense)               (None, 95)           9120        input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_178 (Dense)               (None, 128)          12288       dense_177[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)            (None, 128)          0           dense_178[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_179 (Dense)               (None, 64)           8256        dropout_97[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 64)           0           dense_179[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_180 (Dense)               (None, 32)           2080        dropout_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)            (None, 32)           0           dense_180[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_181 (Dense)               (None, 2)            66          dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_182 (Dense)               (None, 2)            66          dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 2)            0           dense_181[0][0]                  \n",
      "                                                                 dense_182[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,876\n",
      "Trainable params: 31,876\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_34 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 95)                12255     \n",
      "=================================================================\n",
      "Total params: 22,789\n",
      "Trainable params: 22,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        (None, 95)                0         \n",
      "_________________________________________________________________\n",
      "model_49 (Model)             [(None, 2), (None, 2), (N 31876     \n",
      "_________________________________________________________________\n",
      "model_50 (Model)             (None, 95)                22789     \n",
      "=================================================================\n",
      "Total params: 54,665\n",
      "Trainable params: 54,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_50 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_50.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 172.4198 - val_loss: 144.5460\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 124.9403 - val_loss: 129.9914\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 113.1190 - val_loss: 118.9907\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 104.6038 - val_loss: 112.4133\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 98.5811 - val_loss: 105.9590\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 95.5526 - val_loss: 104.6024\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 108us/step - loss: 94.6774 - val_loss: 104.1997\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 94.3106 - val_loss: 103.8948\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 94.3481 - val_loss: 103.7467\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 96us/step - loss: 94.2436 - val_loss: 103.7979\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 94.1560 - val_loss: 103.7922\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.1737 - val_loss: 103.7365\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.1115 - val_loss: 103.7565\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 94.0842 - val_loss: 103.7212\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 94.0884 - val_loss: 103.7581\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0894 - val_loss: 103.8220\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0802 - val_loss: 103.7236\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0545 - val_loss: 103.6726\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0508 - val_loss: 103.6899\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0566 - val_loss: 103.6433\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0431 - val_loss: 103.7213\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.0952 - val_loss: 103.7142\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0415 - val_loss: 103.6806\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0489 - val_loss: 103.7237\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0556 - val_loss: 103.6832\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0386 - val_loss: 103.6640\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0547 - val_loss: 103.7111\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0561 - val_loss: 103.6434\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0337 - val_loss: 103.6718\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0463 - val_loss: 103.6658\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0410 - val_loss: 103.6715\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 94.0300 - val_loss: 103.6638\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0177 - val_loss: 103.6300\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0500 - val_loss: 103.6881\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0270 - val_loss: 103.7023\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0373 - val_loss: 103.6802\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0255 - val_loss: 103.6620\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0228 - val_loss: 103.6773\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0199 - val_loss: 103.6625\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0364 - val_loss: 103.6751\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0272 - val_loss: 103.6789\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0271 - val_loss: 103.6371\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0459 - val_loss: 103.6352\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0178 - val_loss: 103.6938\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0224 - val_loss: 103.6506\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0297 - val_loss: 103.6358\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0364 - val_loss: 103.6636\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0346 - val_loss: 103.6466\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0424 - val_loss: 103.6260\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0338 - val_loss: 103.6837\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0335 - val_loss: 103.6452\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 94.0373 - val_loss: 103.6272\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0492 - val_loss: 103.6695\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0513 - val_loss: 103.6520\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0372 - val_loss: 103.6361\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0284 - val_loss: 103.6537\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0263 - val_loss: 103.6393\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0244 - val_loss: 103.6424\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0319 - val_loss: 103.6602\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0249 - val_loss: 103.6379\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0392 - val_loss: 103.6323\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0216 - val_loss: 103.6640\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0294 - val_loss: 103.6467\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0251 - val_loss: 103.6364\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0237 - val_loss: 103.6650\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0236 - val_loss: 103.6624\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0229 - val_loss: 103.6550\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.0308 - val_loss: 103.6978\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.0248 - val_loss: 103.6480\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.0214 - val_loss: 103.6407\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0252 - val_loss: 103.6790\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.0350 - val_loss: 103.6528\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0359 - val_loss: 103.6333\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.0489 - val_loss: 103.6493\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0316 - val_loss: 103.6704\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0357 - val_loss: 103.6627\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.0326 - val_loss: 103.6200\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0245 - val_loss: 103.6565\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0496 - val_loss: 103.6385\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0385 - val_loss: 103.6095\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0197 - val_loss: 103.6647\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0410 - val_loss: 103.6581\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0361 - val_loss: 103.6499\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0234 - val_loss: 103.6466\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0275 - val_loss: 103.6445\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.0271 - val_loss: 103.6513\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.0364 - val_loss: 103.6462\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0305 - val_loss: 103.6416\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0285 - val_loss: 103.6430\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0391 - val_loss: 103.6462\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0267 - val_loss: 103.6440\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0357 - val_loss: 103.6540\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0171 - val_loss: 103.6341\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.0191 - val_loss: 103.6453\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0281 - val_loss: 103.6549\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0245 - val_loss: 103.6612\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.0324 - val_loss: 103.6479\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.0203 - val_loss: 103.6343\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.0288 - val_loss: 103.6858\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.0245 - val_loss: 103.6527\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 VAE\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  14 INLIERS :  351 ABOD\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  10 INLIERS :  355 FB\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 HBOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 IF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  7 INLIERS :  358 KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  4 INLIERS :  361 Average KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 OCSVM\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 SOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  9 INLIERS :  356 LOF\n",
      "Model: \"model_52\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_35 (InputLayer)           (None, 95)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_188 (Dense)               (None, 95)           9120        input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_189 (Dense)               (None, 128)          12288       dense_188[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)           (None, 128)          0           dense_189[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_190 (Dense)               (None, 64)           8256        dropout_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)           (None, 64)           0           dense_190[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_191 (Dense)               (None, 32)           2080        dropout_104[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, 32)           0           dense_191[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_192 (Dense)               (None, 2)            66          dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_193 (Dense)               (None, 2)            66          dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 2)            0           dense_192[0][0]                  \n",
      "                                                                 dense_193[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,876\n",
      "Trainable params: 31,876\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_36 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 95)                12255     \n",
      "=================================================================\n",
      "Total params: 22,789\n",
      "Trainable params: 22,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_35 (InputLayer)        (None, 95)                0         \n",
      "_________________________________________________________________\n",
      "model_52 (Model)             [(None, 2), (None, 2), (N 31876     \n",
      "_________________________________________________________________\n",
      "model_53 (Model)             (None, 95)                22789     \n",
      "=================================================================\n",
      "Total params: 54,665\n",
      "Trainable params: 54,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_53 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_53.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 190.9265 - val_loss: 125.1490\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 129.3091 - val_loss: 112.3281\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 118.2768 - val_loss: 104.8602\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 109.0500 - val_loss: 97.5324\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 100.3369 - val_loss: 92.1590\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 96.9437 - val_loss: 90.0257\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 95.9466 - val_loss: 90.0984\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 114us/step - loss: 95.8494 - val_loss: 89.9976\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 95.7871 - val_loss: 89.8579\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 94us/step - loss: 95.7477 - val_loss: 89.9160\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 91us/step - loss: 95.7147 - val_loss: 89.8441\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.6720 - val_loss: 89.7925\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.6838 - val_loss: 89.7990\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6040 - val_loss: 89.7762\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.6309 - val_loss: 89.8135\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6417 - val_loss: 89.7670\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6422 - val_loss: 89.7926\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.6643 - val_loss: 89.8749\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.6205 - val_loss: 89.7672\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.6297 - val_loss: 89.8118\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.6344 - val_loss: 89.8149\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6338 - val_loss: 89.8168\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6306 - val_loss: 89.8167\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6023 - val_loss: 89.7629\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6128 - val_loss: 89.7947\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6223 - val_loss: 89.7923\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.5896 - val_loss: 89.7894\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6240 - val_loss: 89.7472\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.5964 - val_loss: 89.7519\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6037 - val_loss: 89.7822\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6221 - val_loss: 89.7851\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6262 - val_loss: 89.7808\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5870 - val_loss: 89.7685\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5983 - val_loss: 89.7522\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6121 - val_loss: 89.7746\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5616 - val_loss: 89.7456\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6033 - val_loss: 89.7769\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.6176 - val_loss: 89.7673\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6030 - val_loss: 89.7818\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6065 - val_loss: 89.8204\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5886 - val_loss: 89.7943\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6321 - val_loss: 89.8129\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6255 - val_loss: 89.8106\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5816 - val_loss: 89.7530\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5753 - val_loss: 89.8254\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.6112 - val_loss: 89.7998\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.5681 - val_loss: 89.8712\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6547 - val_loss: 89.7880\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.6265 - val_loss: 89.7844\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6118 - val_loss: 89.7446\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6004 - val_loss: 89.7468\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6197 - val_loss: 89.7243\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5943 - val_loss: 89.7702\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.6012 - val_loss: 89.7388\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.6172 - val_loss: 89.7669\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6096 - val_loss: 89.7590\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5817 - val_loss: 89.7465\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6133 - val_loss: 89.7686\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5949 - val_loss: 89.7786\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6004 - val_loss: 89.7699\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5765 - val_loss: 89.7686\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6239 - val_loss: 89.7931\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5933 - val_loss: 89.7922\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 95.5918 - val_loss: 89.7923\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.5912 - val_loss: 89.7821\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6239 - val_loss: 89.7800\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5960 - val_loss: 89.7819\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6119 - val_loss: 89.7902\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.5889 - val_loss: 89.7711\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.6077 - val_loss: 89.7882\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5953 - val_loss: 89.7734\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.5905 - val_loss: 89.8206\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6092 - val_loss: 89.7775\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5852 - val_loss: 89.7912\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6020 - val_loss: 89.7732\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5944 - val_loss: 89.7399\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.6191 - val_loss: 89.7412\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6219 - val_loss: 89.7471\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5979 - val_loss: 89.7684\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6061 - val_loss: 89.7521\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5911 - val_loss: 89.7666\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.6119 - val_loss: 89.7700\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.5938 - val_loss: 89.7573\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6055 - val_loss: 89.7822\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6035 - val_loss: 89.7492\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5973 - val_loss: 89.7517\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 95.6001 - val_loss: 89.7647\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5942 - val_loss: 89.7645\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6119 - val_loss: 89.7628\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5916 - val_loss: 89.7807\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5998 - val_loss: 89.7712\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6153 - val_loss: 89.7887\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5908 - val_loss: 89.7964\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5990 - val_loss: 89.8030\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.6068 - val_loss: 89.8084\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.5892 - val_loss: 89.7851\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6047 - val_loss: 89.8093\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5600 - val_loss: 89.8567\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5967 - val_loss: 89.8461\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.5985 - val_loss: 89.8412\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 VAE\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  18 INLIERS :  347 ABOD\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  12 INLIERS :  353 FB\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 HBOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 IF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  11 INLIERS :  354 KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  4 INLIERS :  361 Average KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 OCSVM\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 SOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  13 INLIERS :  352 LOF\n",
      "Model: \"model_55\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_37 (InputLayer)           (None, 95)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_199 (Dense)               (None, 95)           9120        input_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_200 (Dense)               (None, 128)          12288       dense_199[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)           (None, 128)          0           dense_200[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_201 (Dense)               (None, 64)           8256        dropout_109[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)           (None, 64)           0           dense_201[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_202 (Dense)               (None, 32)           2080        dropout_110[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)           (None, 32)           0           dense_202[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_203 (Dense)               (None, 2)            66          dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_204 (Dense)               (None, 2)            66          dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 2)            0           dense_203[0][0]                  \n",
      "                                                                 dense_204[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,876\n",
      "Trainable params: 31,876\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_38 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_205 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_206 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_207 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_208 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 95)                12255     \n",
      "=================================================================\n",
      "Total params: 22,789\n",
      "Trainable params: 22,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_37 (InputLayer)        (None, 95)                0         \n",
      "_________________________________________________________________\n",
      "model_55 (Model)             [(None, 2), (None, 2), (N 31876     \n",
      "_________________________________________________________________\n",
      "model_56 (Model)             (None, 95)                22789     \n",
      "=================================================================\n",
      "Total params: 54,665\n",
      "Trainable params: 54,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_56 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_56.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 184.5029 - val_loss: 125.3141\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 111us/step - loss: 126.6926 - val_loss: 109.0433\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 111us/step - loss: 112.5516 - val_loss: 100.7934\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 102.9088 - val_loss: 94.3222\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 114us/step - loss: 97.7263 - val_loss: 94.0849\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 95.8525 - val_loss: 93.8703\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 110us/step - loss: 95.4151 - val_loss: 93.6245\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 95.3412 - val_loss: 93.7695\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 125us/step - loss: 95.3158 - val_loss: 93.8818\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 95us/step - loss: 95.3462 - val_loss: 93.7250\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 91us/step - loss: 95.2303 - val_loss: 93.7773\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.2295 - val_loss: 93.7130\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.2264 - val_loss: 93.6690\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.2271 - val_loss: 93.6942\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.2041 - val_loss: 93.7261\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1850 - val_loss: 93.7315\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1791 - val_loss: 93.6907\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1579 - val_loss: 93.7032\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.2106 - val_loss: 93.6476\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.1444 - val_loss: 93.7476\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.1783 - val_loss: 93.6214\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.1504 - val_loss: 93.6526\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1624 - val_loss: 93.6543\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1811 - val_loss: 93.6746\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1710 - val_loss: 93.6936\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.1631 - val_loss: 93.6478\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.1642 - val_loss: 93.6775\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1781 - val_loss: 93.7411\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.1801 - val_loss: 93.6706\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1799 - val_loss: 93.6979\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1707 - val_loss: 93.6852\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1620 - val_loss: 93.6862\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1631 - val_loss: 93.6927\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1447 - val_loss: 93.6634\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1591 - val_loss: 93.6995\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1496 - val_loss: 93.6812\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1506 - val_loss: 93.6842\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1638 - val_loss: 93.7028\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1635 - val_loss: 93.6779\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.1490 - val_loss: 93.6975\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1481 - val_loss: 93.6986\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1642 - val_loss: 93.6872\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1487 - val_loss: 93.7033\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.1515 - val_loss: 93.6878\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1511 - val_loss: 93.6830\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1569 - val_loss: 93.6723\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.1323 - val_loss: 93.6816\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1628 - val_loss: 93.6962\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.1600 - val_loss: 93.6867\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1554 - val_loss: 93.6783\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1594 - val_loss: 93.6957\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1536 - val_loss: 93.6898\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1531 - val_loss: 93.6898\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.1521 - val_loss: 93.6870\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1470 - val_loss: 93.6927\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1589 - val_loss: 93.6984\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.1529 - val_loss: 93.7025\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1529 - val_loss: 93.6897\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1525 - val_loss: 93.6870\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1535 - val_loss: 93.6935\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1469 - val_loss: 93.6954\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.1474 - val_loss: 93.6917\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1557 - val_loss: 93.6917\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1543 - val_loss: 93.6990\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1494 - val_loss: 93.6978\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1534 - val_loss: 93.6938\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.1505 - val_loss: 93.6940\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.1461 - val_loss: 93.6946\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.1449 - val_loss: 93.6939\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1497 - val_loss: 93.6919\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1472 - val_loss: 93.6876\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1556 - val_loss: 93.6844\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1473 - val_loss: 93.6922\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.1466 - val_loss: 93.6828\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1480 - val_loss: 93.6846\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1508 - val_loss: 93.6902\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1448 - val_loss: 93.6995\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1552 - val_loss: 93.6696\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1474 - val_loss: 93.6942\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1527 - val_loss: 93.6848\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1456 - val_loss: 93.6891\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1516 - val_loss: 93.6841\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1570 - val_loss: 93.6860\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1482 - val_loss: 93.6898\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1535 - val_loss: 93.6892\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.1491 - val_loss: 93.6952\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1511 - val_loss: 93.6945\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.1481 - val_loss: 93.6871\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.1479 - val_loss: 93.6857\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.1499 - val_loss: 93.6939\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1509 - val_loss: 93.6930\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1564 - val_loss: 93.6890\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.1518 - val_loss: 93.6939\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1506 - val_loss: 93.6911\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 95.1462 - val_loss: 93.6920\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1537 - val_loss: 93.6887\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.1516 - val_loss: 93.6965\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.1498 - val_loss: 93.6908\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.1476 - val_loss: 93.6948\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.1498 - val_loss: 93.6930\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 VAE\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  21 INLIERS :  344 ABOD\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  18 INLIERS :  347 FB\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 HBOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 IF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  12 INLIERS :  353 KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  4 INLIERS :  361 Average KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 OCSVM\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 SOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  17 INLIERS :  348 LOF\n",
      "Model: \"model_58\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_39 (InputLayer)           (None, 95)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_210 (Dense)               (None, 95)           9120        input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_211 (Dense)               (None, 128)          12288       dense_210[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_115 (Dropout)           (None, 128)          0           dense_211[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_212 (Dense)               (None, 64)           8256        dropout_115[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_116 (Dropout)           (None, 64)           0           dense_212[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_213 (Dense)               (None, 32)           2080        dropout_116[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_117 (Dropout)           (None, 32)           0           dense_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_214 (Dense)               (None, 2)            66          dropout_117[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_215 (Dense)               (None, 2)            66          dropout_117[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 2)            0           dense_214[0][0]                  \n",
      "                                                                 dense_215[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,876\n",
      "Trainable params: 31,876\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_40 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_218 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_219 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_220 (Dense)            (None, 95)                12255     \n",
      "=================================================================\n",
      "Total params: 22,789\n",
      "Trainable params: 22,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_39 (InputLayer)        (None, 95)                0         \n",
      "_________________________________________________________________\n",
      "model_58 (Model)             [(None, 2), (None, 2), (N 31876     \n",
      "_________________________________________________________________\n",
      "model_59 (Model)             (None, 95)                22789     \n",
      "=================================================================\n",
      "Total params: 54,665\n",
      "Trainable params: 54,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_59 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_59.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 188.9680 - val_loss: 97.1623\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 131.6875 - val_loss: 84.4780\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 110us/step - loss: 119.2679 - val_loss: 75.6607\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 125us/step - loss: 108.8292 - val_loss: 65.3516\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 110us/step - loss: 101.8323 - val_loss: 62.6148\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 99.5158 - val_loss: 61.6615\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 99.2086 - val_loss: 61.5472\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 99.0589 - val_loss: 61.4632\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 98.9991 - val_loss: 61.2825\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 108us/step - loss: 98.9557 - val_loss: 61.2810\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.9711 - val_loss: 61.1932\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.9121 - val_loss: 61.1815\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8968 - val_loss: 61.1955\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8571 - val_loss: 61.2188\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8855 - val_loss: 61.1414\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8659 - val_loss: 61.1128\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 98.8557 - val_loss: 61.1274\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8333 - val_loss: 61.1343\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8434 - val_loss: 61.1297\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8495 - val_loss: 61.0614\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 98.8240 - val_loss: 61.1121\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8455 - val_loss: 61.1074\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8520 - val_loss: 61.0780\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8580 - val_loss: 61.0946\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8373 - val_loss: 61.0531\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 98.8389 - val_loss: 61.0844\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 98.8571 - val_loss: 61.0804\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8449 - val_loss: 61.0975\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8358 - val_loss: 61.1011\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8337 - val_loss: 61.0730\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 98.8497 - val_loss: 61.0617\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8422 - val_loss: 61.0740\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8375 - val_loss: 61.1059\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 98.8398 - val_loss: 61.0889\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 98.8472 - val_loss: 61.0754\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8525 - val_loss: 61.0776\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8279 - val_loss: 61.0657\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8457 - val_loss: 61.0775\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 98.8390 - val_loss: 61.0748\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8370 - val_loss: 61.0785\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8334 - val_loss: 61.0747\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8323 - val_loss: 61.0619\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8313 - val_loss: 61.0618\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8408 - val_loss: 61.0683\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8260 - val_loss: 61.0675\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8390 - val_loss: 61.0698\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8331 - val_loss: 61.0771\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8267 - val_loss: 61.0768\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8406 - val_loss: 61.0650\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8336 - val_loss: 61.0864\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8226 - val_loss: 61.0796\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8364 - val_loss: 61.0719\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8297 - val_loss: 61.0591\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8330 - val_loss: 61.0683\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8343 - val_loss: 61.0732\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8357 - val_loss: 61.0699\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8322 - val_loss: 61.0715\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8252 - val_loss: 61.0771\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 98.8231 - val_loss: 61.0690\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8359 - val_loss: 61.0865\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8334 - val_loss: 61.0752\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 98.8316 - val_loss: 61.0862\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 98.8454 - val_loss: 61.0679\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8210 - val_loss: 61.0716\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8301 - val_loss: 61.0651\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8288 - val_loss: 61.0703\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8342 - val_loss: 61.0608\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8363 - val_loss: 61.0669\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8346 - val_loss: 61.0699\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8250 - val_loss: 61.0629\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8223 - val_loss: 61.0706\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8279 - val_loss: 61.0826\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8430 - val_loss: 61.0897\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8273 - val_loss: 61.0815\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 98.8339 - val_loss: 61.0587\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8369 - val_loss: 61.0595\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 98.8320 - val_loss: 61.0605\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8354 - val_loss: 61.0643\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8306 - val_loss: 61.0555\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 98.8291 - val_loss: 61.0615\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 98.8374 - val_loss: 61.0513\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8264 - val_loss: 61.0564\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 98.8322 - val_loss: 61.0673\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 98.8238 - val_loss: 61.0601\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 98.8322 - val_loss: 61.0793\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 98.8314 - val_loss: 61.0695\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8338 - val_loss: 61.0715\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8166 - val_loss: 61.0760\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8319 - val_loss: 61.0862\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8178 - val_loss: 61.0651\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8284 - val_loss: 61.0788\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8460 - val_loss: 61.0841\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8389 - val_loss: 61.0697\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 98.8396 - val_loss: 61.0566\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 98.8397 - val_loss: 61.0616\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 98.8305 - val_loss: 61.0577\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 98.8287 - val_loss: 61.0644\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 98.8270 - val_loss: 61.0574\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 98.8336 - val_loss: 61.0582\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 98.8325 - val_loss: 61.0634\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 VAE\n"
     ]
    }
   ],
   "source": [
    "# time the code\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "# empty list to store outliers and inliners\n",
    "df_outliers = []\n",
    "scores_pred_lst = []\n",
    "proba_lst = []\n",
    "predict_rank_lst = []\n",
    "\n",
    "# iterate over different outlier fractions \n",
    "for fraction in outliers_fraction:\n",
    "    classifiers = {#'AE' : AutoEncoder(contamination=outliers_fraction, random_state=random_state), # check why error\n",
    "               'ABOD': ABOD(contamination = fraction),\n",
    "              'CBLOF':CBLOF(check_estimator=False, random_state=random_state), # getting 0 \n",
    "              'FB':FeatureBagging(LOF(n_neighbors=35),contamination=fraction,check_estimator=False,random_state=random_state),\n",
    "              'HBOS': HBOS(contamination=fraction),\n",
    "              'IF': IForest(contamination=fraction,random_state=random_state),\n",
    "              'KNN': KNN(contamination=fraction),\n",
    "              'Average KNN': KNN(method='mean',contamination=fraction),\n",
    "              'OCSVM' : OCSVM(contamination=fraction),\n",
    "              'SOS' : SOS(contamination=fraction),\n",
    "              'LOF': LOF(n_neighbors=35, contamination = fraction),\n",
    "              #'LOCI': LOCI(contamination = fraction), # takes a super long time + don't understand\n",
    "              'VAE' : VAE(contamination=fraction, random_state=random_state) # default nbr of neurons too much for blockchain dataset\n",
    "              }\n",
    "\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        clf.fit(X)\n",
    "        # predict raw anomaly score\n",
    "        scores_pred = clf.decision_function(X) * -1 # WHY? * -1\n",
    "\n",
    "        # prediction of a datapoint category outlier or inlier\n",
    "        y_pred = clf.predict(X)\n",
    "\n",
    "        # probability of outlier \n",
    "        probability = clf.predict_proba(X, method = 'linear')\n",
    "\n",
    "        # predict rank\n",
    "        predict_rank = clf._predict_rank(X)\n",
    "\n",
    "        n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "        n_outliers = np.count_nonzero(y_pred == 1)\n",
    "\n",
    "        # copy of dataframe\n",
    "        dfx = X.copy()\n",
    "\n",
    "        dfx[clf_name] = y_pred.tolist()\n",
    "        #print(clf_name,'\\n',i,clf,'\\n',classifiers,'\\n',classifiers.items())\n",
    "        print('Outlier fraction: ', fraction)\n",
    "        print('OUTLIERS : ',n_outliers,'INLIERS : ',n_inliers, clf_name)\n",
    "        #print('scores_pred: ', scores_pred)\n",
    "        scores_pred_lst.append(scores_pred)\n",
    "        df_outliers.append(dfx[clf_name]) \n",
    "        proba_lst.append(probability)\n",
    "        predict_rank_lst.append(predict_rank)\n",
    "t1 = time.time()\n",
    "total = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run:  44.128012895584106 s\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "print('Time to run: ',total,'s')\n",
    "# we have 50 lists (or essentially 5 with 10 models)\n",
    "print(len(df_outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the outliers lists uniquely for later\n",
    "allthree_df_s = df_outliers\n",
    "# probability lists\n",
    "allthree_proba_s = proba_lst\n",
    "# rank lists\n",
    "allthree_rank_s = predict_rank_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Seperate models per contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack lists based on outlier fraction e.g. first 10 = 0.01 outlier fraction\n",
    "frac_oo1 = df_outliers[0:11]\n",
    "frac_oo2 = df_outliers[11:22]\n",
    "frac_oo3 = df_outliers[22:33]\n",
    "frac_oo4 = df_outliers[33:44]\n",
    "frac_oo5 = df_outliers[44:]\n",
    "\n",
    "# use the function to extract the dates\n",
    "frac_oo1_outliers = outlier_subset_function(frac_oo1)\n",
    "frac_oo2_outliers = outlier_subset_function(frac_oo2)\n",
    "frac_oo3_outliers = outlier_subset_function(frac_oo3)\n",
    "frac_oo4_outliers = outlier_subset_function(frac_oo4)\n",
    "frac_oo5_outliers = outlier_subset_function(frac_oo5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Get unique dates per anomaly contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction 0.01 unique dates:  37\n",
      "Outlier fraction 0.02 unique dates:  38\n",
      "Outlier fraction 0.03 unique dates:  40\n",
      "Outlier fraction 0.04 unique dates:  44\n",
      "Outlier fraction 0.05 unique dates:  48\n"
     ]
    }
   ],
   "source": [
    "dates_per_fraction(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Anomaly likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Anomaly Dates:  48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AllContaminations_Vote</th>\n",
       "      <th>ModelAnomaly_Vote</th>\n",
       "      <th>weight_anomaly_models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>0.060870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>0.053913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.052174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.038261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>0.036522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.034783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-10</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.034783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.033043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.033043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-11-24</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.033043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.033043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.031304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.027826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.026087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.026087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.024348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.024348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.022609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.022609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.020870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.020870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.019130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.015652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.013913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.012174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.012174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.012174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.012174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.010435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.015652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2018-10-25</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2018-04-23</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2018-10-28</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2018-08-20</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2018-10-24</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2018-10-27</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2018-10-29</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2018-10-23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2018-02-14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  AllContaminations_Vote  ModelAnomaly_Vote  \\\n",
       "4   2018-04-08                       5                 35   \n",
       "36  2018-01-16                       5                 31   \n",
       "30  2018-01-17                       5                 30   \n",
       "27  2018-01-02                       5                 25   \n",
       "17  2018-01-05                       5                 23   \n",
       "20  2018-01-01                       5                 22   \n",
       "16  2018-01-09                       5                 21   \n",
       "1   2018-02-01                       5                 20   \n",
       "2   2018-09-10                       5                 20   \n",
       "3   2018-01-06                       5                 19   \n",
       "19  2018-01-07                       5                 19   \n",
       "29  2018-11-24                       5                 19   \n",
       "32  2018-04-09                       5                 19   \n",
       "8   2018-01-03                       5                 18   \n",
       "6   2018-01-08                       5                 16   \n",
       "5   2018-12-31                       5                 15   \n",
       "34  2018-01-04                       5                 15   \n",
       "10  2018-01-10                       5                 14   \n",
       "18  2018-12-30                       5                 14   \n",
       "11  2018-12-28                       5                 13   \n",
       "26  2018-12-29                       5                 13   \n",
       "15  2018-01-11                       5                 12   \n",
       "24  2018-01-12                       5                 12   \n",
       "13  2018-11-26                       5                 11   \n",
       "9   2018-01-15                       5                  9   \n",
       "21  2018-01-19                       5                  8   \n",
       "7   2018-01-20                       5                  7   \n",
       "12  2018-01-13                       5                  7   \n",
       "28  2018-01-22                       5                  7   \n",
       "31  2018-01-14                       5                  7   \n",
       "23  2018-01-18                       5                  6   \n",
       "0   2018-01-30                       5                  5   \n",
       "14  2018-01-24                       5                  5   \n",
       "22  2018-01-31                       5                  5   \n",
       "25  2018-01-23                       5                  5   \n",
       "33  2018-01-21                       5                  5   \n",
       "35  2018-01-25                       5                  5   \n",
       "37  2018-12-27                       4                  9   \n",
       "38  2018-10-26                       3                  5   \n",
       "39  2018-10-25                       3                  5   \n",
       "42  2018-04-23                       2                  4   \n",
       "40  2018-10-28                       2                  3   \n",
       "41  2018-08-20                       2                  3   \n",
       "43  2018-10-24                       2                  3   \n",
       "45  2018-10-27                       1                  2   \n",
       "47  2018-10-29                       1                  2   \n",
       "44  2018-10-23                       1                  1   \n",
       "46  2018-02-14                       1                  1   \n",
       "\n",
       "    weight_anomaly_models  \n",
       "4                0.060870  \n",
       "36               0.053913  \n",
       "30               0.052174  \n",
       "27               0.043478  \n",
       "17               0.040000  \n",
       "20               0.038261  \n",
       "16               0.036522  \n",
       "1                0.034783  \n",
       "2                0.034783  \n",
       "3                0.033043  \n",
       "19               0.033043  \n",
       "29               0.033043  \n",
       "32               0.033043  \n",
       "8                0.031304  \n",
       "6                0.027826  \n",
       "5                0.026087  \n",
       "34               0.026087  \n",
       "10               0.024348  \n",
       "18               0.024348  \n",
       "11               0.022609  \n",
       "26               0.022609  \n",
       "15               0.020870  \n",
       "24               0.020870  \n",
       "13               0.019130  \n",
       "9                0.015652  \n",
       "21               0.013913  \n",
       "7                0.012174  \n",
       "12               0.012174  \n",
       "28               0.012174  \n",
       "31               0.012174  \n",
       "23               0.010435  \n",
       "0                0.008696  \n",
       "14               0.008696  \n",
       "22               0.008696  \n",
       "25               0.008696  \n",
       "33               0.008696  \n",
       "35               0.008696  \n",
       "37               0.015652  \n",
       "38               0.008696  \n",
       "39               0.008696  \n",
       "42               0.006957  \n",
       "40               0.005217  \n",
       "41               0.005217  \n",
       "43               0.005217  \n",
       "45               0.003478  \n",
       "47               0.003478  \n",
       "44               0.001739  \n",
       "46               0.001739  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allthree_anomalies = date_ranking(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)\n",
    "print('Unique Anomaly Dates: ',len(allthree_anomalies.Date.unique()))\n",
    "allthree_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Borderline probability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.03208341e-04, 9.99596792e-01],\n",
       "       [3.44879937e-03, 9.96551201e-01],\n",
       "       [1.79113641e-02, 9.82088636e-01],\n",
       "       [5.98103490e-03, 9.94018965e-01],\n",
       "       [2.70745040e-03, 9.97292550e-01],\n",
       "       [1.62647169e-03, 9.98373528e-01],\n",
       "       [1.31880221e-02, 9.86811978e-01],\n",
       "       [5.48687835e-03, 9.94513122e-01],\n",
       "       [4.57138893e-03, 9.95428611e-01],\n",
       "       [6.37301930e-03, 9.93626981e-01]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allthree_proba_s[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "allthree_datasets_close = close_prices_anomalies(allthree_anomalies.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Converting the index as date\n",
    "price.index = pd.to_datetime(price.index)\n",
    "common = \\\n",
    "   set.intersection(set(price.index), set(allthree_datasets_close.index))\n",
    "filter1 = price[price.index.isin(common)]\n",
    "filter1 = filter1[['close']]\n",
    "\n",
    "# Alter size for the plot\n",
    "plt.subplots(dpi=100,figsize=(16,6))\n",
    "# plot all close price data\n",
    "plt.plot(price.index, price.close,  alpha=0.5)\n",
    "# set x-axis label and specific size\n",
    "plt.xlabel('Date',size=16)\n",
    "# set y-axis label and specific size\n",
    "plt.ylabel('Close Price',size=16)\n",
    "# set plot title with specific size\n",
    "plt.title('Unique Anomalies between all Models',size=16)\n",
    "\n",
    "# plot anomalies \n",
    "plt.scatter(filter1.index, filter1.close, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice into 365 - 1\n",
    "# test with 1 model \n",
    "for idx, row in X.iterrows(): # that row is the thing to test\n",
    "    # remainder user for training, remove row from dataframe and that row becomes your testing in that round\n",
    "    X_pred = 0\n",
    "    X_test = row\n",
    "    X_train = X.drop(idx,axis=0)\n",
    "    \n",
    "    #clf = IForest(contamination=0.05,random_state=123)\n",
    "    clf = KNN(method='mean',contamination=0.03)\n",
    "    clf.fit(X_train)\n",
    "    \n",
    "    X_test = X_test.values.reshape(1,-1)\n",
    "    \n",
    "    X_pred = clf.predict(X_test)\n",
    "    print(X_pred)\n",
    "    # save everything after each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all close price values \n",
    "close_anomalies = []\n",
    "for dates in allthree_anomalies['Date']:\n",
    "    # get close price \n",
    "    d = X.loc[str(dates),'close']\n",
    "    close_anomalies.append(d)\n",
    "close_anomalies = pd.Series(close_anomalies)\n",
    "table_dates = allthree_anomalies['Date']\n",
    "anomalies = pd.concat([table_dates,close_anomalies], axis = 1)\n",
    "anomalies.columns = ['Date','Close']\n",
    "anomalies.Date = pd.to_datetime(anomalies.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filter for same dates \n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "common = \\\n",
    "    set.intersection(set(X.index), set(anomalies.Date))\n",
    "filter1 = X[X.index.isin(common)]\n",
    "filter1 = filter1[['close']]\n",
    "\n",
    "# Converting the index as date\n",
    "X.index = pd.to_datetime(X.index)\n",
    "# Alter size for the plot\n",
    "plt.subplots(dpi=100,figsize=(16,6))\n",
    "# plot all close price data\n",
    "plt.plot(X.index, X.close,  alpha=0.5)\n",
    "# set x-axis label and specific size\n",
    "plt.xlabel('Date',size=16)\n",
    "# set y-axis label and specific size\n",
    "plt.ylabel('Close Price',size=16)\n",
    "# set plot title with specific size\n",
    "plt.title('Unique Anomalies between all Models',size=16)\n",
    "\n",
    "# plot anomalies \n",
    "plt.scatter(filter1.index, filter1.close, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_prices_anomalies(anomaly_df, main_df):\n",
    "    # getting all close price values \n",
    "    close_anomalies = []\n",
    "    for dates in anomaly_df['Date']:\n",
    "        # get close price \n",
    "        d = main_df.loc[str(dates),'close']\n",
    "        close_anomalies.append(d)\n",
    "    close_anomalies = pd.Series(close_anomalies)\n",
    "    table_dates = allthree_anomalies['Date']\n",
    "    anomalies = pd.concat([table_dates,close_anomalies], axis = 1)\n",
    "    anomalies.columns = ['Date','Close']\n",
    "    anomalies.Date = pd.to_datetime(anomalies.Date)\n",
    "    return anomalies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies(main_df, anomaly_df):\n",
    "    common = \\\n",
    "    set.intersection(set(main_df.index), set(anomaly_df.Date))\n",
    "    filter1 = main_df[main_df.index.isin(common)]\n",
    "    filter1 = filter1[['close']]\n",
    "\n",
    "    # Converting the index as date\n",
    "    main_df.index = pd.to_datetime(main_df.index)\n",
    "    # Alter size for the plot\n",
    "    plt.subplots(dpi=100,figsize=(16,6))\n",
    "    # plot all close price data\n",
    "    plt.plot(main_df.index, main_df.close,  alpha=0.5)\n",
    "    # set x-axis label and specific size\n",
    "    plt.xlabel('Date',size=16)\n",
    "    # set y-axis label and specific size\n",
    "    plt.ylabel('Close Price',size=16)\n",
    "    # set plot title with specific size\n",
    "    plt.title('Unique Anomalies between all Models',size=16)\n",
    "\n",
    "    # plot anomalies \n",
    "    plt.scatter(filter1.index, filter1.close, color=\"red\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_anomalies(X,anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it would be interesting to see if there are any \"signals\" for data identified as an anomaly right before price **spikes** or **drops**\n",
    "- I would say major anomalies were flagged but some are still missing (visially) granted only 5 models were used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green color for those voted by 2 \n",
    "# different colors for years "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Be Continued "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outlier_proba = np.transpose(pd.DataFrame(probability)).T\n",
    "outlier_proba.index = X.index\n",
    "#outlier_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Date Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "btc_outliers = np.transpose(pd.DataFrame(df_outliers))\n",
    "#btc_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Rank Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_rank_df = np.transpose(pd.DataFrame(predict_rank_lst))\n",
    "pred_rank_df.columns = classifiers.keys()\n",
    "pred_rank_df.index = merged_df.index\n",
    "#pred_rank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersection Between Merge and different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of outliers identified per method \n",
    "outlier_dates = [] # store dates here \n",
    "for col in btc_outliers.columns:\n",
    "    outliers = btc_outliers[btc_outliers[col] == 1] # subset outlier data\n",
    "    #df_outliers.timestamp.apply(lambda x: x.strftime('%Y-%m-%d')) # datetime to string \n",
    "    outlier_dates.append(outliers.index) # append the string/date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nbr of common anomalies among all models: ',len(intersection(outlier_dates[0],outlier_dates[1],outlier_dates[2],\n",
    "        outlier_dates[3],outlier_dates[4],outlier_dates[5],outlier_dates[6])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBLOF intersection dates across tests (arbitrary model check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in three[3] if x in prices_df[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices and blockchain\n",
    "intersection(prices_df[1], block_df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection(prices_df[1], social_df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_knn = list(btc_outliers[btc_outliers['K Nearest Neighbors (KNN)']==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "pca = PCA(n_components=3)  # Reduce to k=3 dimensions\n",
    "X_reduce = pca.fit_transform(X)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_zlabel(\"x_composite_3\")\n",
    "# Plot the compressed data points\n",
    "ax.scatter(X_reduce[:, 0], X_reduce[:, 1], zs=X_reduce[:, 2], s=4, lw=1, label=\"inliers\",c=\"green\")\n",
    "# Plot x's for the ground truth outliers\n",
    "ax.scatter(X_reduce[dates_knn,0],X_reduce[dates_knn,1], X_reduce[dates_knn,2],\n",
    "           lw=2, s=60, marker=\"x\", c=\"red\", label=\"outliers\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN\n",
    "\n",
    "clf_name = 'KNN'\n",
    "clf = KNN(contamination=0.05)\n",
    "clf.fit(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict raw anomaly score \n",
    "# np.set_printoptions(precision=None)\n",
    "\n",
    "#clf.decision_scores_\n",
    "\n",
    "scores_pred = clf.decision_function(X) \n",
    "print(scores_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction on the test data\n",
    "y_pred = clf.predict(X)  # outlier labels (0 or 1)\n",
    "print(y_pred.shape)\n",
    "n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "n_outliers = np.count_nonzero(y_pred == 1)\n",
    "print('OUTLIERS : ',n_outliers,'INLIERS : ',n_inliers, clf_name)\n",
    "# threshold value to consider a datapoint inlier or outlier\n",
    "threshold = stats.scoreatpercentile(scores_pred,100 * 0.05)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# evaluate and print the results\n",
    "print(\"\\nOn Training Data:\")\n",
    "evaluate_print(clf_name, y_train, y_train_scores)\n",
    "print(\"\\nOn Test Data:\")\n",
    "evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "visualize(clf_name, X_train, y_train, X_test, y_test, y_train_pred,\n",
    "          y_test_pred, show_figure=True, save_figure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
