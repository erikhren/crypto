{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Detect anomalies on seperate 3 seperate datasets \n",
    "    - Use different anomaly thresholds (1% -5%) \n",
    "    - And compare/verify similar dates and relationship \n",
    "    - Create a ranking/voting system for \"popular\" anomaly dates \n",
    "    - Additionally use Leave-one out method\n",
    "2. Same but merge 3 datasets \n",
    "3. Create labels from anomalies and run supervised models\n",
    "4. Optimization\n",
    "5. Build a website "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/in-12-minutes-stocks-analysis-with-pandas-and-scikit-learn-a8d8a7b50ee7\n",
    "https://stackoverflow.com/questions/47211866/how-to-mark-specific-data-points-in-matplotlib-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Import all models\n",
    "import sklearn\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.feature_bagging import FeatureBagging\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.loci import LOCI\n",
    "from pyod.models.lscp import LSCP\n",
    "from pyod.models.mcd import MCD\n",
    "from pyod.models.mo_gaal import MO_GAAL\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.sos import SOS\n",
    "from pyod.models.vae import VAE\n",
    "# from pyod.models.xgbod import XGBOD \n",
    "\n",
    "from scipy import stats\n",
    "pd.options.display.max_rows = 365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on each Dataset Seperetly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['close', 'high', 'low', 'open', 'volumefrom', 'volumeto', 'CCI',\n",
      "       'ichimoku_leadSpanA', 'ichimoku_leadSpanB', 'MACD', 'MACD_diff',\n",
      "       'MACD_signal', 'AwesomeOscillator', 'KAMA', 'RateofChange', 'RSI_7',\n",
      "       'RSI_14', 'StochasticOscillator', 'stoch_signal', 'TrueStrenghtIndex',\n",
      "       'UltimateOscilator', 'williamsR', 'Accum_Distrubution', 'ForceIndex',\n",
      "       'AvgTrueRange', 'BollingerHighBand', 'BHB_indicator',\n",
      "       'BollingerLowBand', 'BLB_indicator', 'BC_middleBand', 'BC_percBand',\n",
      "       'BCB_width', 'sma_5', 'sma_10', 'sma_20', 'sma_30', 'sma_50', 'sma_100',\n",
      "       'sma_200', 'ema_5', 'ema_10', 'ema_20', 'ema_30', 'ema_50', 'ema_100',\n",
      "       'ema_200', 'vwma', 'hull_Moving', 'Mkt_Cap'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volumefrom</th>\n",
       "      <th>volumeto</th>\n",
       "      <th>CCI</th>\n",
       "      <th>ichimoku_leadSpanA</th>\n",
       "      <th>ichimoku_leadSpanB</th>\n",
       "      <th>MACD</th>\n",
       "      <th>...</th>\n",
       "      <th>ema_5</th>\n",
       "      <th>ema_10</th>\n",
       "      <th>ema_20</th>\n",
       "      <th>ema_30</th>\n",
       "      <th>ema_50</th>\n",
       "      <th>ema_100</th>\n",
       "      <th>ema_200</th>\n",
       "      <th>vwma</th>\n",
       "      <th>hull_Moving</th>\n",
       "      <th>Mkt_Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.309506</td>\n",
       "      <td>0.323185</td>\n",
       "      <td>0.314349</td>\n",
       "      <td>0.311447</td>\n",
       "      <td>0.158511</td>\n",
       "      <td>0.165547</td>\n",
       "      <td>0.610031</td>\n",
       "      <td>0.360129</td>\n",
       "      <td>0.336818</td>\n",
       "      <td>0.567335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340866</td>\n",
       "      <td>0.341051</td>\n",
       "      <td>0.342827</td>\n",
       "      <td>0.340625</td>\n",
       "      <td>0.333706</td>\n",
       "      <td>0.318544</td>\n",
       "      <td>0.299443</td>\n",
       "      <td>0.373789</td>\n",
       "      <td>0.382758</td>\n",
       "      <td>0.312638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.173618</td>\n",
       "      <td>0.181712</td>\n",
       "      <td>0.171755</td>\n",
       "      <td>0.174608</td>\n",
       "      <td>0.120752</td>\n",
       "      <td>0.150214</td>\n",
       "      <td>0.147846</td>\n",
       "      <td>0.207782</td>\n",
       "      <td>0.236865</td>\n",
       "      <td>0.167418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195385</td>\n",
       "      <td>0.203388</td>\n",
       "      <td>0.210658</td>\n",
       "      <td>0.215703</td>\n",
       "      <td>0.221806</td>\n",
       "      <td>0.226211</td>\n",
       "      <td>0.227650</td>\n",
       "      <td>0.186165</td>\n",
       "      <td>0.189751</td>\n",
       "      <td>0.172265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.225885</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.236921</td>\n",
       "      <td>0.226146</td>\n",
       "      <td>0.077413</td>\n",
       "      <td>0.061453</td>\n",
       "      <td>0.505965</td>\n",
       "      <td>0.254712</td>\n",
       "      <td>0.192137</td>\n",
       "      <td>0.470921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242474</td>\n",
       "      <td>0.231902</td>\n",
       "      <td>0.218803</td>\n",
       "      <td>0.202378</td>\n",
       "      <td>0.178738</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>0.130824</td>\n",
       "      <td>0.282522</td>\n",
       "      <td>0.295410</td>\n",
       "      <td>0.232673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.264223</td>\n",
       "      <td>0.276441</td>\n",
       "      <td>0.267946</td>\n",
       "      <td>0.264287</td>\n",
       "      <td>0.124178</td>\n",
       "      <td>0.117303</td>\n",
       "      <td>0.592183</td>\n",
       "      <td>0.305660</td>\n",
       "      <td>0.234630</td>\n",
       "      <td>0.598206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287933</td>\n",
       "      <td>0.285601</td>\n",
       "      <td>0.273462</td>\n",
       "      <td>0.257315</td>\n",
       "      <td>0.255200</td>\n",
       "      <td>0.249962</td>\n",
       "      <td>0.231692</td>\n",
       "      <td>0.336388</td>\n",
       "      <td>0.337407</td>\n",
       "      <td>0.263083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.386708</td>\n",
       "      <td>0.406506</td>\n",
       "      <td>0.391708</td>\n",
       "      <td>0.390215</td>\n",
       "      <td>0.211678</td>\n",
       "      <td>0.220504</td>\n",
       "      <td>0.726902</td>\n",
       "      <td>0.458545</td>\n",
       "      <td>0.416687</td>\n",
       "      <td>0.650943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428683</td>\n",
       "      <td>0.437853</td>\n",
       "      <td>0.411689</td>\n",
       "      <td>0.418349</td>\n",
       "      <td>0.430019</td>\n",
       "      <td>0.422910</td>\n",
       "      <td>0.401396</td>\n",
       "      <td>0.498581</td>\n",
       "      <td>0.483048</td>\n",
       "      <td>0.386545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            close        high         low        open  volumefrom    volumeto  \\\n",
       "count  365.000000  365.000000  365.000000  365.000000  365.000000  365.000000   \n",
       "mean     0.309506    0.323185    0.314349    0.311447    0.158511    0.165547   \n",
       "std      0.173618    0.181712    0.171755    0.174608    0.120752    0.150214   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.225885    0.230769    0.236921    0.226146    0.077413    0.061453   \n",
       "50%      0.264223    0.276441    0.267946    0.264287    0.124178    0.117303   \n",
       "75%      0.386708    0.406506    0.391708    0.390215    0.211678    0.220504   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "              CCI  ichimoku_leadSpanA  ichimoku_leadSpanB        MACD  ...  \\\n",
       "count  365.000000          365.000000          365.000000  365.000000  ...   \n",
       "mean     0.610031            0.360129            0.336818    0.567335  ...   \n",
       "std      0.147846            0.207782            0.236865    0.167418  ...   \n",
       "min      0.000000            0.000000            0.000000    0.000000  ...   \n",
       "25%      0.505965            0.254712            0.192137    0.470921  ...   \n",
       "50%      0.592183            0.305660            0.234630    0.598206  ...   \n",
       "75%      0.726902            0.458545            0.416687    0.650943  ...   \n",
       "max      1.000000            1.000000            1.000000    1.000000  ...   \n",
       "\n",
       "            ema_5      ema_10      ema_20      ema_30      ema_50     ema_100  \\\n",
       "count  365.000000  365.000000  365.000000  365.000000  365.000000  365.000000   \n",
       "mean     0.340866    0.341051    0.342827    0.340625    0.333706    0.318544   \n",
       "std      0.195385    0.203388    0.210658    0.215703    0.221806    0.226211   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.242474    0.231902    0.218803    0.202378    0.178738    0.152300   \n",
       "50%      0.287933    0.285601    0.273462    0.257315    0.255200    0.249962   \n",
       "75%      0.428683    0.437853    0.411689    0.418349    0.430019    0.422910   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "          ema_200        vwma  hull_Moving     Mkt_Cap  \n",
       "count  365.000000  365.000000   365.000000  365.000000  \n",
       "mean     0.299443    0.373789     0.382758    0.312638  \n",
       "std      0.227650    0.186165     0.189751    0.172265  \n",
       "min      0.000000    0.000000     0.000000    0.000000  \n",
       "25%      0.130824    0.282522     0.295410    0.232673  \n",
       "50%      0.231692    0.336388     0.337407    0.263083  \n",
       "75%      0.401396    0.498581     0.483048    0.386545  \n",
       "max      1.000000    1.000000     1.000000    1.000000  \n",
       "\n",
       "[8 rows x 49 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = pd.read_csv('btc_preprocessed.csv', index_col = 0)\n",
    "# apply X and standerdize \n",
    "def scale(df):\n",
    "    cols = df.columns\n",
    "    index = df.index\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(df)\n",
    "    X = pd.DataFrame(X)\n",
    "    X.columns = cols\n",
    "    X = X.set_index(index)\n",
    "    return X\n",
    "X = scale(price)\n",
    "print(X.columns)\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick model and fraction\n",
    "# pick first date out and fit to remaining\n",
    "# and apply it to first date to make it a probability to be an anomlay --> fit n - 1, and test it \n",
    "# pick just 1 model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VAE neurons adjusted (default nbr of neurons too much for blockchain dataset) \n",
    "    - for innitial run I would like to keep all the same\n",
    "- VAE returns:<br>\n",
    "UserWarning: Output model_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  5 INLIERS :  360 ABOD\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  0 INLIERS :  365 FB\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 HBOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 IF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  1 INLIERS :  364 Average KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 OCSVM\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 SOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  2 INLIERS :  363 LOF\n",
      "Model: \"model_46\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_31 (InputLayer)           (None, 49)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_166 (Dense)               (None, 49)           2450        input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_167 (Dense)               (None, 128)          6400        dense_166[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)            (None, 128)          0           dense_167[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_168 (Dense)               (None, 64)           8256        dropout_91[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_92 (Dropout)            (None, 64)           0           dense_168[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_169 (Dense)               (None, 32)           2080        dropout_92[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)            (None, 32)           0           dense_169[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_170 (Dense)               (None, 2)            66          dropout_93[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_171 (Dense)               (None, 2)            66          dropout_93[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 2)            0           dense_170[0][0]                  \n",
      "                                                                 dense_171[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 19,318\n",
      "Trainable params: 19,318\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_173 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_174 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_176 (Dense)            (None, 49)                6321      \n",
      "=================================================================\n",
      "Total params: 16,855\n",
      "Trainable params: 16,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_47 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_47.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_31 (InputLayer)        (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "model_46 (Model)             [(None, 2), (None, 2), (N 19318     \n",
      "_________________________________________________________________\n",
      "model_47 (Model)             (None, 49)                16855     \n",
      "=================================================================\n",
      "Total params: 36,173\n",
      "Trainable params: 36,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 132.4243 - val_loss: 73.6746\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 83.3453 - val_loss: 60.8217\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 67.9376 - val_loss: 54.3646\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 61.3712 - val_loss: 48.9603\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 55.2342 - val_loss: 44.8013\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 52.6910 - val_loss: 41.6443\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 108us/step - loss: 51.4514 - val_loss: 41.3706\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 110us/step - loss: 50.4571 - val_loss: 40.6301\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 50.5709 - val_loss: 41.2913\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 91us/step - loss: 50.2587 - val_loss: 40.9983\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 50.1599 - val_loss: 41.0590\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 50.1037 - val_loss: 40.8327\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 50.1767 - val_loss: 40.7986\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 49.9559 - val_loss: 40.7569\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.9716 - val_loss: 40.5702\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 50.0501 - val_loss: 40.7734\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 50.0045 - val_loss: 40.6709\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 50.0076 - val_loss: 41.0363\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 50.0338 - val_loss: 40.8131\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 50.0464 - val_loss: 40.9604\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 50.0245 - val_loss: 40.6017\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 50.0391 - val_loss: 40.7737\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9587 - val_loss: 40.5795\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9626 - val_loss: 40.7726\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9450 - val_loss: 40.7208\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 50.0310 - val_loss: 40.6670\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.9929 - val_loss: 40.6501\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 49.9634 - val_loss: 40.7106\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9738 - val_loss: 40.6225\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9820 - val_loss: 40.6483\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9731 - val_loss: 40.6409\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9946 - val_loss: 40.6671\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.9521 - val_loss: 40.6536\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9651 - val_loss: 40.6411\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.9187 - val_loss: 40.6099\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9550 - val_loss: 40.6564\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.9419 - val_loss: 40.6701\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9455 - val_loss: 40.6000\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9105 - val_loss: 40.6178\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.9616 - val_loss: 40.6464\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9468 - val_loss: 40.6158\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.9685 - val_loss: 40.6484\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 49.9195 - val_loss: 40.6755\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 49.9560 - val_loss: 40.6416\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 49.9488 - val_loss: 40.6201\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 49.9328 - val_loss: 40.7204\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.9210 - val_loss: 40.6844\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 49.9333 - val_loss: 40.6609\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 49.9798 - val_loss: 40.5590\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.9555 - val_loss: 40.7138\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9567 - val_loss: 40.6066\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.9725 - val_loss: 40.6468\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.9471 - val_loss: 40.6329\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.9402 - val_loss: 40.6527\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9276 - val_loss: 40.6412\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.9150 - val_loss: 40.6338\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9519 - val_loss: 40.6992\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.9132 - val_loss: 40.6939\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9628 - val_loss: 40.5974\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9879 - val_loss: 40.6890\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9450 - val_loss: 40.6045\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9663 - val_loss: 40.6461\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.9438 - val_loss: 40.7104\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9357 - val_loss: 40.6448\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 49.9330 - val_loss: 40.6816\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9680 - val_loss: 40.7046\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.9528 - val_loss: 40.6609\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 49.9357 - val_loss: 40.6889\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.9648 - val_loss: 40.6239\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.9240 - val_loss: 40.6745\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.9726 - val_loss: 40.6036\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9549 - val_loss: 40.6813\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.9436 - val_loss: 40.6487\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 49.9385 - val_loss: 40.6736\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 49.9512 - val_loss: 40.6455\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.9434 - val_loss: 40.6411\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.9359 - val_loss: 40.6371\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 49.9445 - val_loss: 40.6549\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.9539 - val_loss: 40.6364\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9600 - val_loss: 40.6549\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9324 - val_loss: 40.6247\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9493 - val_loss: 40.6477\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9396 - val_loss: 40.6411\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9411 - val_loss: 40.6441\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9395 - val_loss: 40.6210\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.9427 - val_loss: 40.6327\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9463 - val_loss: 40.6569\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.9473 - val_loss: 40.6642\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9584 - val_loss: 40.6377\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9495 - val_loss: 40.6486\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9402 - val_loss: 40.6430\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9457 - val_loss: 40.6426\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.9430 - val_loss: 40.6440\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9447 - val_loss: 40.6465\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.9380 - val_loss: 40.6416\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9437 - val_loss: 40.6283\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.9549 - val_loss: 40.6349\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.9395 - val_loss: 40.6411\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.9489 - val_loss: 40.6636\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.9528 - val_loss: 40.6329\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 VAE\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 ABOD\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  5 INLIERS :  360 FB\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 HBOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 IF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  4 INLIERS :  361 KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  1 INLIERS :  364 Average KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 OCSVM\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 SOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  4 INLIERS :  361 LOF\n",
      "Model: \"model_49\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           (None, 49)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_177 (Dense)               (None, 49)           2450        input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_178 (Dense)               (None, 128)          6400        dense_177[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)            (None, 128)          0           dense_178[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_179 (Dense)               (None, 64)           8256        dropout_97[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 64)           0           dense_179[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_180 (Dense)               (None, 32)           2080        dropout_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)            (None, 32)           0           dense_180[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_181 (Dense)               (None, 2)            66          dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_182 (Dense)               (None, 2)            66          dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 2)            0           dense_181[0][0]                  \n",
      "                                                                 dense_182[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 19,318\n",
      "Trainable params: 19,318\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_34 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 49)                6321      \n",
      "=================================================================\n",
      "Total params: 16,855\n",
      "Trainable params: 16,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_50 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_50.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "model_49 (Model)             [(None, 2), (None, 2), (N 19318     \n",
      "_________________________________________________________________\n",
      "model_50 (Model)             (None, 49)                16855     \n",
      "=================================================================\n",
      "Total params: 36,173\n",
      "Trainable params: 36,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 105.5355 - val_loss: 76.8697\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 108us/step - loss: 73.8341 - val_loss: 64.5339\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 111us/step - loss: 63.5393 - val_loss: 59.5552\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 57.5056 - val_loss: 55.2391\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 52.1810 - val_loss: 52.4442\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 50.1339 - val_loss: 52.3565\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 118us/step - loss: 49.3867 - val_loss: 52.0073\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 49.2040 - val_loss: 51.8737\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 118us/step - loss: 48.9994 - val_loss: 51.6620\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 91us/step - loss: 48.9520 - val_loss: 51.7450\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 48.8526 - val_loss: 51.6746\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 48.8283 - val_loss: 51.7290\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.8291 - val_loss: 51.5853\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7839 - val_loss: 51.7098\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7840 - val_loss: 51.7290\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 48.8142 - val_loss: 51.6547\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.7289 - val_loss: 51.7377\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7535 - val_loss: 51.7177\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7571 - val_loss: 51.6583\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7353 - val_loss: 51.6831\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.7271 - val_loss: 51.6633\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 48.7363 - val_loss: 51.6925\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7138 - val_loss: 51.6778\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.7309 - val_loss: 51.6329\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7075 - val_loss: 51.6872\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7181 - val_loss: 51.6758\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.7029 - val_loss: 51.6671\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7044 - val_loss: 51.6468\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7191 - val_loss: 51.6619\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6827 - val_loss: 51.6756\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7072 - val_loss: 51.7120\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7441 - val_loss: 51.6969\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6982 - val_loss: 51.6574\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.7183 - val_loss: 51.6619\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.6889 - val_loss: 51.6254\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7136 - val_loss: 51.6410\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7111 - val_loss: 51.6506\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7081 - val_loss: 51.6724\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7129 - val_loss: 51.6566\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7194 - val_loss: 51.6421\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7062 - val_loss: 51.6785\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7016 - val_loss: 51.6502\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7039 - val_loss: 51.6400\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.7015 - val_loss: 51.6579\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7042 - val_loss: 51.6693\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6931 - val_loss: 51.6571\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7064 - val_loss: 51.6488\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.7040 - val_loss: 51.6459\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.6985 - val_loss: 51.6525\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.6989 - val_loss: 51.6713\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6990 - val_loss: 51.6491\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.7003 - val_loss: 51.6682\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.7029 - val_loss: 51.6651\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7069 - val_loss: 51.6603\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6942 - val_loss: 51.6448\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6979 - val_loss: 51.6669\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7094 - val_loss: 51.6673\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.6924 - val_loss: 51.6516\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.7015 - val_loss: 51.6434\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7125 - val_loss: 51.6627\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.7095 - val_loss: 51.6675\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.7061 - val_loss: 51.6699\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.7007 - val_loss: 51.6667\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7015 - val_loss: 51.6421\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.7066 - val_loss: 51.6705\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.6986 - val_loss: 51.6552\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.7013 - val_loss: 51.6706\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.7058 - val_loss: 51.6634\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6937 - val_loss: 51.6648\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6990 - val_loss: 51.6644\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6948 - val_loss: 51.6835\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7047 - val_loss: 51.6585\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7010 - val_loss: 51.6616\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 48.7024 - val_loss: 51.6589\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.6911 - val_loss: 51.6491\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6962 - val_loss: 51.6555\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.6992 - val_loss: 51.6700\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7003 - val_loss: 51.6707\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.6953 - val_loss: 51.6688\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7021 - val_loss: 51.6652\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6878 - val_loss: 51.6621\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.6948 - val_loss: 51.6478\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.7036 - val_loss: 51.6775\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 48.7073 - val_loss: 51.6738\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.6973 - val_loss: 51.6510\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6990 - val_loss: 51.6601\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7042 - val_loss: 51.6712\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6940 - val_loss: 51.6690\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 48.7049 - val_loss: 51.6665\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7032 - val_loss: 51.6468\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.7088 - val_loss: 51.6697\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.7058 - val_loss: 51.6664\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6996 - val_loss: 51.6631\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6999 - val_loss: 51.6556\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.6991 - val_loss: 51.6663\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.6996 - val_loss: 51.6663\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.6973 - val_loss: 51.6575\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7048 - val_loss: 51.6658\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.7001 - val_loss: 51.6598\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.7011 - val_loss: 51.6724\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 VAE\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  10 INLIERS :  355 ABOD\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  9 INLIERS :  356 FB\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 HBOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 IF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  9 INLIERS :  356 KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  3 INLIERS :  362 Average KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 OCSVM\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 SOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  7 INLIERS :  358 LOF\n",
      "Model: \"model_52\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_35 (InputLayer)           (None, 49)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_188 (Dense)               (None, 49)           2450        input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_189 (Dense)               (None, 128)          6400        dense_188[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)           (None, 128)          0           dense_189[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_190 (Dense)               (None, 64)           8256        dropout_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)           (None, 64)           0           dense_190[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_191 (Dense)               (None, 32)           2080        dropout_104[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, 32)           0           dense_191[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_192 (Dense)               (None, 2)            66          dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_193 (Dense)               (None, 2)            66          dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 2)            0           dense_192[0][0]                  \n",
      "                                                                 dense_193[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 19,318\n",
      "Trainable params: 19,318\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_36 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 49)                6321      \n",
      "=================================================================\n",
      "Total params: 16,855\n",
      "Trainable params: 16,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_53 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_53.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_35 (InputLayer)        (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "model_52 (Model)             [(None, 2), (None, 2), (N 19318     \n",
      "_________________________________________________________________\n",
      "model_53 (Model)             (None, 49)                16855     \n",
      "=================================================================\n",
      "Total params: 36,173\n",
      "Trainable params: 36,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 122.6463 - val_loss: 101.1162\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 77.7349 - val_loss: 81.2228\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 64.7695 - val_loss: 72.8893\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 58.0240 - val_loss: 62.4267\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 110us/step - loss: 51.9359 - val_loss: 60.6877\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 112us/step - loss: 50.6326 - val_loss: 59.7626\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 114us/step - loss: 48.6813 - val_loss: 58.6748\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 108us/step - loss: 48.6146 - val_loss: 58.6700\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 48.5032 - val_loss: 58.7458\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 48.3665 - val_loss: 58.2340\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 48.1164 - val_loss: 58.2354\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 48.2148 - val_loss: 57.9309\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.0839 - val_loss: 58.0384\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.1341 - val_loss: 57.6891\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.1416 - val_loss: 57.9390\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.1191 - val_loss: 57.8468\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.0951 - val_loss: 58.3298\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.0138 - val_loss: 58.1256\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.0885 - val_loss: 58.1569\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.0707 - val_loss: 57.9506\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.0822 - val_loss: 57.9840\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 48.0217 - val_loss: 57.9848\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.0199 - val_loss: 58.0951\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 48.0277 - val_loss: 58.1677\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.0338 - val_loss: 58.0209\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 48.0501 - val_loss: 58.0016\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 48.0159 - val_loss: 58.0069\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.9577 - val_loss: 58.1490\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.0336 - val_loss: 58.0296\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.0508 - val_loss: 57.9760\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.0135 - val_loss: 57.9943\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 47.9906 - val_loss: 58.0243\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 48.0133 - val_loss: 57.9990\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.9538 - val_loss: 57.9960\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.9917 - val_loss: 57.9656\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 48.0096 - val_loss: 58.0924\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 47.9722 - val_loss: 57.8835\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 47.9849 - val_loss: 57.9677\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.9738 - val_loss: 57.9156\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.9959 - val_loss: 58.0429\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.0203 - val_loss: 58.0025\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.0076 - val_loss: 57.9182\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.9708 - val_loss: 57.9151\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.0007 - val_loss: 57.9659\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.0009 - val_loss: 57.9876\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.0350 - val_loss: 57.9755\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.9946 - val_loss: 58.0509\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.0060 - val_loss: 58.0055\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 48.0035 - val_loss: 57.9848\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.9969 - val_loss: 57.9422\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.9951 - val_loss: 58.0255\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9899 - val_loss: 57.9835\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.9896 - val_loss: 57.9859\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9917 - val_loss: 57.9853\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9968 - val_loss: 57.9964\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.0012 - val_loss: 57.9426\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9932 - val_loss: 57.9846\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.9944 - val_loss: 57.9804\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.9775 - val_loss: 57.9843\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.0017 - val_loss: 57.9784\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.9995 - val_loss: 58.0005\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9951 - val_loss: 57.9415\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.9882 - val_loss: 57.9955\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.9822 - val_loss: 57.9486\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9999 - val_loss: 57.9830\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9812 - val_loss: 57.9753\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.9964 - val_loss: 57.9478\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.9929 - val_loss: 57.9799\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.9941 - val_loss: 57.9790\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.9813 - val_loss: 57.9861\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.0001 - val_loss: 57.9586\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.9904 - val_loss: 57.9487\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.9913 - val_loss: 57.9502\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.0018 - val_loss: 57.9816\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.9987 - val_loss: 57.9756\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9824 - val_loss: 57.9751\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9975 - val_loss: 57.9721\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.9842 - val_loss: 57.9812\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9947 - val_loss: 57.9753\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.0034 - val_loss: 57.9726\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.9869 - val_loss: 57.9830\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9914 - val_loss: 57.9534\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9910 - val_loss: 57.9421\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.9919 - val_loss: 57.9707\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.9886 - val_loss: 57.9478\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9830 - val_loss: 57.9568\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9884 - val_loss: 57.9730\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.9786 - val_loss: 57.9837\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.9852 - val_loss: 57.9417\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.9797 - val_loss: 57.9593\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.9925 - val_loss: 57.9718\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.9753 - val_loss: 57.9549\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.9901 - val_loss: 58.0171\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 48.0068 - val_loss: 57.9886\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9917 - val_loss: 57.9937\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9784 - val_loss: 57.9682\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.9852 - val_loss: 57.9691\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.9904 - val_loss: 57.9963\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 48.0029 - val_loss: 57.9575\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.9875 - val_loss: 57.9917\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 VAE\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  16 INLIERS :  349 ABOD\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  12 INLIERS :  353 FB\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 HBOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 IF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  12 INLIERS :  353 KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  3 INLIERS :  362 Average KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 OCSVM\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 SOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  12 INLIERS :  353 LOF\n",
      "Model: \"model_55\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_37 (InputLayer)           (None, 49)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_199 (Dense)               (None, 49)           2450        input_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_200 (Dense)               (None, 128)          6400        dense_199[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)           (None, 128)          0           dense_200[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_201 (Dense)               (None, 64)           8256        dropout_109[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)           (None, 64)           0           dense_201[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_202 (Dense)               (None, 32)           2080        dropout_110[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)           (None, 32)           0           dense_202[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_203 (Dense)               (None, 2)            66          dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_204 (Dense)               (None, 2)            66          dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 2)            0           dense_203[0][0]                  \n",
      "                                                                 dense_204[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 19,318\n",
      "Trainable params: 19,318\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_38 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_205 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_206 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_207 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_208 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 49)                6321      \n",
      "=================================================================\n",
      "Total params: 16,855\n",
      "Trainable params: 16,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_56 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_56.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_37 (InputLayer)        (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "model_55 (Model)             [(None, 2), (None, 2), (N 19318     \n",
      "_________________________________________________________________\n",
      "model_56 (Model)             (None, 49)                16855     \n",
      "=================================================================\n",
      "Total params: 36,173\n",
      "Trainable params: 36,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 101.5092 - val_loss: 76.8683\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 113us/step - loss: 72.1333 - val_loss: 66.2985\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 62.5785 - val_loss: 60.2028\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 56.5506 - val_loss: 52.9582\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 51.8124 - val_loss: 49.9915\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 134us/step - loss: 50.3679 - val_loss: 49.0247\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 49.7586 - val_loss: 48.8708\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 133us/step - loss: 49.5992 - val_loss: 48.8495\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 111us/step - loss: 49.5173 - val_loss: 48.4802\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 100us/step - loss: 49.4038 - val_loss: 48.3727\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 96us/step - loss: 49.4630 - val_loss: 48.4101\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 96us/step - loss: 49.2293 - val_loss: 48.1541\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 49.2409 - val_loss: 48.2349\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 49.2303 - val_loss: 48.1935\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.1955 - val_loss: 48.1872\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 91us/step - loss: 49.1535 - val_loss: 48.1894\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.2235 - val_loss: 48.3383\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.1240 - val_loss: 48.1468\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.1784 - val_loss: 48.2550\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.1392 - val_loss: 48.1721\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.1673 - val_loss: 48.1724\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.1680 - val_loss: 48.0662\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.1212 - val_loss: 48.1036\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.1348 - val_loss: 48.0966\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.0677 - val_loss: 48.1332\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.1234 - val_loss: 48.0897\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.1402 - val_loss: 48.2406\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 49.1431 - val_loss: 48.1331\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.1413 - val_loss: 48.0773\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 49.1225 - val_loss: 48.1571\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 91us/step - loss: 49.1149 - val_loss: 48.1511\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 91us/step - loss: 49.1132 - val_loss: 48.0170\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 93us/step - loss: 49.1117 - val_loss: 48.1581\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 49.1090 - val_loss: 48.1107\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.1154 - val_loss: 48.0717\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.1139 - val_loss: 48.0857\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.1070 - val_loss: 48.1084\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.1246 - val_loss: 48.0518\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.1056 - val_loss: 48.0648\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.0999 - val_loss: 48.0672\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.1139 - val_loss: 48.1274\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.1358 - val_loss: 48.0526\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.1257 - val_loss: 48.1892\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.1112 - val_loss: 48.1313\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.1150 - val_loss: 48.0524\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.1255 - val_loss: 48.0777\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.0859 - val_loss: 48.0881\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.1468 - val_loss: 48.1210\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 49.1150 - val_loss: 48.0730\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.1108 - val_loss: 48.0768\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.0971 - val_loss: 48.0423\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 49.1061 - val_loss: 48.0973\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.1054 - val_loss: 48.1196\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.0997 - val_loss: 48.0832\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.0980 - val_loss: 48.0477\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.0946 - val_loss: 48.1175\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.1196 - val_loss: 48.0631\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.1134 - val_loss: 48.0895\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.1137 - val_loss: 48.0624\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.1048 - val_loss: 48.0628\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 49.1002 - val_loss: 48.1065\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.1031 - val_loss: 48.0537\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.1013 - val_loss: 48.1420\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.1046 - val_loss: 48.0623\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.0964 - val_loss: 48.1056\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.0942 - val_loss: 48.0564\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.1016 - val_loss: 48.0492\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.0880 - val_loss: 48.0708\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.0887 - val_loss: 48.0697\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.0736 - val_loss: 47.9993\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.0985 - val_loss: 48.1020\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.1224 - val_loss: 48.0935\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.0781 - val_loss: 48.1183\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.1231 - val_loss: 48.0747\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.1063 - val_loss: 48.0911\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.1094 - val_loss: 48.0241\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.1197 - val_loss: 48.1141\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.1525 - val_loss: 48.1343\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.1157 - val_loss: 48.0817\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.0959 - val_loss: 48.0628\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.1064 - val_loss: 48.0449\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.0992 - val_loss: 48.0673\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.1054 - val_loss: 48.0408\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.0962 - val_loss: 48.0984\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.0998 - val_loss: 48.0918\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.1217 - val_loss: 48.0678\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.1171 - val_loss: 48.0569\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.1041 - val_loss: 48.0859\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 49.1078 - val_loss: 48.0982\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.0970 - val_loss: 48.0963\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 49.1070 - val_loss: 48.0635\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.1111 - val_loss: 48.0922\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 49.1087 - val_loss: 48.0531\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.0947 - val_loss: 48.0847\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 49.0865 - val_loss: 48.0811\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 49.1087 - val_loss: 48.0195\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 49.0918 - val_loss: 48.1204\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.0932 - val_loss: 48.0600\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 49.0949 - val_loss: 48.1121\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 49.1237 - val_loss: 48.1204\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 VAE\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  21 INLIERS :  344 ABOD\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  14 INLIERS :  351 FB\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 HBOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 IF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  15 INLIERS :  350 KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  3 INLIERS :  362 Average KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 OCSVM\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 SOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  16 INLIERS :  349 LOF\n",
      "Model: \"model_58\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_39 (InputLayer)           (None, 49)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_210 (Dense)               (None, 49)           2450        input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_211 (Dense)               (None, 128)          6400        dense_210[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_115 (Dropout)           (None, 128)          0           dense_211[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_212 (Dense)               (None, 64)           8256        dropout_115[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_116 (Dropout)           (None, 64)           0           dense_212[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_213 (Dense)               (None, 32)           2080        dropout_116[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_117 (Dropout)           (None, 32)           0           dense_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_214 (Dense)               (None, 2)            66          dropout_117[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_215 (Dense)               (None, 2)            66          dropout_117[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 2)            0           dense_214[0][0]                  \n",
      "                                                                 dense_215[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 19,318\n",
      "Trainable params: 19,318\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_40 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_218 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_219 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_220 (Dense)            (None, 49)                6321      \n",
      "=================================================================\n",
      "Total params: 16,855\n",
      "Trainable params: 16,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_59 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_59.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_39 (InputLayer)        (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "model_58 (Model)             [(None, 2), (None, 2), (N 19318     \n",
      "_________________________________________________________________\n",
      "model_59 (Model)             (None, 49)                16855     \n",
      "=================================================================\n",
      "Total params: 36,173\n",
      "Trainable params: 36,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 102.9549 - val_loss: 94.3199\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 71.4321 - val_loss: 77.0833\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 60.4411 - val_loss: 68.2471\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 53.6038 - val_loss: 62.5679\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 50.3685 - val_loss: 61.1798\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 110us/step - loss: 48.8165 - val_loss: 60.5176\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 48.3401 - val_loss: 60.2469\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 48.1263 - val_loss: 59.8749\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 48.0341 - val_loss: 59.8118\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 91us/step - loss: 47.9219 - val_loss: 59.6241\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.8920 - val_loss: 59.7680\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.9052 - val_loss: 59.9442\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.8685 - val_loss: 59.9410\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 47.8133 - val_loss: 59.7164\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.8610 - val_loss: 59.8761\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.8581 - val_loss: 59.7275\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.8405 - val_loss: 59.8925\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 47.8186 - val_loss: 59.7346\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.8020 - val_loss: 59.7308\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 47.7889 - val_loss: 59.9280\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 47.8060 - val_loss: 59.7246\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 94us/step - loss: 47.8656 - val_loss: 59.6701\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.8511 - val_loss: 59.7648\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.7986 - val_loss: 59.7858\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 47.8215 - val_loss: 59.9635\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.7651 - val_loss: 59.7551\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7997 - val_loss: 59.7982\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.7430 - val_loss: 59.7714\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.8051 - val_loss: 59.7884\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7966 - val_loss: 59.7099\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7999 - val_loss: 59.6558\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.8291 - val_loss: 59.7303\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.7648 - val_loss: 59.7173\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.8074 - val_loss: 59.8692\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7857 - val_loss: 59.7428\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.8114 - val_loss: 59.6940\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.8100 - val_loss: 59.7391\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.8017 - val_loss: 59.7239\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.7876 - val_loss: 59.7761\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7880 - val_loss: 59.7522\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.7776 - val_loss: 59.8082\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.7941 - val_loss: 59.7626\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.7862 - val_loss: 59.7606\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7969 - val_loss: 59.6625\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 47.8143 - val_loss: 59.7181\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.7912 - val_loss: 59.7221\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.7935 - val_loss: 59.8020\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7793 - val_loss: 59.7520\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.8008 - val_loss: 59.7469\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7671 - val_loss: 59.7226\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.7876 - val_loss: 59.7879\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.7982 - val_loss: 59.7704\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.7702 - val_loss: 59.7788\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.7898 - val_loss: 59.7802\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7729 - val_loss: 59.7423\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7587 - val_loss: 59.7800\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.8034 - val_loss: 59.7496\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.7928 - val_loss: 59.8118\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7573 - val_loss: 59.7441\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.7930 - val_loss: 59.7233\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.7893 - val_loss: 59.6870\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.7573 - val_loss: 59.8050\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.7783 - val_loss: 59.8690\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7758 - val_loss: 59.8718\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.8186 - val_loss: 59.7449\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7846 - val_loss: 59.7427\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.7766 - val_loss: 59.7927\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.8138 - val_loss: 59.7505\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.7708 - val_loss: 59.7423\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.7556 - val_loss: 59.7837\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.7770 - val_loss: 59.8716\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.7744 - val_loss: 59.6551\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7888 - val_loss: 59.7005\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.8046 - val_loss: 59.8016\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.7852 - val_loss: 59.7951\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 47.7567 - val_loss: 59.7994\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.7676 - val_loss: 59.7157\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.7727 - val_loss: 59.8835\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 47.8091 - val_loss: 59.8242\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.7264 - val_loss: 59.7821\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.8277 - val_loss: 59.6623\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.8102 - val_loss: 59.7754\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7923 - val_loss: 59.8464\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.7828 - val_loss: 59.9395\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7633 - val_loss: 59.9548\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.8764 - val_loss: 59.7010\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.7803 - val_loss: 59.7769\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7860 - val_loss: 59.7456\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7834 - val_loss: 59.8282\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 47.7389 - val_loss: 59.7670\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.8251 - val_loss: 59.8141\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.8066 - val_loss: 59.8410\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.7937 - val_loss: 59.6938\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.7688 - val_loss: 59.7529\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 47.7928 - val_loss: 59.7674\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.7881 - val_loss: 59.7779\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.8086 - val_loss: 59.7801\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 47.7845 - val_loss: 59.7713\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.8257 - val_loss: 59.7807\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 47.8033 - val_loss: 59.7456\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 VAE\n"
     ]
    }
   ],
   "source": [
    "# time the code\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "# empty list to store outliers and inliners\n",
    "df_outliers = []\n",
    "scores_pred_lst = []\n",
    "proba_lst = []\n",
    "predict_rank_lst = []\n",
    "\n",
    "# iterate over different outlier fractions \n",
    "outliers_fraction = [0.01,0.02,0.03,0.04,0.05]\n",
    "random_state = 123 # set random state\n",
    "\n",
    "for fraction in outliers_fraction:\n",
    "    \n",
    "    # Define 13 outlier detection tools to be compared\n",
    "    classifiers = {#'AE' : AutoEncoder(contamination=outliers_fraction, random_state=random_state), # check why error\n",
    "               'ABOD': ABOD(contamination = fraction),\n",
    "              'CBLOF':CBLOF(check_estimator=False, random_state=random_state), # getting 0 \n",
    "              'FB':FeatureBagging(LOF(n_neighbors=35),contamination=fraction,check_estimator=False,random_state=random_state),\n",
    "              'HBOS': HBOS(contamination=fraction),\n",
    "              'IF': IForest(contamination=fraction,random_state=random_state),\n",
    "              'KNN': KNN(contamination=fraction),\n",
    "              'Average KNN': KNN(method='mean',contamination=fraction),\n",
    "              'OCSVM' : OCSVM(contamination=fraction),\n",
    "              'SOS' : SOS(contamination=fraction),\n",
    "              'LOF': LOF(n_neighbors=35, contamination = fraction),\n",
    "              #'LOCI': LOCI(contamination = fraction), # takes a super long time + don't understand\n",
    "              'VAE' : VAE(contamination=fraction, random_state=random_state) \n",
    "              }\n",
    "    \n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        clf.fit(X)\n",
    "        # predict raw anomaly score\n",
    "        scores_pred = clf.decision_function(X) * -1 # WHY? * -1\n",
    "\n",
    "        # prediction of a datapoint category outlier or inlier\n",
    "        y_pred = clf.predict(X)\n",
    "\n",
    "        # probability of outlier \n",
    "        probability = clf.predict_proba(X, method = 'linear')\n",
    "\n",
    "        # predict rank\n",
    "        predict_rank = clf._predict_rank(X)\n",
    "\n",
    "        n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "        n_outliers = np.count_nonzero(y_pred == 1)\n",
    "\n",
    "        # copy of dataframe\n",
    "        dfx = X.copy()\n",
    "\n",
    "        dfx[clf_name] = y_pred.tolist()\n",
    "        #print(clf_name,'\\n',i,clf,'\\n',classifiers,'\\n',classifiers.items())\n",
    "        print('Outlier fraction: ', fraction)\n",
    "        print('OUTLIERS : ',n_outliers,'INLIERS : ',n_inliers, clf_name)\n",
    "        #print('scores_pred: ', scores_pred)\n",
    "        scores_pred_lst.append(scores_pred)\n",
    "        df_outliers.append(dfx[clf_name]) \n",
    "        proba_lst.append(probability)\n",
    "        predict_rank_lst.append(predict_rank)\n",
    "t1 = time.time()\n",
    "total = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run:  41.368916273117065 s\n",
      "Nbr of lists 55 (5 outlier fractions and 11 models)\n"
     ]
    }
   ],
   "source": [
    "print('Time to run: ',total,'s')\n",
    "# we have 50 lists (or essentially 5 with 10 models)\n",
    "print('Nbr of lists',len(df_outliers), '(5 outlier fractions and 11 models)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get unique dates per outlier fraction iteration across models\n",
    "- Change if number of models change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_oo1 = df_outliers[0:11]\n",
    "frac_oo2 = df_outliers[11:22]\n",
    "frac_oo3 = df_outliers[22:33]\n",
    "frac_oo4 = df_outliers[33:44]\n",
    "frac_oo5 = df_outliers[44:]\n",
    "\n",
    "# subset data so we only get dates where models identified them as outliers\n",
    "def outlier_subset_function(outliers_lst):\n",
    "# convert lists into df and transpose it\n",
    "    btc_outliers = np.transpose(pd.DataFrame(outliers_lst))#.reset_index()\n",
    "    \n",
    "    outlier_dates = [] # this stores outlier dates\n",
    "    #lenght = len(outlier_dates)\n",
    "    for col in btc_outliers.columns:\n",
    "        outliers = btc_outliers[btc_outliers[col] == 1] # subset outlier data\n",
    "        outlier_dates.append(outliers.index.tolist()) # append the string/date \n",
    "        \n",
    "        #lenght2 = len(outlier_dates)\n",
    "        #nbr_dates = \n",
    "    return outlier_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function to extract the dates\n",
    "frac_oo1_outliers = outlier_subset_function(frac_oo1)\n",
    "frac_oo2_outliers = outlier_subset_function(frac_oo2)\n",
    "frac_oo3_outliers = outlier_subset_function(frac_oo3)\n",
    "frac_oo4_outliers = outlier_subset_function(frac_oo4)\n",
    "frac_oo5_outliers = outlier_subset_function(frac_oo5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Probability Ranking\n",
    "- how many fractions (AnomalyCount_Fraction) and models (AnomalyCount_Models) identified the date as an outlier\n",
    "- count_weight_outliers list number **UPDATED** based on the number of models in dataset\n",
    "\n",
    "**NOTE:** Weight col enumerator (division) needs to be changed depending on the number of models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_weight_outliers(outlier_dates): \n",
    "    join =outlier_dates[0] + outlier_dates[1] + outlier_dates[2] + outlier_dates[3] + outlier_dates[4]# +\\\n",
    "    #outlier_dates[5]+outlier_dates[6]+outlier_dates[7]\n",
    "    (unique, counts) = np.unique(join, return_counts=True)\n",
    "    frequencies = np.asarray((unique, counts)).T\n",
    "    frequencies_df = pd.DataFrame(frequencies)\n",
    "    frequencies_df.columns = ['Date','Rank']\n",
    "    frequencies_df.Rank = frequencies_df.Rank.astype(int)\n",
    "    frequencies_df['Weight'] = frequencies_df['Rank']/55 # ALTER THIS if nbr models changes\n",
    "    frequencies_df = frequencies_df.sort_values(by='Rank', ascending = False)\n",
    "    return frequencies_df\n",
    "\n",
    "# all fraction anomalous dates found merged \n",
    "def date_ranking(df1, df2, df3, df4, df5):\n",
    "    # concat all unique dates \n",
    "    concat = pd.concat([count_weight_outliers(df1),count_weight_outliers(df2),\n",
    "            count_weight_outliers(df3),count_weight_outliers(df4), count_weight_outliers(df5)], \n",
    "            axis = 0)\n",
    "    \n",
    "    # how many times was a date selected \n",
    "    anomalyCount_Fractions = pd.DataFrame(concat.Date.value_counts()).reset_index()\n",
    "    anomalyCount_Fractions.columns = ['Date','AnomalyCount_Fractions']\n",
    " \n",
    "    # sum votes over all \n",
    "    sum_votes = concat.Rank.sum()\n",
    "    \n",
    "    # sum of nbr of models that selected a date as an anomaly \n",
    "    vote_count = pd.DataFrame(concat.groupby(\"Date\")['Rank'].agg('sum')).reset_index()\n",
    "    # merge it with anomalyCount_Fractions df so that dates match \n",
    "    merged_inner = pd.merge(left=anomalyCount_Fractions, right=vote_count, on='Date')\n",
    "    merged_inner.columns = ['Date','AnomalyCount_Fractions','AnomalyCount_Models']\n",
    "    # sum models per dates / total anomaly votes\n",
    "    merged_inner['weight_models'] = merged_inner.AnomalyCount_Models/sum_votes\n",
    "    return merged_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Anomaly Dates:  59\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AnomalyCount_Fractions</th>\n",
       "      <th>AnomalyCount_Models</th>\n",
       "      <th>weight_models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.050125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-22</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-07-24</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.035088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.027569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-11-14</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.020050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.015038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-05-11</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.027569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-11-24</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-11-21</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.020050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-08-08</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-04-15</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.015038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-10-11</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.030075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-11-16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-11-17</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.045113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.032581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-03-10</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.020050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.027569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018-11-15</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.020050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-11-18</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.045113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2018-06-10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-11-20</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.020050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.020050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.020050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2018-12-20</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2018-12-22</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2018-12-23</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2018-03-30</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2018-07-17</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2018-10-15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2018-08-27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  AnomalyCount_Fractions  AnomalyCount_Models  weight_models\n",
       "0   2018-01-16                       5                   20       0.050125\n",
       "1   2018-11-22                       5                    5       0.012531\n",
       "2   2018-07-24                       5                   10       0.025063\n",
       "3   2018-01-07                       5                   14       0.035088\n",
       "4   2018-02-06                       5                   10       0.025063\n",
       "5   2018-01-03                       5                   11       0.027569\n",
       "6   2018-11-14                       5                    8       0.020050\n",
       "7   2018-01-13                       5                    6       0.015038\n",
       "8   2018-05-11                       5                    7       0.017544\n",
       "9   2018-02-05                       5                   11       0.027569\n",
       "10  2018-11-24                       5                    5       0.012531\n",
       "11  2018-06-12                       5                    5       0.012531\n",
       "12  2018-11-21                       5                    5       0.012531\n",
       "13  2018-02-02                       5                    8       0.020050\n",
       "14  2018-08-08                       5                    7       0.017544\n",
       "15  2018-04-15                       5                    5       0.012531\n",
       "16  2018-01-15                       5                    6       0.015038\n",
       "17  2018-10-11                       5                    5       0.012531\n",
       "18  2018-01-04                       5                   12       0.030075\n",
       "19  2018-01-02                       5                   10       0.025063\n",
       "20  2018-11-16                       5                    5       0.012531\n",
       "21  2018-11-19                       5                    5       0.012531\n",
       "22  2018-11-17                       5                    5       0.012531\n",
       "23  2018-01-06                       5                   18       0.045113\n",
       "24  2018-01-08                       5                   10       0.025063\n",
       "25  2018-01-01                       5                   13       0.032581\n",
       "26  2018-01-20                       5                    5       0.012531\n",
       "27  2018-03-10                       5                    8       0.020050\n",
       "28  2018-01-09                       5                    7       0.017544\n",
       "29  2018-01-14                       5                   11       0.027569\n",
       "30  2018-11-15                       5                    8       0.020050\n",
       "31  2018-11-18                       5                    5       0.012531\n",
       "32  2018-01-05                       5                   18       0.045113\n",
       "33  2018-01-12                       5                    7       0.017544\n",
       "34  2018-06-10                       5                    5       0.012531\n",
       "35  2018-06-13                       5                    5       0.012531\n",
       "36  2018-11-20                       5                    8       0.020050\n",
       "37  2018-01-17                       5                   19       0.047619\n",
       "38  2018-01-11                       5                    8       0.020050\n",
       "39  2018-01-10                       5                    8       0.020050\n",
       "40  2018-12-24                       4                    4       0.010025\n",
       "41  2018-12-20                       4                    4       0.010025\n",
       "42  2018-01-21                       4                    4       0.010025\n",
       "43  2018-02-01                       4                    4       0.010025\n",
       "44  2018-12-22                       4                    4       0.010025\n",
       "45  2018-01-18                       4                    7       0.017544\n",
       "46  2018-01-22                       3                    3       0.007519\n",
       "47  2018-12-23                       3                    3       0.007519\n",
       "48  2018-02-04                       2                    4       0.010025\n",
       "49  2018-03-30                       2                    2       0.005013\n",
       "50  2018-07-17                       2                    2       0.005013\n",
       "51  2018-01-19                       2                    2       0.005013\n",
       "52  2018-07-18                       2                    2       0.005013\n",
       "53  2018-01-23                       1                    1       0.002506\n",
       "54  2018-01-30                       1                    1       0.002506\n",
       "55  2018-09-19                       1                    1       0.002506\n",
       "56  2018-01-28                       1                    1       0.002506\n",
       "57  2018-10-15                       1                    1       0.002506\n",
       "58  2018-08-27                       1                    1       0.002506"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_anomalies = date_ranking(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)\n",
    "print('Unique Anomaly Dates: ',len(price_anomalies.Date.unique()))\n",
    "price_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction 0.01 unique dates:  40\n",
      "Outlier fraction 0.02 unique dates:  46\n",
      "Outlier fraction 0.03 unique dates:  48\n",
      "Outlier fraction 0.04 unique dates:  53\n",
      "Outlier fraction 0.05 unique dates:  59\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>001</th>\n",
       "      <th>002</th>\n",
       "      <th>003</th>\n",
       "      <th>004</th>\n",
       "      <th>005</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-18</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-21</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-04</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-06</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-08</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-20</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-22</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-24</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            001  002  003  004  005\n",
       "2018-01-01    1    1    1    1    1\n",
       "2018-01-02    1    1    1    1    1\n",
       "2018-01-03    1    1    1    1    1\n",
       "2018-01-04    1    1    1    1    1\n",
       "2018-01-05    1    1    1    1    1\n",
       "2018-01-06    1    1    1    1    1\n",
       "2018-01-07    1    1    1    1    1\n",
       "2018-01-08    1    1    1    1    1\n",
       "2018-01-09    1    1    1    1    1\n",
       "2018-01-10    1    1    1    1    1\n",
       "2018-01-11    1    1    1    1    1\n",
       "2018-01-12    1    1    1    1    1\n",
       "2018-01-13    1    1    1    1    1\n",
       "2018-01-14    1    1    1    1    1\n",
       "2018-01-15    1    1    1    1    1\n",
       "2018-01-16    1    1    1    1    1\n",
       "2018-01-17    1    1    1    1    1\n",
       "2018-01-18    0    1    1    1    1\n",
       "2018-01-19    0    0    0    1    1\n",
       "2018-01-20    1    1    1    1    1\n",
       "2018-01-21    0    1    1    1    1\n",
       "2018-01-22    0    0    1    1    1\n",
       "2018-01-23    0    0    0    0    1\n",
       "2018-01-28    0    0    0    0    1\n",
       "2018-01-30    0    0    0    0    1\n",
       "2018-02-01    0    1    1    1    1\n",
       "2018-02-02    1    1    1    1    1\n",
       "2018-02-04    0    0    0    1    1\n",
       "2018-02-05    1    1    1    1    1\n",
       "2018-02-06    1    1    1    1    1\n",
       "2018-03-10    1    1    1    1    1\n",
       "2018-03-30    0    0    0    1    1\n",
       "2018-04-15    1    1    1    1    1\n",
       "2018-05-11    1    1    1    1    1\n",
       "2018-06-10    1    1    1    1    1\n",
       "2018-06-12    1    1    1    1    1\n",
       "2018-06-13    1    1    1    1    1\n",
       "2018-07-17    0    0    0    1    1\n",
       "2018-07-18    0    0    0    1    1\n",
       "2018-07-24    1    1    1    1    1\n",
       "2018-08-08    1    1    1    1    1\n",
       "2018-08-27    0    0    0    0    1\n",
       "2018-09-19    0    0    0    0    1\n",
       "2018-10-11    1    1    1    1    1\n",
       "2018-10-15    0    0    0    0    1\n",
       "2018-11-14    1    1    1    1    1\n",
       "2018-11-15    1    1    1    1    1\n",
       "2018-11-16    1    1    1    1    1\n",
       "2018-11-17    1    1    1    1    1\n",
       "2018-11-18    1    1    1    1    1\n",
       "2018-11-19    1    1    1    1    1\n",
       "2018-11-20    1    1    1    1    1\n",
       "2018-11-21    1    1    1    1    1\n",
       "2018-11-22    1    1    1    1    1\n",
       "2018-11-24    1    1    1    1    1\n",
       "2018-12-20    0    1    1    1    1\n",
       "2018-12-22    0    1    1    1    1\n",
       "2018-12-23    0    0    1    1    1\n",
       "2018-12-24    0    1    1    1    1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dates_per_fraction(l1,l2,l3,l4,l5):\n",
    "    d1 = pd.concat([count_weight_outliers(l1),count_weight_outliers(l2),count_weight_outliers(l3),\n",
    "                    count_weight_outliers(l4),count_weight_outliers(l5)],axis=1)\n",
    "    d1.columns = ['001','Count1','W1','002','Count2','W2','003','Count3','W3','004','Count4','W4','005',\n",
    "                'Count5','W5']\n",
    "    d01 = d1['001'].value_counts()\n",
    "    d02 = d1['002'].value_counts()\n",
    "    d03 = d1['003'].value_counts()\n",
    "    d04 = d1['004'].value_counts()\n",
    "    d05 = d1['005'].value_counts()\n",
    "    l = pd.concat([d01,d02,d03,d04,d05],axis=1, sort=True)\n",
    "    l = l.groupby(l.index)['001','002','003','004','005'].agg('count')\n",
    "    print('Outlier fraction 0.01 unique dates: ',l['001'].sum())\n",
    "    print('Outlier fraction 0.02 unique dates: ',l['002'].sum())\n",
    "    print('Outlier fraction 0.03 unique dates: ',l['003'].sum())\n",
    "    print('Outlier fraction 0.04 unique dates: ',l['004'].sum())\n",
    "    print('Outlier fraction 0.05 unique dates: ',l['005'].sum())\n",
    "    return l\n",
    "\n",
    "dates_per_fraction(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ranking table\n",
    "#frequencies_df_price = frequencies_df\n",
    "# save the outliers lists uniquely for later\n",
    "prices_df = df_outliers\n",
    "# probability lists\n",
    "prices_proba = proba_lst\n",
    "# rank lists\n",
    "prices_rank_lst = predict_rank_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Probabilities Output \n",
    "- I think it's similar to predict() where I get labels not ratios (based on threshold it labels?)\n",
    "- predict_proba(): Predict the probability of a sample being outlier using the fitted detector.\n",
    "- simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. \n",
    "- Description of labeling formula vs probabilty: https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#BaseDetector.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.26869892e-07, 9.99999773e-01],\n",
       "       [1.22951083e-04, 9.99877049e-01],\n",
       "       [3.55243856e-04, 9.99644756e-01],\n",
       "       [4.31716397e-04, 9.99568284e-01],\n",
       "       [3.90444856e-05, 9.99960956e-01],\n",
       "       [1.49117160e-05, 9.99985088e-01],\n",
       "       [8.06483994e-05, 9.99919352e-01],\n",
       "       [3.57676071e-05, 9.99964232e-01],\n",
       "       [2.57806667e-05, 9.99974219e-01],\n",
       "       [1.64204572e-05, 9.99983580e-01]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=100)\n",
    "prices_proba[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blockhain Test (need to add some feature engineering)\n",
    "- note no feature engineering added to this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['active_addresses', 'average_transaction_value', 'block_height',\n",
      "       'block_size', 'block_time', 'current_supply', 'difficulty', 'hashrate',\n",
      "       'large_transaction_count', 'new_addresses', 'transaction_count',\n",
      "       'transaction_count_all_time', 'unique_addresses_all_time',\n",
      "       'zero_balance_addresses_all_time'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>active_addresses</th>\n",
       "      <th>average_transaction_value</th>\n",
       "      <th>block_height</th>\n",
       "      <th>block_size</th>\n",
       "      <th>block_time</th>\n",
       "      <th>current_supply</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>hashrate</th>\n",
       "      <th>large_transaction_count</th>\n",
       "      <th>new_addresses</th>\n",
       "      <th>transaction_count</th>\n",
       "      <th>transaction_count_all_time</th>\n",
       "      <th>unique_addresses_all_time</th>\n",
       "      <th>zero_balance_addresses_all_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.312659</td>\n",
       "      <td>0.224696</td>\n",
       "      <td>0.513188</td>\n",
       "      <td>0.577511</td>\n",
       "      <td>0.325309</td>\n",
       "      <td>0.513188</td>\n",
       "      <td>0.550890</td>\n",
       "      <td>0.476739</td>\n",
       "      <td>0.170867</td>\n",
       "      <td>0.224342</td>\n",
       "      <td>0.303136</td>\n",
       "      <td>0.482607</td>\n",
       "      <td>0.507523</td>\n",
       "      <td>0.518616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.171182</td>\n",
       "      <td>0.136495</td>\n",
       "      <td>0.290541</td>\n",
       "      <td>0.223873</td>\n",
       "      <td>0.162468</td>\n",
       "      <td>0.290539</td>\n",
       "      <td>0.301799</td>\n",
       "      <td>0.243572</td>\n",
       "      <td>0.152479</td>\n",
       "      <td>0.133257</td>\n",
       "      <td>0.152195</td>\n",
       "      <td>0.275745</td>\n",
       "      <td>0.276457</td>\n",
       "      <td>0.275090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.212106</td>\n",
       "      <td>0.132698</td>\n",
       "      <td>0.262062</td>\n",
       "      <td>0.417905</td>\n",
       "      <td>0.210205</td>\n",
       "      <td>0.262065</td>\n",
       "      <td>0.287124</td>\n",
       "      <td>0.268642</td>\n",
       "      <td>0.098229</td>\n",
       "      <td>0.151384</td>\n",
       "      <td>0.198545</td>\n",
       "      <td>0.246273</td>\n",
       "      <td>0.271502</td>\n",
       "      <td>0.300481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.279694</td>\n",
       "      <td>0.196649</td>\n",
       "      <td>0.517795</td>\n",
       "      <td>0.610013</td>\n",
       "      <td>0.302476</td>\n",
       "      <td>0.517790</td>\n",
       "      <td>0.575492</td>\n",
       "      <td>0.476230</td>\n",
       "      <td>0.134256</td>\n",
       "      <td>0.212049</td>\n",
       "      <td>0.281559</td>\n",
       "      <td>0.464341</td>\n",
       "      <td>0.496377</td>\n",
       "      <td>0.509794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.361409</td>\n",
       "      <td>0.287395</td>\n",
       "      <td>0.770601</td>\n",
       "      <td>0.771129</td>\n",
       "      <td>0.417515</td>\n",
       "      <td>0.770599</td>\n",
       "      <td>0.855096</td>\n",
       "      <td>0.684369</td>\n",
       "      <td>0.189775</td>\n",
       "      <td>0.265077</td>\n",
       "      <td>0.394768</td>\n",
       "      <td>0.709336</td>\n",
       "      <td>0.739602</td>\n",
       "      <td>0.748531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       active_addresses  average_transaction_value  block_height  block_size  \\\n",
       "count        365.000000                 365.000000    365.000000  365.000000   \n",
       "mean           0.312659                   0.224696      0.513188    0.577511   \n",
       "std            0.171182                   0.136495      0.290541    0.223873   \n",
       "min            0.000000                   0.000000      0.000000    0.000000   \n",
       "25%            0.212106                   0.132698      0.262062    0.417905   \n",
       "50%            0.279694                   0.196649      0.517795    0.610013   \n",
       "75%            0.361409                   0.287395      0.770601    0.771129   \n",
       "max            1.000000                   1.000000      1.000000    1.000000   \n",
       "\n",
       "       block_time  current_supply  difficulty    hashrate  \\\n",
       "count  365.000000      365.000000  365.000000  365.000000   \n",
       "mean     0.325309        0.513188    0.550890    0.476739   \n",
       "std      0.162468        0.290539    0.301799    0.243572   \n",
       "min      0.000000        0.000000    0.000000    0.000000   \n",
       "25%      0.210205        0.262065    0.287124    0.268642   \n",
       "50%      0.302476        0.517790    0.575492    0.476230   \n",
       "75%      0.417515        0.770599    0.855096    0.684369   \n",
       "max      1.000000        1.000000    1.000000    1.000000   \n",
       "\n",
       "       large_transaction_count  new_addresses  transaction_count  \\\n",
       "count               365.000000     365.000000         365.000000   \n",
       "mean                  0.170867       0.224342           0.303136   \n",
       "std                   0.152479       0.133257           0.152195   \n",
       "min                   0.000000       0.000000           0.000000   \n",
       "25%                   0.098229       0.151384           0.198545   \n",
       "50%                   0.134256       0.212049           0.281559   \n",
       "75%                   0.189775       0.265077           0.394768   \n",
       "max                   1.000000       1.000000           1.000000   \n",
       "\n",
       "       transaction_count_all_time  unique_addresses_all_time  \\\n",
       "count                  365.000000                 365.000000   \n",
       "mean                     0.482607                   0.507523   \n",
       "std                      0.275745                   0.276457   \n",
       "min                      0.000000                   0.000000   \n",
       "25%                      0.246273                   0.271502   \n",
       "50%                      0.464341                   0.496377   \n",
       "75%                      0.709336                   0.739602   \n",
       "max                      1.000000                   1.000000   \n",
       "\n",
       "       zero_balance_addresses_all_time  \n",
       "count                       365.000000  \n",
       "mean                          0.518616  \n",
       "std                           0.275090  \n",
       "min                           0.000000  \n",
       "25%                           0.300481  \n",
       "50%                           0.509794  \n",
       "75%                           0.748531  \n",
       "max                           1.000000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = pd.read_csv('blockchain18.csv', index_col = 0)\n",
    "X = scale(block)\n",
    "print(X.columns)\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adjusted neurons for VAE (not enogh features for default setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  6 INLIERS :  359 ABOD\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 FB\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 HBOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 IF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  1 INLIERS :  364 Average KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 OCSVM\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 SOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 LOF\n",
      "Model: \"model_61\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_41 (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_221 (Dense)               (None, 14)           210         input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_222 (Dense)               (None, 56)           840         dense_221[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_121 (Dropout)           (None, 56)           0           dense_222[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_223 (Dense)               (None, 28)           1596        dropout_121[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_122 (Dropout)           (None, 28)           0           dense_223[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_224 (Dense)               (None, 14)           406         dropout_122[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_123 (Dropout)           (None, 14)           0           dense_224[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_225 (Dense)               (None, 2)            30          dropout_123[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_226 (Dense)               (None, 2)            30          dropout_123[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 2)            0           dense_225[0][0]                  \n",
      "                                                                 dense_226[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,112\n",
      "Trainable params: 3,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_42 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_227 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_228 (Dense)            (None, 14)                42        \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_229 (Dense)            (None, 28)                420       \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_230 (Dense)            (None, 56)                1624      \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_231 (Dense)            (None, 14)                798       \n",
      "=================================================================\n",
      "Total params: 2,890\n",
      "Trainable params: 2,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_62 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_62.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_41 (InputLayer)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "model_61 (Model)             [(None, 2), (None, 2), (N 3112      \n",
      "_________________________________________________________________\n",
      "model_62 (Model)             (None, 14)                2890      \n",
      "=================================================================\n",
      "Total params: 6,002\n",
      "Trainable params: 6,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 34.7772 - val_loss: 31.8541\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 27.1696 - val_loss: 27.3567\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 23.6009 - val_loss: 24.5240\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 21.2667 - val_loss: 22.6178\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 19.3582 - val_loss: 21.3488\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 18.1714 - val_loss: 20.3119\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 16.8925 - val_loss: 19.2047\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 16.0048 - val_loss: 18.6795\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 15.3864 - val_loss: 18.4461\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 14.8934 - val_loss: 18.0435\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 14.6356 - val_loss: 17.9382\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 14.4551 - val_loss: 17.7771\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 14.3202 - val_loss: 17.6506\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.2221 - val_loss: 17.5394\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.1465 - val_loss: 17.4697\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0913 - val_loss: 17.4173\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.0064 - val_loss: 17.3666\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 13.9502 - val_loss: 17.2936\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9246 - val_loss: 17.2686\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.8854 - val_loss: 17.2388\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8434 - val_loss: 17.2012\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8116 - val_loss: 17.2243\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.7963 - val_loss: 17.1837\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.7852 - val_loss: 17.1638\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8053 - val_loss: 17.1394\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.7488 - val_loss: 17.1478\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7671 - val_loss: 17.1227\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.7560 - val_loss: 17.0739\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 13.7373 - val_loss: 17.1236\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7352 - val_loss: 17.1088\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.7117 - val_loss: 17.1050\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7106 - val_loss: 17.1029\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7163 - val_loss: 17.0846\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6809 - val_loss: 17.0983\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.7056 - val_loss: 17.0920\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7211 - val_loss: 17.0806\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6941 - val_loss: 17.0852\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6874 - val_loss: 17.0592\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.6861 - val_loss: 17.0798\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6910 - val_loss: 17.0718\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.6799 - val_loss: 17.0827\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.6735 - val_loss: 17.0780\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.6807 - val_loss: 17.0905\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6744 - val_loss: 17.0537\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6779 - val_loss: 17.0614\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.6785 - val_loss: 17.0788\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 13.6733 - val_loss: 17.0731\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6797 - val_loss: 17.0554\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6701 - val_loss: 17.0539\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.6668 - val_loss: 17.0494\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6730 - val_loss: 17.0676\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6634 - val_loss: 17.0621\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6607 - val_loss: 17.0672\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6674 - val_loss: 17.0737\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.6732 - val_loss: 17.0488\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6656 - val_loss: 17.0577\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6754 - val_loss: 17.0631\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6674 - val_loss: 17.0667\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6633 - val_loss: 17.0596\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6655 - val_loss: 17.0471\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6692 - val_loss: 17.0616\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6692 - val_loss: 17.0743\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6626 - val_loss: 17.0468\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6733 - val_loss: 17.0597\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.6691 - val_loss: 17.0605\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.6609 - val_loss: 17.0652\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6735 - val_loss: 17.0572\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6661 - val_loss: 17.0627\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6638 - val_loss: 17.0579\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6610 - val_loss: 17.0672\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6715 - val_loss: 17.0529\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.6630 - val_loss: 17.0627\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6623 - val_loss: 17.0557\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6611 - val_loss: 17.0542\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.6620 - val_loss: 17.0570\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6643 - val_loss: 17.0573\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6624 - val_loss: 17.0565\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6558 - val_loss: 17.0607\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.6669 - val_loss: 17.0536\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6593 - val_loss: 17.0537\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.6619 - val_loss: 17.0565\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6614 - val_loss: 17.0528\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6575 - val_loss: 17.0503\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6624 - val_loss: 17.0526\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6637 - val_loss: 17.0530\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.6593 - val_loss: 17.0591\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.6659 - val_loss: 17.0558\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6614 - val_loss: 17.0529\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6588 - val_loss: 17.0530\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6630 - val_loss: 17.0538\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.6596 - val_loss: 17.0558\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6581 - val_loss: 17.0556\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6581 - val_loss: 17.0479\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6579 - val_loss: 17.0506\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.6556 - val_loss: 17.0521\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6572 - val_loss: 17.0546\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6574 - val_loss: 17.0495\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.6607 - val_loss: 17.0467\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6565 - val_loss: 17.0461\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.6587 - val_loss: 17.0483\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 VAE\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 ABOD\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  7 INLIERS :  358 FB\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 HBOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 IF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  7 INLIERS :  358 KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  2 INLIERS :  363 Average KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 OCSVM\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 SOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  6 INLIERS :  359 LOF\n",
      "Model: \"model_64\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_43 (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_232 (Dense)               (None, 14)           210         input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_233 (Dense)               (None, 56)           840         dense_232[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_127 (Dropout)           (None, 56)           0           dense_233[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_234 (Dense)               (None, 28)           1596        dropout_127[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_128 (Dropout)           (None, 28)           0           dense_234[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_235 (Dense)               (None, 14)           406         dropout_128[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_129 (Dropout)           (None, 14)           0           dense_235[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_236 (Dense)               (None, 2)            30          dropout_129[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_237 (Dense)               (None, 2)            30          dropout_129[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 2)            0           dense_236[0][0]                  \n",
      "                                                                 dense_237[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,112\n",
      "Trainable params: 3,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_44 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_238 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_239 (Dense)            (None, 14)                42        \n",
      "_________________________________________________________________\n",
      "dropout_130 (Dropout)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 28)                420       \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 56)                1624      \n",
      "_________________________________________________________________\n",
      "dropout_132 (Dropout)        (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_242 (Dense)            (None, 14)                798       \n",
      "=================================================================\n",
      "Total params: 2,890\n",
      "Trainable params: 2,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_65 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_65.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_43 (InputLayer)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "model_64 (Model)             [(None, 2), (None, 2), (N 3112      \n",
      "_________________________________________________________________\n",
      "model_65 (Model)             (None, 14)                2890      \n",
      "=================================================================\n",
      "Total params: 6,002\n",
      "Trainable params: 6,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 37.8419 - val_loss: 30.8411\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 29.1564 - val_loss: 25.5435\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 98us/step - loss: 24.6414 - val_loss: 22.7304\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 22.0967 - val_loss: 20.7524\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 98us/step - loss: 20.3130 - val_loss: 19.3865\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 100us/step - loss: 18.7360 - val_loss: 18.2681\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 113us/step - loss: 17.5333 - val_loss: 17.4055\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 16.5322 - val_loss: 16.6784\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 15.6604 - val_loss: 15.9776\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 100us/step - loss: 15.1241 - val_loss: 15.6680\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 14.8459 - val_loss: 15.8658\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.5317 - val_loss: 15.3214\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 14.4526 - val_loss: 15.2326\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.3860 - val_loss: 15.1820\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.2520 - val_loss: 15.1749\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.2076 - val_loss: 15.2017\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1579 - val_loss: 15.1858\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1202 - val_loss: 15.1022\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.0615 - val_loss: 15.0609\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0498 - val_loss: 15.0966\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.0328 - val_loss: 15.0939\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.9662 - val_loss: 15.0711\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.9756 - val_loss: 15.0695\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.9807 - val_loss: 15.0698\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 13.9769 - val_loss: 15.0414\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9623 - val_loss: 15.0654\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9444 - val_loss: 15.0576\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9400 - val_loss: 15.0481\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 13.9508 - val_loss: 15.0251\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9391 - val_loss: 15.0348\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.9331 - val_loss: 15.0144\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9263 - val_loss: 15.0319\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9287 - val_loss: 15.0218\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9290 - val_loss: 15.0066\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9321 - val_loss: 15.0434\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9125 - val_loss: 15.0390\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9228 - val_loss: 15.0218\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9280 - val_loss: 15.0185\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.9096 - val_loss: 15.0117\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.9128 - val_loss: 15.0174\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9131 - val_loss: 14.9960\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9086 - val_loss: 15.0201\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8972 - val_loss: 15.0203\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9107 - val_loss: 15.0146\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9030 - val_loss: 15.0103\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8960 - val_loss: 15.0151\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9127 - val_loss: 15.0085\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9088 - val_loss: 15.0064\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9029 - val_loss: 15.0078\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9018 - val_loss: 15.0170\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9086 - val_loss: 15.0175\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8897 - val_loss: 15.0163\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8993 - val_loss: 15.0218\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.9017 - val_loss: 15.0107\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9009 - val_loss: 15.0034\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8864 - val_loss: 15.0123\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8973 - val_loss: 15.0146\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8978 - val_loss: 15.0118\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8913 - val_loss: 15.0127\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9018 - val_loss: 15.0059\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8933 - val_loss: 15.0078\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8916 - val_loss: 15.0108\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8954 - val_loss: 15.0114\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8971 - val_loss: 15.0115\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8894 - val_loss: 15.0083\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.9004 - val_loss: 15.0083\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8951 - val_loss: 15.0094\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8934 - val_loss: 15.0032\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8930 - val_loss: 15.0128\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8887 - val_loss: 15.0145\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8903 - val_loss: 15.0127\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8852 - val_loss: 15.0089\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8863 - val_loss: 15.0034\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8929 - val_loss: 15.0110\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8893 - val_loss: 15.0148\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8941 - val_loss: 15.0055\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9008 - val_loss: 15.0047\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8927 - val_loss: 15.0098\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8925 - val_loss: 15.0129\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8912 - val_loss: 15.0098\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8965 - val_loss: 15.0077\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8882 - val_loss: 15.0034\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8903 - val_loss: 15.0087\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8960 - val_loss: 15.0089\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8888 - val_loss: 15.0097\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8950 - val_loss: 15.0071\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8909 - val_loss: 15.0074\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8913 - val_loss: 15.0104\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8940 - val_loss: 15.0118\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8933 - val_loss: 15.0078\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8919 - val_loss: 15.0059\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8884 - val_loss: 15.0071\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8879 - val_loss: 15.0064\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8902 - val_loss: 15.0090\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8871 - val_loss: 15.0084\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8902 - val_loss: 15.0129\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8898 - val_loss: 15.0103\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8937 - val_loss: 15.0111\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8907 - val_loss: 15.0119\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8932 - val_loss: 15.0093\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 VAE\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  15 INLIERS :  350 ABOD\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 FB\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 HBOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 IF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  8 INLIERS :  357 KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  2 INLIERS :  363 Average KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 OCSVM\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 SOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 LOF\n",
      "Model: \"model_67\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_45 (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_243 (Dense)               (None, 14)           210         input_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_244 (Dense)               (None, 56)           840         dense_243[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_133 (Dropout)           (None, 56)           0           dense_244[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_245 (Dense)               (None, 28)           1596        dropout_133[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_134 (Dropout)           (None, 28)           0           dense_245[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_246 (Dense)               (None, 14)           406         dropout_134[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_135 (Dropout)           (None, 14)           0           dense_246[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_247 (Dense)               (None, 2)            30          dropout_135[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_248 (Dense)               (None, 2)            30          dropout_135[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 2)            0           dense_247[0][0]                  \n",
      "                                                                 dense_248[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,112\n",
      "Trainable params: 3,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_46 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_249 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_250 (Dense)            (None, 14)                42        \n",
      "_________________________________________________________________\n",
      "dropout_136 (Dropout)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_251 (Dense)            (None, 28)                420       \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_252 (Dense)            (None, 56)                1624      \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_253 (Dense)            (None, 14)                798       \n",
      "=================================================================\n",
      "Total params: 2,890\n",
      "Trainable params: 2,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_68 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_68.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_45 (InputLayer)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "model_67 (Model)             [(None, 2), (None, 2), (N 3112      \n",
      "_________________________________________________________________\n",
      "model_68 (Model)             (None, 14)                2890      \n",
      "=================================================================\n",
      "Total params: 6,002\n",
      "Trainable params: 6,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 30.9201 - val_loss: 24.5587\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 98us/step - loss: 24.5387 - val_loss: 21.0002\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 21.4922 - val_loss: 19.1686\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 19.6505 - val_loss: 17.4247\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 18.2044 - val_loss: 16.3292\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 100us/step - loss: 17.3234 - val_loss: 15.2192\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 16.2104 - val_loss: 14.7886\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 15.6677 - val_loss: 14.3181\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 98us/step - loss: 15.4673 - val_loss: 13.6191\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 14.8983 - val_loss: 13.8479\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.7824 - val_loss: 13.5107\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.6317 - val_loss: 13.5980\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.5413 - val_loss: 13.2965\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.4353 - val_loss: 13.3690\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.3677 - val_loss: 13.2465\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.3250 - val_loss: 13.3116\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.3359 - val_loss: 13.2334\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.3055 - val_loss: 13.2454\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.2382 - val_loss: 13.2030\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.2359 - val_loss: 13.1562\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.2351 - val_loss: 13.1582\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.2220 - val_loss: 13.1559\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1692 - val_loss: 13.1148\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1737 - val_loss: 13.1475\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1672 - val_loss: 13.1108\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1933 - val_loss: 13.1072\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1215 - val_loss: 13.1530\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1717 - val_loss: 13.1050\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1433 - val_loss: 13.1394\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1552 - val_loss: 13.1090\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1371 - val_loss: 13.0732\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1540 - val_loss: 13.1000\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1616 - val_loss: 13.0982\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1387 - val_loss: 13.0939\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1306 - val_loss: 13.1045\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1402 - val_loss: 13.0709\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1291 - val_loss: 13.0857\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1198 - val_loss: 13.0673\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1353 - val_loss: 13.0972\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1291 - val_loss: 13.0631\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1232 - val_loss: 13.0627\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.1072 - val_loss: 13.0787\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1126 - val_loss: 13.0851\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1353 - val_loss: 13.0647\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.1148 - val_loss: 13.0794\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1156 - val_loss: 13.0726\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1084 - val_loss: 13.0819\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.1194 - val_loss: 13.0754\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1070 - val_loss: 13.0710\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1092 - val_loss: 13.0709\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.1119 - val_loss: 13.0780\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1189 - val_loss: 13.0839\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.1346 - val_loss: 13.0666\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1151 - val_loss: 13.0779\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1061 - val_loss: 13.0767\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1049 - val_loss: 13.0712\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1151 - val_loss: 13.0808\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1094 - val_loss: 13.0807\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.1115 - val_loss: 13.0775\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.1096 - val_loss: 13.0692\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1103 - val_loss: 13.0731\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1093 - val_loss: 13.0827\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1082 - val_loss: 13.0757\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1083 - val_loss: 13.0731\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1111 - val_loss: 13.0722\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1092 - val_loss: 13.0740\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1051 - val_loss: 13.0770\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1117 - val_loss: 13.0700\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1139 - val_loss: 13.0749\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1148 - val_loss: 13.0715\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0977 - val_loss: 13.0743\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.1036 - val_loss: 13.0746\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1051 - val_loss: 13.0754\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1143 - val_loss: 13.0789\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.1066 - val_loss: 13.0779\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.1066 - val_loss: 13.0712\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1029 - val_loss: 13.0745\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1081 - val_loss: 13.0688\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1121 - val_loss: 13.0689\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1010 - val_loss: 13.0777\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1020 - val_loss: 13.0767\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1103 - val_loss: 13.0697\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 74us/step - loss: 14.1043 - val_loss: 13.0702\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.1032 - val_loss: 13.0699\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1048 - val_loss: 13.0793\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1016 - val_loss: 13.0714\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1053 - val_loss: 13.0703\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1025 - val_loss: 13.0694\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0996 - val_loss: 13.0697\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1172 - val_loss: 13.0704\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1012 - val_loss: 13.0707\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1066 - val_loss: 13.0706\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.1057 - val_loss: 13.0723\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1005 - val_loss: 13.0743\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1031 - val_loss: 13.0707\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1026 - val_loss: 13.0720\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1139 - val_loss: 13.0722\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1067 - val_loss: 13.0710\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0969 - val_loss: 13.0751\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1024 - val_loss: 13.0691\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 VAE\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  18 INLIERS :  347 ABOD\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  14 INLIERS :  351 FB\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 HBOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 IF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  11 INLIERS :  354 KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  2 INLIERS :  363 Average KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 OCSVM\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 SOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  14 INLIERS :  351 LOF\n",
      "Model: \"model_70\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_47 (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_254 (Dense)               (None, 14)           210         input_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_255 (Dense)               (None, 56)           840         dense_254[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_139 (Dropout)           (None, 56)           0           dense_255[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_256 (Dense)               (None, 28)           1596        dropout_139[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_140 (Dropout)           (None, 28)           0           dense_256[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_257 (Dense)               (None, 14)           406         dropout_140[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_141 (Dropout)           (None, 14)           0           dense_257[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_258 (Dense)               (None, 2)            30          dropout_141[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_259 (Dense)               (None, 2)            30          dropout_141[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 2)            0           dense_258[0][0]                  \n",
      "                                                                 dense_259[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,112\n",
      "Trainable params: 3,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_48 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_260 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_261 (Dense)            (None, 14)                42        \n",
      "_________________________________________________________________\n",
      "dropout_142 (Dropout)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_262 (Dense)            (None, 28)                420       \n",
      "_________________________________________________________________\n",
      "dropout_143 (Dropout)        (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_263 (Dense)            (None, 56)                1624      \n",
      "_________________________________________________________________\n",
      "dropout_144 (Dropout)        (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_264 (Dense)            (None, 14)                798       \n",
      "=================================================================\n",
      "Total params: 2,890\n",
      "Trainable params: 2,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_71 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_71.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_47 (InputLayer)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "model_70 (Model)             [(None, 2), (None, 2), (N 3112      \n",
      "_________________________________________________________________\n",
      "model_71 (Model)             (None, 14)                2890      \n",
      "=================================================================\n",
      "Total params: 6,002\n",
      "Trainable params: 6,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 46.4089 - val_loss: 32.3017\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 34.6002 - val_loss: 26.4561\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 98us/step - loss: 28.2941 - val_loss: 23.0905\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 24.6081 - val_loss: 20.6639\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 98us/step - loss: 22.1082 - val_loss: 19.1015\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 97us/step - loss: 20.0203 - val_loss: 17.6499\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 18.6676 - val_loss: 16.4573\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 17.2774 - val_loss: 16.1454\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 125us/step - loss: 16.5026 - val_loss: 14.9622\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 15.7836 - val_loss: 14.4613\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 15.3080 - val_loss: 14.5022\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 15.0282 - val_loss: 14.1735\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.8932 - val_loss: 13.9472\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.6959 - val_loss: 13.8629\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.5675 - val_loss: 13.7892\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.4886 - val_loss: 13.8310\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.4232 - val_loss: 13.8270\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.3748 - val_loss: 13.7134\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.3284 - val_loss: 13.7314\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.3158 - val_loss: 13.7606\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.2878 - val_loss: 13.6863\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.2183 - val_loss: 13.6803\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.2162 - val_loss: 13.6738\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1546 - val_loss: 13.5574\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1704 - val_loss: 13.6410\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1677 - val_loss: 13.6333\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1465 - val_loss: 13.5930\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1391 - val_loss: 13.5853\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1385 - val_loss: 13.5234\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1270 - val_loss: 13.5459\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1346 - val_loss: 13.5563\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1119 - val_loss: 13.5598\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1364 - val_loss: 13.5685\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1019 - val_loss: 13.5767\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0762 - val_loss: 13.5785\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.1056 - val_loss: 13.5306\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1028 - val_loss: 13.5164\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.1004 - val_loss: 13.5507\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0595 - val_loss: 13.5141\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0760 - val_loss: 13.5833\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0896 - val_loss: 13.5174\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.1182 - val_loss: 13.5130\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0733 - val_loss: 13.5351\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0890 - val_loss: 13.5165\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0719 - val_loss: 13.5240\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0805 - val_loss: 13.5156\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0764 - val_loss: 13.5225\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0444 - val_loss: 13.5192\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0909 - val_loss: 13.4989\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0958 - val_loss: 13.5041\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0516 - val_loss: 13.4979\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0665 - val_loss: 13.5160\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0642 - val_loss: 13.5214\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0831 - val_loss: 13.5343\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0749 - val_loss: 13.5125\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0567 - val_loss: 13.5057\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0676 - val_loss: 13.5043\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 74us/step - loss: 14.0700 - val_loss: 13.5269\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0677 - val_loss: 13.5106\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0705 - val_loss: 13.5040\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0746 - val_loss: 13.5072\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0677 - val_loss: 13.5250\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0716 - val_loss: 13.5028\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0662 - val_loss: 13.5013\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0657 - val_loss: 13.5039\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0681 - val_loss: 13.4973\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0566 - val_loss: 13.5045\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0517 - val_loss: 13.5026\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0517 - val_loss: 13.5042\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0658 - val_loss: 13.5293\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0530 - val_loss: 13.5225\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0871 - val_loss: 13.4930\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0460 - val_loss: 13.5173\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0671 - val_loss: 13.5170\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0724 - val_loss: 13.4988\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0611 - val_loss: 13.5202\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0752 - val_loss: 13.4997\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0607 - val_loss: 13.5126\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 74us/step - loss: 14.0646 - val_loss: 13.5076\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0514 - val_loss: 13.5024\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0484 - val_loss: 13.5018\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0706 - val_loss: 13.5129\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 74us/step - loss: 14.0670 - val_loss: 13.5021\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0633 - val_loss: 13.5045\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0703 - val_loss: 13.5007\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0493 - val_loss: 13.5158\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0507 - val_loss: 13.5047\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 71us/step - loss: 14.0585 - val_loss: 13.5067\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0584 - val_loss: 13.5130\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0668 - val_loss: 13.4975\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0683 - val_loss: 13.5053\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0689 - val_loss: 13.5042\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0492 - val_loss: 13.5038\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0741 - val_loss: 13.5060\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0750 - val_loss: 13.5026\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 73us/step - loss: 14.0560 - val_loss: 13.4985\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 14.0643 - val_loss: 13.5020\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 14.0584 - val_loss: 13.5032\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 14.0602 - val_loss: 13.4974\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 73us/step - loss: 14.0549 - val_loss: 13.4999\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 VAE\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  24 INLIERS :  341 ABOD\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  16 INLIERS :  349 FB\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 HBOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 IF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  15 INLIERS :  350 KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  4 INLIERS :  361 Average KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 OCSVM\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 SOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  17 INLIERS :  348 LOF\n",
      "Model: \"model_73\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_49 (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_265 (Dense)               (None, 14)           210         input_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_266 (Dense)               (None, 56)           840         dense_265[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_145 (Dropout)           (None, 56)           0           dense_266[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_267 (Dense)               (None, 28)           1596        dropout_145[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_146 (Dropout)           (None, 28)           0           dense_267[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_268 (Dense)               (None, 14)           406         dropout_146[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_147 (Dropout)           (None, 14)           0           dense_268[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_269 (Dense)               (None, 2)            30          dropout_147[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_270 (Dense)               (None, 2)            30          dropout_147[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 2)            0           dense_269[0][0]                  \n",
      "                                                                 dense_270[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,112\n",
      "Trainable params: 3,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_50 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_271 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_272 (Dense)            (None, 14)                42        \n",
      "_________________________________________________________________\n",
      "dropout_148 (Dropout)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_273 (Dense)            (None, 28)                420       \n",
      "_________________________________________________________________\n",
      "dropout_149 (Dropout)        (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_274 (Dense)            (None, 56)                1624      \n",
      "_________________________________________________________________\n",
      "dropout_150 (Dropout)        (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_275 (Dense)            (None, 14)                798       \n",
      "=================================================================\n",
      "Total params: 2,890\n",
      "Trainable params: 2,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_74 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_74.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_49 (InputLayer)        (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "model_73 (Model)             [(None, 2), (None, 2), (N 3112      \n",
      "_________________________________________________________________\n",
      "model_74 (Model)             (None, 14)                2890      \n",
      "=================================================================\n",
      "Total params: 6,002\n",
      "Trainable params: 6,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 36.7090 - val_loss: 31.8700\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 28.8220 - val_loss: 27.1068\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 96us/step - loss: 24.5771 - val_loss: 24.0861\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 96us/step - loss: 21.6658 - val_loss: 22.0927\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 97us/step - loss: 19.6803 - val_loss: 20.7475\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 18.2615 - val_loss: 19.7455\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 96us/step - loss: 17.0464 - val_loss: 18.3868\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 95us/step - loss: 16.3076 - val_loss: 17.6874\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 125us/step - loss: 15.4781 - val_loss: 17.2529\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 15.1596 - val_loss: 16.7815\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 14.8246 - val_loss: 16.8270\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 14.7858 - val_loss: 16.4124\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.5038 - val_loss: 16.1081\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.3688 - val_loss: 16.2503\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.2253 - val_loss: 16.0424\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.1802 - val_loss: 15.9750\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.1436 - val_loss: 16.0329\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.0695 - val_loss: 16.0161\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 14.0469 - val_loss: 15.9516\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 14.0204 - val_loss: 15.9186\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9861 - val_loss: 15.8280\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 14.0012 - val_loss: 15.9319\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.9614 - val_loss: 15.8519\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9324 - val_loss: 15.8347\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9197 - val_loss: 15.7710\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8924 - val_loss: 15.8125\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9020 - val_loss: 15.8167\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.9004 - val_loss: 15.7394\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8813 - val_loss: 15.8416\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8705 - val_loss: 15.7298\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8484 - val_loss: 15.7722\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8736 - val_loss: 15.7699\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.8686 - val_loss: 15.7996\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8357 - val_loss: 15.7252\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8681 - val_loss: 15.7695\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8411 - val_loss: 15.7276\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8239 - val_loss: 15.7507\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 74us/step - loss: 13.8607 - val_loss: 15.7755\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.8381 - val_loss: 15.7098\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8279 - val_loss: 15.7417\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8222 - val_loss: 15.7712\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8394 - val_loss: 15.7645\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8453 - val_loss: 15.7261\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8451 - val_loss: 15.7546\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8267 - val_loss: 15.7418\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8295 - val_loss: 15.7641\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8363 - val_loss: 15.7223\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8246 - val_loss: 15.7373\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8102 - val_loss: 15.7193\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8242 - val_loss: 15.7216\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8296 - val_loss: 15.7296\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8079 - val_loss: 15.7554\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8312 - val_loss: 15.7189\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.7925 - val_loss: 15.7291\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8234 - val_loss: 15.7241\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8204 - val_loss: 15.7213\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.7996 - val_loss: 15.7268\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8091 - val_loss: 15.6981\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8277 - val_loss: 15.7191\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8053 - val_loss: 15.7003\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8144 - val_loss: 15.7234\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8136 - val_loss: 15.7511\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8094 - val_loss: 15.7077\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8047 - val_loss: 15.7153\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8174 - val_loss: 15.7250\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8249 - val_loss: 15.7130\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8172 - val_loss: 15.7224\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8078 - val_loss: 15.7274\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8076 - val_loss: 15.7217\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8102 - val_loss: 15.7329\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8034 - val_loss: 15.7194\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 74us/step - loss: 13.8257 - val_loss: 15.7089\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8187 - val_loss: 15.7137\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8124 - val_loss: 15.7103\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8224 - val_loss: 15.7145\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8160 - val_loss: 15.7108\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8084 - val_loss: 15.7047\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8301 - val_loss: 15.7047\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8126 - val_loss: 15.7120\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 74us/step - loss: 13.8054 - val_loss: 15.7045\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8065 - val_loss: 15.7223\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8058 - val_loss: 15.7089\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8162 - val_loss: 15.7135\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8108 - val_loss: 15.7179\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8021 - val_loss: 15.7148\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 78us/step - loss: 13.8133 - val_loss: 15.6944\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8065 - val_loss: 15.7329\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 13.8128 - val_loss: 15.7155\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8028 - val_loss: 15.7208\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8185 - val_loss: 15.6945\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8022 - val_loss: 15.7054\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 77us/step - loss: 13.8121 - val_loss: 15.7157\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8183 - val_loss: 15.7145\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7926 - val_loss: 15.7080\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8160 - val_loss: 15.6982\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8118 - val_loss: 15.7236\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.7994 - val_loss: 15.7290\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8060 - val_loss: 15.7004\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 76us/step - loss: 13.8177 - val_loss: 15.7020\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 75us/step - loss: 13.8207 - val_loss: 15.7088\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  18 INLIERS :  347 VAE\n"
     ]
    }
   ],
   "source": [
    "# time the code\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "# empty list to store outliers and inliners\n",
    "df_outliers = []\n",
    "scores_pred_lst = []\n",
    "proba_lst = []\n",
    "predict_rank_lst = []\n",
    "\n",
    "for fraction in outliers_fraction:\n",
    "    # Define 13 outlier detection tools to be compared\n",
    "    classifiers = {#'AE' : AutoEncoder(contamination=outliers_fraction, random_state=random_state), # check why error\n",
    "               'ABOD': ABOD(contamination = fraction),\n",
    "              'CBLOF':CBLOF(check_estimator=False, random_state=random_state), # getting 0 \n",
    "              'FB':FeatureBagging(LOF(n_neighbors=35),contamination=fraction,check_estimator=False,random_state=random_state),\n",
    "              'HBOS': HBOS(contamination=fraction),\n",
    "              'IF': IForest(contamination=fraction,random_state=random_state),\n",
    "              'KNN': KNN(contamination=fraction),\n",
    "              'Average KNN': KNN(method='mean',contamination=fraction),\n",
    "              'OCSVM' : OCSVM(contamination=fraction),\n",
    "              'SOS' : SOS(contamination=fraction),\n",
    "              'LOF': LOF(n_neighbors=35, contamination = fraction),\n",
    "              #'LOCI': LOCI(contamination = fraction), # takes a super long time + don't understand\n",
    "              'VAE' : VAE(contamination=fraction, random_state=random_state,encoder_neurons=[56, 28, 14],\n",
    "                         decoder_neurons=[14,28,56]) # default nbr of neurons too much for blockchain dataset\n",
    "              }\n",
    "\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        clf.fit(X)\n",
    "        # predict raw anomaly score\n",
    "        scores_pred = clf.decision_function(X) * -1 # WHY? * -1\n",
    "\n",
    "        # prediction of a datapoint category outlier or inlier\n",
    "        y_pred = clf.predict(X)\n",
    "\n",
    "        # probability of outlier \n",
    "        probability = clf.predict_proba(X, method = 'linear')\n",
    "\n",
    "        # predict rank\n",
    "        predict_rank = clf._predict_rank(X)\n",
    "\n",
    "        n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "        n_outliers = np.count_nonzero(y_pred == 1)\n",
    "\n",
    "        # copy of dataframe\n",
    "        dfx = X.copy()\n",
    "\n",
    "        dfx[clf_name] = y_pred.tolist()\n",
    "        #print(clf_name,'\\n',i,clf,'\\n',classifiers,'\\n',classifiers.items())\n",
    "        print('Outlier fraction: ', fraction)\n",
    "        print('OUTLIERS : ',n_outliers,'INLIERS : ',n_inliers, clf_name)\n",
    "        #print('scores_pred: ', scores_pred)\n",
    "        scores_pred_lst.append(scores_pred)\n",
    "        df_outliers.append(dfx[clf_name]) \n",
    "        proba_lst.append(probability)\n",
    "        predict_rank_lst.append(predict_rank)\n",
    "t1 = time.time()\n",
    "total = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run:  36.16200065612793 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Time to run: ',total,'s')\n",
    "# we have 50 lists (or essentially 5 with 7 models)\n",
    "len(df_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_oo1 = df_outliers[0:11]\n",
    "frac_oo2 = df_outliers[11:22]\n",
    "frac_oo3 = df_outliers[22:33]\n",
    "frac_oo4 = df_outliers[33:44]\n",
    "frac_oo5 = df_outliers[44:]\n",
    "# use the function to extract the dates\n",
    "frac_oo1_outliers = outlier_subset_function(frac_oo1)\n",
    "frac_oo2_outliers = outlier_subset_function(frac_oo2)\n",
    "frac_oo3_outliers = outlier_subset_function(frac_oo3)\n",
    "frac_oo4_outliers = outlier_subset_function(frac_oo4)\n",
    "frac_oo5_outliers = outlier_subset_function(frac_oo5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- updated the function list lenght above (because here we have less models \n",
    "- also with the number to divide to get weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Anomaly Dates:  51\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AnomalyCount_Fractions</th>\n",
       "      <th>AnomalyCount_Models</th>\n",
       "      <th>weight_models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.033254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-05-11</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.019002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.040380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-22</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.014252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>0.059382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.028504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.026128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.026128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.019002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-04-24</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.023753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.014252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-11-21</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.019002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.023753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>0.049881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.040380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.023753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.035629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.028504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.026128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.019002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-04-07</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.016627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.035629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.038005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-12-23</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.016627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-10-14</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.016627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.028504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.028504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-11-29</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>0.054632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2018-12-25</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.026128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2018-06-23</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2018-06-27</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2018-01-27</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2018-03-13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2018-10-03</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2018-06-04</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2018-03-03</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2018-04-14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2018-07-22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2018-06-03</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  AnomalyCount_Fractions  AnomalyCount_Models  weight_models\n",
       "0   2018-01-09                       5                   14       0.033254\n",
       "1   2018-05-11                       5                    5       0.011876\n",
       "2   2018-12-30                       5                    8       0.019002\n",
       "3   2018-01-11                       5                   17       0.040380\n",
       "4   2018-12-22                       5                    6       0.014252\n",
       "5   2018-01-04                       5                   25       0.059382\n",
       "6   2018-01-08                       5                   12       0.028504\n",
       "7   2018-01-16                       5                   11       0.026128\n",
       "8   2018-01-18                       5                   11       0.026128\n",
       "9   2018-01-13                       5                    8       0.019002\n",
       "10  2018-04-24                       5                   10       0.023753\n",
       "11  2018-01-19                       5                    6       0.014252\n",
       "12  2018-11-21                       5                    5       0.011876\n",
       "13  2018-01-15                       5                    8       0.019002\n",
       "14  2018-01-28                       5                   10       0.023753\n",
       "15  2018-12-27                       5                    5       0.011876\n",
       "16  2018-01-03                       5                   21       0.049881\n",
       "17  2018-01-06                       5                   17       0.040380\n",
       "18  2018-02-25                       5                   10       0.023753\n",
       "19  2018-03-17                       5                   15       0.035629\n",
       "20  2018-01-17                       5                   12       0.028504\n",
       "21  2018-01-01                       5                   11       0.026128\n",
       "22  2018-04-25                       5                    8       0.019002\n",
       "23  2018-04-07                       5                    5       0.011876\n",
       "24  2018-11-30                       5                    7       0.016627\n",
       "25  2018-01-02                       5                   15       0.035629\n",
       "26  2018-01-07                       5                   16       0.038005\n",
       "27  2018-12-23                       5                    7       0.016627\n",
       "28  2018-04-01                       5                    5       0.011876\n",
       "29  2018-10-14                       5                    7       0.016627\n",
       "30  2018-01-10                       5                   12       0.028504\n",
       "31  2018-01-31                       5                   12       0.028504\n",
       "32  2018-11-29                       5                    5       0.011876\n",
       "33  2018-04-08                       5                    5       0.011876\n",
       "34  2018-01-05                       5                   23       0.054632\n",
       "35  2018-12-25                       5                    5       0.011876\n",
       "36  2018-01-12                       5                   11       0.026128\n",
       "37  2018-06-23                       4                    4       0.009501\n",
       "38  2018-07-03                       3                    3       0.007126\n",
       "39  2018-06-27                       3                    3       0.007126\n",
       "40  2018-01-27                       3                    3       0.007126\n",
       "41  2018-03-13                       3                    3       0.007126\n",
       "42  2018-10-03                       3                    3       0.007126\n",
       "43  2018-06-04                       3                    3       0.007126\n",
       "44  2018-03-03                       2                    2       0.004751\n",
       "45  2018-07-18                       2                    2       0.004751\n",
       "46  2018-01-21                       1                    1       0.002375\n",
       "47  2018-01-20                       1                    1       0.002375\n",
       "48  2018-04-14                       1                    1       0.002375\n",
       "49  2018-07-22                       1                    1       0.002375\n",
       "50  2018-06-03                       1                    1       0.002375"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_anomalies = date_ranking(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)\n",
    "print('Unique Anomaly Dates: ',len(block_anomalies.Date.unique()))\n",
    "block_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction 0.01 unique dates:  37\n",
      "Outlier fraction 0.02 unique dates:  38\n",
      "Outlier fraction 0.03 unique dates:  44\n",
      "Outlier fraction 0.04 unique dates:  46\n",
      "Outlier fraction 0.05 unique dates:  51\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>001</th>\n",
       "      <th>002</th>\n",
       "      <th>003</th>\n",
       "      <th>004</th>\n",
       "      <th>005</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-03</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-07</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-08</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-03</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-04</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-23</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-03</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-03</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            001  002  003  004  005\n",
       "2018-01-01    1    1    1    1    1\n",
       "2018-01-02    1    1    1    1    1\n",
       "2018-01-03    1    1    1    1    1\n",
       "2018-01-04    1    1    1    1    1\n",
       "2018-01-05    1    1    1    1    1\n",
       "2018-01-06    1    1    1    1    1\n",
       "2018-01-07    1    1    1    1    1\n",
       "2018-01-08    1    1    1    1    1\n",
       "2018-01-09    1    1    1    1    1\n",
       "2018-01-10    1    1    1    1    1\n",
       "2018-01-11    1    1    1    1    1\n",
       "2018-01-12    1    1    1    1    1\n",
       "2018-01-13    1    1    1    1    1\n",
       "2018-01-15    1    1    1    1    1\n",
       "2018-01-16    1    1    1    1    1\n",
       "2018-01-17    1    1    1    1    1\n",
       "2018-01-18    1    1    1    1    1\n",
       "2018-01-19    1    1    1    1    1\n",
       "2018-01-20    0    0    0    0    1\n",
       "2018-01-21    0    0    0    0    1\n",
       "2018-01-27    0    0    1    1    1\n",
       "2018-01-28    1    1    1    1    1\n",
       "2018-01-31    1    1    1    1    1\n",
       "2018-02-25    1    1    1    1    1\n",
       "2018-03-03    0    0    0    1    1\n",
       "2018-03-13    0    0    1    1    1\n",
       "2018-03-17    1    1    1    1    1\n",
       "2018-04-01    1    1    1    1    1\n",
       "2018-04-07    1    1    1    1    1\n",
       "2018-04-08    1    1    1    1    1\n",
       "2018-04-14    0    0    0    0    1\n",
       "2018-04-24    1    1    1    1    1\n",
       "2018-04-25    1    1    1    1    1\n",
       "2018-05-11    1    1    1    1    1\n",
       "2018-06-03    0    0    0    0    1\n",
       "2018-06-04    0    0    1    1    1\n",
       "2018-06-23    0    1    1    1    1\n",
       "2018-06-27    0    0    1    1    1\n",
       "2018-07-03    0    0    1    1    1\n",
       "2018-07-18    0    0    0    1    1\n",
       "2018-07-22    0    0    0    0    1\n",
       "2018-10-03    0    0    1    1    1\n",
       "2018-10-14    1    1    1    1    1\n",
       "2018-11-21    1    1    1    1    1\n",
       "2018-11-29    1    1    1    1    1\n",
       "2018-11-30    1    1    1    1    1\n",
       "2018-12-22    1    1    1    1    1\n",
       "2018-12-23    1    1    1    1    1\n",
       "2018-12-25    1    1    1    1    1\n",
       "2018-12-27    1    1    1    1    1\n",
       "2018-12-30    1    1    1    1    1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_per_fraction(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the outliers lists uniquely for later\n",
    "block_df = df_outliers\n",
    "# probability lists\n",
    "block_proba = proba_lst\n",
    "# rank lists\n",
    "block_rank = predict_rank_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Dataset Test \n",
    "- drop code_repo_contributors as it only has 0 \n",
    "- SOS gives Runtime error for some reason "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['comments', 'posts', 'followers', 'points', 'overview_page_views',\n",
      "       'analysis_page_views', 'markets_page_views', 'charts_page_views',\n",
      "       'trades_page_views', 'forum_page_views', 'influence_page_views',\n",
      "       'total_page_views', 'fb_likes', 'fb_talking_about', 'twitter_followers',\n",
      "       'twitter_following', 'twitter_lists', 'twitter_favourites',\n",
      "       'twitter_statuses', 'reddit_subscribers', 'reddit_active_users',\n",
      "       'reddit_posts_per_hour', 'reddit_posts_per_day',\n",
      "       'reddit_comments_per_hour', 'reddit_comments_per_day',\n",
      "       'code_repo_stars', 'code_repo_forks', 'code_repo_subscribers',\n",
      "       'code_repo_open_pull_issues', 'code_repo_closed_pull_issues',\n",
      "       'code_repo_open_issues', 'code_repo_closed_issues'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>posts</th>\n",
       "      <th>followers</th>\n",
       "      <th>points</th>\n",
       "      <th>overview_page_views</th>\n",
       "      <th>analysis_page_views</th>\n",
       "      <th>markets_page_views</th>\n",
       "      <th>charts_page_views</th>\n",
       "      <th>trades_page_views</th>\n",
       "      <th>forum_page_views</th>\n",
       "      <th>...</th>\n",
       "      <th>reddit_posts_per_day</th>\n",
       "      <th>reddit_comments_per_hour</th>\n",
       "      <th>reddit_comments_per_day</th>\n",
       "      <th>code_repo_stars</th>\n",
       "      <th>code_repo_forks</th>\n",
       "      <th>code_repo_subscribers</th>\n",
       "      <th>code_repo_open_pull_issues</th>\n",
       "      <th>code_repo_closed_pull_issues</th>\n",
       "      <th>code_repo_open_issues</th>\n",
       "      <th>code_repo_closed_issues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.553743</td>\n",
       "      <td>0.599464</td>\n",
       "      <td>0.667312</td>\n",
       "      <td>0.588562</td>\n",
       "      <td>0.658026</td>\n",
       "      <td>0.676191</td>\n",
       "      <td>0.710861</td>\n",
       "      <td>0.688210</td>\n",
       "      <td>0.666003</td>\n",
       "      <td>0.609329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278695</td>\n",
       "      <td>0.196713</td>\n",
       "      <td>0.196708</td>\n",
       "      <td>0.648723</td>\n",
       "      <td>0.680061</td>\n",
       "      <td>0.737169</td>\n",
       "      <td>0.414537</td>\n",
       "      <td>0.500259</td>\n",
       "      <td>0.401739</td>\n",
       "      <td>0.542089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.284817</td>\n",
       "      <td>0.275271</td>\n",
       "      <td>0.214088</td>\n",
       "      <td>0.269895</td>\n",
       "      <td>0.260020</td>\n",
       "      <td>0.261618</td>\n",
       "      <td>0.241494</td>\n",
       "      <td>0.259187</td>\n",
       "      <td>0.268737</td>\n",
       "      <td>0.279112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169271</td>\n",
       "      <td>0.163744</td>\n",
       "      <td>0.163743</td>\n",
       "      <td>0.265907</td>\n",
       "      <td>0.266401</td>\n",
       "      <td>0.255689</td>\n",
       "      <td>0.215412</td>\n",
       "      <td>0.301799</td>\n",
       "      <td>0.244852</td>\n",
       "      <td>0.291751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.332275</td>\n",
       "      <td>0.409717</td>\n",
       "      <td>0.566065</td>\n",
       "      <td>0.397166</td>\n",
       "      <td>0.501349</td>\n",
       "      <td>0.514024</td>\n",
       "      <td>0.595820</td>\n",
       "      <td>0.574112</td>\n",
       "      <td>0.511107</td>\n",
       "      <td>0.420929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175227</td>\n",
       "      <td>0.090028</td>\n",
       "      <td>0.090025</td>\n",
       "      <td>0.471224</td>\n",
       "      <td>0.511847</td>\n",
       "      <td>0.601688</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.239695</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.327409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.574602</td>\n",
       "      <td>0.630516</td>\n",
       "      <td>0.705386</td>\n",
       "      <td>0.616027</td>\n",
       "      <td>0.721745</td>\n",
       "      <td>0.738860</td>\n",
       "      <td>0.777052</td>\n",
       "      <td>0.774184</td>\n",
       "      <td>0.727668</td>\n",
       "      <td>0.663521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260633</td>\n",
       "      <td>0.149065</td>\n",
       "      <td>0.149058</td>\n",
       "      <td>0.697656</td>\n",
       "      <td>0.740560</td>\n",
       "      <td>0.814346</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.491935</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>0.547240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.848427</td>\n",
       "      <td>0.802549</td>\n",
       "      <td>0.827863</td>\n",
       "      <td>0.872159</td>\n",
       "      <td>0.896440</td>\n",
       "      <td>0.892082</td>\n",
       "      <td>0.887800</td>\n",
       "      <td>0.891561</td>\n",
       "      <td>0.851346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331805</td>\n",
       "      <td>0.240735</td>\n",
       "      <td>0.240726</td>\n",
       "      <td>0.875716</td>\n",
       "      <td>0.915828</td>\n",
       "      <td>0.947679</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.773297</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.781104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         comments       posts   followers      points  overview_page_views  \\\n",
       "count  365.000000  365.000000  365.000000  365.000000           365.000000   \n",
       "mean     0.553743    0.599464    0.667312    0.588562             0.658026   \n",
       "std      0.284817    0.275271    0.214088    0.269895             0.260020   \n",
       "min      0.000000    0.000000    0.000000    0.000000             0.000000   \n",
       "25%      0.332275    0.409717    0.566065    0.397166             0.501349   \n",
       "50%      0.574602    0.630516    0.705386    0.616027             0.721745   \n",
       "75%      0.817308    0.848427    0.802549    0.827863             0.872159   \n",
       "max      1.000000    1.000000    1.000000    1.000000             1.000000   \n",
       "\n",
       "       analysis_page_views  markets_page_views  charts_page_views  \\\n",
       "count           365.000000          365.000000         365.000000   \n",
       "mean              0.676191            0.710861           0.688210   \n",
       "std               0.261618            0.241494           0.259187   \n",
       "min               0.000000            0.000000           0.000000   \n",
       "25%               0.514024            0.595820           0.574112   \n",
       "50%               0.738860            0.777052           0.774184   \n",
       "75%               0.896440            0.892082           0.887800   \n",
       "max               1.000000            1.000000           1.000000   \n",
       "\n",
       "       trades_page_views  forum_page_views  ...  reddit_posts_per_day  \\\n",
       "count         365.000000        365.000000  ...            365.000000   \n",
       "mean            0.666003          0.609329  ...              0.278695   \n",
       "std             0.268737          0.279112  ...              0.169271   \n",
       "min             0.000000          0.000000  ...              0.000000   \n",
       "25%             0.511107          0.420929  ...              0.175227   \n",
       "50%             0.727668          0.663521  ...              0.260633   \n",
       "75%             0.891561          0.851346  ...              0.331805   \n",
       "max             1.000000          1.000000  ...              1.000000   \n",
       "\n",
       "       reddit_comments_per_hour  reddit_comments_per_day  code_repo_stars  \\\n",
       "count                365.000000               365.000000       365.000000   \n",
       "mean                   0.196713                 0.196708         0.648723   \n",
       "std                    0.163744                 0.163743         0.265907   \n",
       "min                    0.000000                 0.000000         0.000000   \n",
       "25%                    0.090028                 0.090025         0.471224   \n",
       "50%                    0.149065                 0.149058         0.697656   \n",
       "75%                    0.240735                 0.240726         0.875716   \n",
       "max                    1.000000                 1.000000         1.000000   \n",
       "\n",
       "       code_repo_forks  code_repo_subscribers  code_repo_open_pull_issues  \\\n",
       "count       365.000000             365.000000                  365.000000   \n",
       "mean          0.680061               0.737169                    0.414537   \n",
       "std           0.266401               0.255689                    0.215412   \n",
       "min           0.000000               0.000000                    0.000000   \n",
       "25%           0.511847               0.601688                    0.258824   \n",
       "50%           0.740560               0.814346                    0.400000   \n",
       "75%           0.915828               0.947679                    0.541176   \n",
       "max           1.000000               1.000000                    1.000000   \n",
       "\n",
       "       code_repo_closed_pull_issues  code_repo_open_issues  \\\n",
       "count                    365.000000             365.000000   \n",
       "mean                       0.500259               0.401739   \n",
       "std                        0.301799               0.244852   \n",
       "min                        0.000000               0.000000   \n",
       "25%                        0.239695               0.211538   \n",
       "50%                        0.491935               0.365385   \n",
       "75%                        0.773297               0.602564   \n",
       "max                        1.000000               1.000000   \n",
       "\n",
       "       code_repo_closed_issues  \n",
       "count               365.000000  \n",
       "mean                  0.542089  \n",
       "std                   0.291751  \n",
       "min                   0.000000  \n",
       "25%                   0.327409  \n",
       "50%                   0.547240  \n",
       "75%                   0.781104  \n",
       "max                   1.000000  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social = pd.read_csv(\"social18.csv\", index_col = 0).drop('code_repo_contributors', axis = 1)\n",
    "X = scale(social)\n",
    "print(X.columns)\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  0 INLIERS :  365 ABOD\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 FB\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 HBOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 IF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  2 INLIERS :  363 Average KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 OCSVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:375: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:376: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  0 INLIERS :  365 SOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 LOF\n",
      "Model: \"model_76\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_51 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_276 (Dense)               (None, 32)           1056        input_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_277 (Dense)               (None, 128)          4224        dense_276[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_151 (Dropout)           (None, 128)          0           dense_277[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_278 (Dense)               (None, 64)           8256        dropout_151[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_152 (Dropout)           (None, 64)           0           dense_278[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_279 (Dense)               (None, 32)           2080        dropout_152[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_153 (Dropout)           (None, 32)           0           dense_279[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_280 (Dense)               (None, 2)            66          dropout_153[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_281 (Dense)               (None, 2)            66          dropout_153[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 2)            0           dense_280[0][0]                  \n",
      "                                                                 dense_281[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,748\n",
      "Trainable params: 15,748\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_52 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_282 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_283 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_154 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_284 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_155 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_285 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_156 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_286 (Dense)            (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 14,662\n",
      "Trainable params: 14,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_77 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_77.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "model_76 (Model)             [(None, 2), (None, 2), (N 15748     \n",
      "_________________________________________________________________\n",
      "model_77 (Model)             (None, 32)                14662     \n",
      "=================================================================\n",
      "Total params: 30,410\n",
      "Trainable params: 30,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 80.5292 - val_loss: 70.5384\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 53.3590 - val_loss: 57.3040\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 43.3803 - val_loss: 49.6236\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 38.0853 - val_loss: 45.2051\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 132us/step - loss: 34.7746 - val_loss: 40.6094\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 33.5126 - val_loss: 40.4706\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 132us/step - loss: 32.6365 - val_loss: 39.5755\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 32.2329 - val_loss: 39.1358\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 32.0244 - val_loss: 38.9061\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 117us/step - loss: 31.9496 - val_loss: 38.8987\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.7860 - val_loss: 38.5652\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 31.7244 - val_loss: 38.3104\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 31.5998 - val_loss: 38.4756\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 31.6047 - val_loss: 38.2399\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.4695 - val_loss: 38.3103\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.4509 - val_loss: 38.3872\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.4226 - val_loss: 38.3501\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.4475 - val_loss: 38.2253\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.4252 - val_loss: 38.2781\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.3883 - val_loss: 38.2333\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.3948 - val_loss: 38.2508\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3263 - val_loss: 38.2101\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.4138 - val_loss: 38.3008\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3129 - val_loss: 38.2611\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.3938 - val_loss: 38.3069\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.3320 - val_loss: 38.2567\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 31.3340 - val_loss: 38.3069\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.3201 - val_loss: 38.1808\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3569 - val_loss: 38.1318\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.2838 - val_loss: 38.2887\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3633 - val_loss: 38.3661\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.2834 - val_loss: 38.2886\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.2593 - val_loss: 38.1916\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.4096 - val_loss: 38.2669\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3601 - val_loss: 38.4167\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.2736 - val_loss: 38.2892\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.3090 - val_loss: 38.1691\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3311 - val_loss: 38.2976\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.3456 - val_loss: 38.4145\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.2902 - val_loss: 38.2172\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3463 - val_loss: 38.3401\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.3564 - val_loss: 38.1074\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.2990 - val_loss: 38.0972\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3883 - val_loss: 38.3441\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.3728 - val_loss: 38.2065\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.2855 - val_loss: 38.1733\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3027 - val_loss: 38.1901\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.3584 - val_loss: 38.1113\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3303 - val_loss: 38.1739\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.3443 - val_loss: 38.1661\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.3206 - val_loss: 38.2322\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3215 - val_loss: 38.3421\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3268 - val_loss: 38.1126\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.3322 - val_loss: 38.1832\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3277 - val_loss: 38.1085\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3244 - val_loss: 38.2533\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3553 - val_loss: 38.2265\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3251 - val_loss: 38.2892\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3166 - val_loss: 38.1717\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.2794 - val_loss: 38.2078\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.3002 - val_loss: 38.1804\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3260 - val_loss: 38.1410\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.3207 - val_loss: 38.2718\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3508 - val_loss: 38.1149\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3037 - val_loss: 38.2277\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3714 - val_loss: 38.4708\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3400 - val_loss: 38.2113\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3072 - val_loss: 38.0709\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3362 - val_loss: 38.0790\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3451 - val_loss: 38.3843\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.2749 - val_loss: 38.1236\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.2924 - val_loss: 38.4165\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3594 - val_loss: 38.5499\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3712 - val_loss: 38.2538\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.2696 - val_loss: 38.3116\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.3171 - val_loss: 38.1787\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3388 - val_loss: 38.2515\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3631 - val_loss: 38.1305\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.3300 - val_loss: 38.1737\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.3252 - val_loss: 38.0940\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3543 - val_loss: 38.1181\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.3515 - val_loss: 38.1502\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.2897 - val_loss: 38.1540\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3350 - val_loss: 38.1606\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3509 - val_loss: 38.1901\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3297 - val_loss: 38.1723\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.3505 - val_loss: 38.1555\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.3895 - val_loss: 38.0939\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3361 - val_loss: 38.0498\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3438 - val_loss: 38.1292\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3404 - val_loss: 38.0908\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3460 - val_loss: 38.1796\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.3516 - val_loss: 38.1029\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3491 - val_loss: 38.0804\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3244 - val_loss: 38.1072\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3419 - val_loss: 38.1108\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3400 - val_loss: 38.0724\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3329 - val_loss: 38.1079\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.3227 - val_loss: 38.1010\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.3256 - val_loss: 38.1156\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 VAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  0 INLIERS :  365 ABOD\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 FB\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 HBOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 IF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  5 INLIERS :  360 KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  3 INLIERS :  362 Average KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 OCSVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:375: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:376: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  0 INLIERS :  365 SOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 LOF\n",
      "Model: \"model_79\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_53 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_287 (Dense)               (None, 32)           1056        input_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_288 (Dense)               (None, 128)          4224        dense_287[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_157 (Dropout)           (None, 128)          0           dense_288[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_289 (Dense)               (None, 64)           8256        dropout_157[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_158 (Dropout)           (None, 64)           0           dense_289[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_290 (Dense)               (None, 32)           2080        dropout_158[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_159 (Dropout)           (None, 32)           0           dense_290[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_291 (Dense)               (None, 2)            66          dropout_159[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_292 (Dense)               (None, 2)            66          dropout_159[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 2)            0           dense_291[0][0]                  \n",
      "                                                                 dense_292[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,748\n",
      "Trainable params: 15,748\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_54 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_293 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_294 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_160 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_295 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_161 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_296 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_162 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_297 (Dense)            (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 14,662\n",
      "Trainable params: 14,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_80 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_80.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_53 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "model_79 (Model)             [(None, 2), (None, 2), (N 15748     \n",
      "_________________________________________________________________\n",
      "model_80 (Model)             (None, 32)                14662     \n",
      "=================================================================\n",
      "Total params: 30,410\n",
      "Trainable params: 30,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 75.4510 - val_loss: 50.2201\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 53.4945 - val_loss: 42.0707\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 45.2474 - val_loss: 36.9050\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 100us/step - loss: 40.3039 - val_loss: 35.0114\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 36.8930 - val_loss: 32.5158\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 34.5801 - val_loss: 31.1972\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 33.8926 - val_loss: 31.0127\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 100us/step - loss: 33.2802 - val_loss: 30.7166\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 114us/step - loss: 32.9686 - val_loss: 30.6082\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 32.7827 - val_loss: 30.5912\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.6411 - val_loss: 30.6949\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 32.6229 - val_loss: 30.5137\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.4724 - val_loss: 30.5811\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.4712 - val_loss: 30.4195\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.4074 - val_loss: 30.4971\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.3660 - val_loss: 30.4420\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.3387 - val_loss: 30.3963\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 32.2819 - val_loss: 30.4316\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.3155 - val_loss: 30.5120\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.2609 - val_loss: 30.3733\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.2044 - val_loss: 30.3428\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.2159 - val_loss: 30.3310\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.2534 - val_loss: 30.4940\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.2641 - val_loss: 30.4277\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.2307 - val_loss: 30.3602\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.2252 - val_loss: 30.3409\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1712 - val_loss: 30.3932\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.1873 - val_loss: 30.4602\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.2204 - val_loss: 30.4277\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 32.2051 - val_loss: 30.3579\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.1910 - val_loss: 30.3840\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.2035 - val_loss: 30.4241\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.2115 - val_loss: 30.3720\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1974 - val_loss: 30.4765\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.2163 - val_loss: 30.3575\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.1929 - val_loss: 30.3387\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.1910 - val_loss: 30.3868\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.2139 - val_loss: 30.4365\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1937 - val_loss: 30.3776\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1973 - val_loss: 30.4048\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1922 - val_loss: 30.3898\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1882 - val_loss: 30.4244\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1670 - val_loss: 30.3779\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1790 - val_loss: 30.3389\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1846 - val_loss: 30.3644\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1848 - val_loss: 30.3710\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.2000 - val_loss: 30.3883\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1777 - val_loss: 30.3886\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1966 - val_loss: 30.3255\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.2037 - val_loss: 30.3477\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 32.1931 - val_loss: 30.3990\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1819 - val_loss: 30.3997\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1651 - val_loss: 30.3711\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1940 - val_loss: 30.3433\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1747 - val_loss: 30.3444\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.2028 - val_loss: 30.3614\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.1912 - val_loss: 30.3860\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.1941 - val_loss: 30.3827\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1823 - val_loss: 30.3460\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1899 - val_loss: 30.3614\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.1846 - val_loss: 30.3899\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1998 - val_loss: 30.3814\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1801 - val_loss: 30.3839\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.1969 - val_loss: 30.3682\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1916 - val_loss: 30.3775\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1771 - val_loss: 30.3812\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1934 - val_loss: 30.3673\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1851 - val_loss: 30.3936\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1796 - val_loss: 30.3611\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.1970 - val_loss: 30.3686\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.1705 - val_loss: 30.3473\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.1864 - val_loss: 30.3690\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.1891 - val_loss: 30.3693\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1934 - val_loss: 30.3520\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.1846 - val_loss: 30.3596\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1949 - val_loss: 30.3870\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1860 - val_loss: 30.3819\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.1882 - val_loss: 30.3740\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.1931 - val_loss: 30.3717\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1834 - val_loss: 30.3777\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1843 - val_loss: 30.3593\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1812 - val_loss: 30.3822\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1883 - val_loss: 30.3761\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1773 - val_loss: 30.3846\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1933 - val_loss: 30.3689\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1840 - val_loss: 30.3734\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1913 - val_loss: 30.3663\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1800 - val_loss: 30.3710\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 32.1794 - val_loss: 30.3798\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.1877 - val_loss: 30.3812\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1846 - val_loss: 30.3720\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.1817 - val_loss: 30.3762\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1815 - val_loss: 30.3580\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1848 - val_loss: 30.3616\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.1787 - val_loss: 30.3717\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.1785 - val_loss: 30.3695\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1858 - val_loss: 30.3667\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 32.1879 - val_loss: 30.3671\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.1863 - val_loss: 30.3824\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.1840 - val_loss: 30.3699\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 VAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  0 INLIERS :  365 ABOD\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  10 INLIERS :  355 FB\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 HBOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 IF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  5 INLIERS :  360 KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  4 INLIERS :  361 Average KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 OCSVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:375: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:376: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  0 INLIERS :  365 SOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  10 INLIERS :  355 LOF\n",
      "Model: \"model_82\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_55 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_298 (Dense)               (None, 32)           1056        input_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_299 (Dense)               (None, 128)          4224        dense_298[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_163 (Dropout)           (None, 128)          0           dense_299[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_300 (Dense)               (None, 64)           8256        dropout_163[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_164 (Dropout)           (None, 64)           0           dense_300[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_301 (Dense)               (None, 32)           2080        dropout_164[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_165 (Dropout)           (None, 32)           0           dense_301[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_302 (Dense)               (None, 2)            66          dropout_165[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_303 (Dense)               (None, 2)            66          dropout_165[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 2)            0           dense_302[0][0]                  \n",
      "                                                                 dense_303[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,748\n",
      "Trainable params: 15,748\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_56 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_304 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_305 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_166 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_306 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_167 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_307 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_168 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_308 (Dense)            (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 14,662\n",
      "Trainable params: 14,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_83 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_83.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_84\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_55 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "model_82 (Model)             [(None, 2), (None, 2), (N 15748     \n",
      "_________________________________________________________________\n",
      "model_83 (Model)             (None, 32)                14662     \n",
      "=================================================================\n",
      "Total params: 30,410\n",
      "Trainable params: 30,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 77.6068 - val_loss: 42.9736\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 54.9476 - val_loss: 34.6979\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 46.4514 - val_loss: 30.6325\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 132us/step - loss: 41.2065 - val_loss: 27.4179\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 37.1916 - val_loss: 26.8984\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 34.7538 - val_loss: 25.8671\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 126us/step - loss: 34.0883 - val_loss: 25.4189\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 33.5851 - val_loss: 25.3939\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 101us/step - loss: 33.3244 - val_loss: 25.4544\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 112us/step - loss: 33.2569 - val_loss: 25.5535\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 33.1517 - val_loss: 25.4159\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 33.1135 - val_loss: 25.2999\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.9914 - val_loss: 25.3321\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.9538 - val_loss: 25.1517\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.9524 - val_loss: 25.1579\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.8931 - val_loss: 25.1959\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.8779 - val_loss: 25.1869\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.8352 - val_loss: 25.0717\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.9234 - val_loss: 25.1490\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.9298 - val_loss: 25.1939\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.8418 - val_loss: 25.1719\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.8551 - val_loss: 25.0871\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.8560 - val_loss: 25.1092\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.8398 - val_loss: 25.1738\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.8019 - val_loss: 25.0891\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.8565 - val_loss: 25.1583\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.8271 - val_loss: 25.0822\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.8168 - val_loss: 25.1221\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.8327 - val_loss: 25.1105\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7978 - val_loss: 25.1230\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.8202 - val_loss: 25.1656\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.8050 - val_loss: 25.1175\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.8175 - val_loss: 25.1072\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.8044 - val_loss: 25.1324\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.8107 - val_loss: 25.1212\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.8075 - val_loss: 25.1230\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7848 - val_loss: 25.1261\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7993 - val_loss: 25.1136\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7890 - val_loss: 25.1169\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7967 - val_loss: 25.0995\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7890 - val_loss: 25.0992\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.8124 - val_loss: 25.1015\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7889 - val_loss: 25.1161\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7918 - val_loss: 25.0923\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7909 - val_loss: 25.0963\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7932 - val_loss: 25.1065\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.7889 - val_loss: 25.0904\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7957 - val_loss: 25.0954\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7789 - val_loss: 25.0876\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7901 - val_loss: 25.0976\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7914 - val_loss: 25.0984\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7927 - val_loss: 25.0868\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7854 - val_loss: 25.0843\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7838 - val_loss: 25.0868\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.7922 - val_loss: 25.0956\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7913 - val_loss: 25.0904\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7862 - val_loss: 25.0876\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7865 - val_loss: 25.0876\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7849 - val_loss: 25.0878\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7853 - val_loss: 25.0726\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7831 - val_loss: 25.0835\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7896 - val_loss: 25.0829\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7863 - val_loss: 25.0860\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7889 - val_loss: 25.0832\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7881 - val_loss: 25.0894\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7899 - val_loss: 25.0861\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7852 - val_loss: 25.0919\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7874 - val_loss: 25.0933\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7837 - val_loss: 25.0796\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7829 - val_loss: 25.0886\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7887 - val_loss: 25.0857\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7859 - val_loss: 25.0819\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7879 - val_loss: 25.0834\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7840 - val_loss: 25.0933\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7874 - val_loss: 25.0778\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7855 - val_loss: 25.0828\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7839 - val_loss: 25.0770\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.7843 - val_loss: 25.0798\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7851 - val_loss: 25.0810\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7808 - val_loss: 25.0807\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7875 - val_loss: 25.0828\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7836 - val_loss: 25.0839\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7847 - val_loss: 25.0824\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7831 - val_loss: 25.0827\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.7841 - val_loss: 25.0775\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7852 - val_loss: 25.0828\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7835 - val_loss: 25.0796\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7858 - val_loss: 25.0820\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7875 - val_loss: 25.0801\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7804 - val_loss: 25.0801\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7810 - val_loss: 25.0795\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7821 - val_loss: 25.0788\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7853 - val_loss: 25.0831\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7838 - val_loss: 25.0829\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.7836 - val_loss: 25.0810\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7806 - val_loss: 25.0780\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7823 - val_loss: 25.0791\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.7841 - val_loss: 25.0799\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7850 - val_loss: 25.0799\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7846 - val_loss: 25.0797\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 VAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  0 INLIERS :  365 ABOD\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  14 INLIERS :  351 FB\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 HBOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 IF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  10 INLIERS :  355 KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  7 INLIERS :  358 Average KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 OCSVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:375: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:376: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  0 INLIERS :  365 SOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  14 INLIERS :  351 LOF\n",
      "Model: \"model_85\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_57 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_309 (Dense)               (None, 32)           1056        input_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_310 (Dense)               (None, 128)          4224        dense_309[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_169 (Dropout)           (None, 128)          0           dense_310[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_311 (Dense)               (None, 64)           8256        dropout_169[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_170 (Dropout)           (None, 64)           0           dense_311[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_312 (Dense)               (None, 32)           2080        dropout_170[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_171 (Dropout)           (None, 32)           0           dense_312[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_313 (Dense)               (None, 2)            66          dropout_171[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_314 (Dense)               (None, 2)            66          dropout_171[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 2)            0           dense_313[0][0]                  \n",
      "                                                                 dense_314[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,748\n",
      "Trainable params: 15,748\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_86\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_58 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_315 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_316 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_172 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_317 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_173 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_318 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_174 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_319 (Dense)            (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 14,662\n",
      "Trainable params: 14,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_86 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_86.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_57 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "model_85 (Model)             [(None, 2), (None, 2), (N 15748     \n",
      "_________________________________________________________________\n",
      "model_86 (Model)             (None, 32)                14662     \n",
      "=================================================================\n",
      "Total params: 30,410\n",
      "Trainable params: 30,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 98.7128 - val_loss: 72.8123\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 60.1746 - val_loss: 54.5130\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 47.1610 - val_loss: 45.3954\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 40.5916 - val_loss: 39.9682\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 98us/step - loss: 36.1787 - val_loss: 35.8063\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 33.7418 - val_loss: 34.6779\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 98us/step - loss: 33.2478 - val_loss: 34.0440\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 32.7667 - val_loss: 34.0225\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 32.6124 - val_loss: 33.7624\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 91us/step - loss: 32.4734 - val_loss: 33.7065\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.3589 - val_loss: 33.6028\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.2599 - val_loss: 33.3618\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.2148 - val_loss: 33.5563\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 32.1608 - val_loss: 33.3329\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.0634 - val_loss: 33.5057\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.0687 - val_loss: 33.3069\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.0475 - val_loss: 33.4653\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0092 - val_loss: 33.2404\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 32.0123 - val_loss: 33.3080\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9961 - val_loss: 33.2318\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9540 - val_loss: 33.2956\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.0021 - val_loss: 33.2043\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.9713 - val_loss: 33.2225\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9780 - val_loss: 33.2455\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 31.9467 - val_loss: 33.2516\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9303 - val_loss: 33.2303\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9410 - val_loss: 33.1856\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9227 - val_loss: 33.1825\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.9397 - val_loss: 33.2164\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.9299 - val_loss: 33.1505\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.9215 - val_loss: 33.1445\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.9073 - val_loss: 33.1600\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8972 - val_loss: 33.1228\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8908 - val_loss: 33.1568\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8918 - val_loss: 33.1549\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.9099 - val_loss: 33.1711\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8948 - val_loss: 33.1448\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8919 - val_loss: 33.1194\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.9072 - val_loss: 33.1266\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.9026 - val_loss: 33.1316\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8832 - val_loss: 33.1530\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8983 - val_loss: 33.1323\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8960 - val_loss: 33.1016\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.9033 - val_loss: 33.1512\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8644 - val_loss: 33.1027\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8939 - val_loss: 33.0897\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8974 - val_loss: 33.1542\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 31.8966 - val_loss: 33.0970\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 31.8935 - val_loss: 33.1143\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8997 - val_loss: 33.1255\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8720 - val_loss: 33.1179\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8766 - val_loss: 33.1442\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8840 - val_loss: 33.1069\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8899 - val_loss: 33.1204\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8911 - val_loss: 33.0901\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8914 - val_loss: 33.0746\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8767 - val_loss: 33.1246\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8689 - val_loss: 33.1014\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8918 - val_loss: 33.0793\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8857 - val_loss: 33.1097\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8871 - val_loss: 33.1292\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8766 - val_loss: 33.1065\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8901 - val_loss: 33.0973\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8893 - val_loss: 33.1011\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8925 - val_loss: 33.1126\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8731 - val_loss: 33.0993\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8746 - val_loss: 33.1182\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8798 - val_loss: 33.0972\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8780 - val_loss: 33.1114\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8866 - val_loss: 33.1161\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8761 - val_loss: 33.1331\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8693 - val_loss: 33.1081\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8808 - val_loss: 33.1095\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8840 - val_loss: 33.1176\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8824 - val_loss: 33.1293\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8700 - val_loss: 33.1066\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8897 - val_loss: 33.1157\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8913 - val_loss: 33.1032\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8715 - val_loss: 33.1173\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8807 - val_loss: 33.1063\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8707 - val_loss: 33.1137\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8781 - val_loss: 33.1265\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8750 - val_loss: 33.1318\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8727 - val_loss: 33.1138\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8782 - val_loss: 33.1130\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8865 - val_loss: 33.0934\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8786 - val_loss: 33.1253\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8744 - val_loss: 33.1167\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 31.8762 - val_loss: 33.1017\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8630 - val_loss: 33.1361\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 31.8841 - val_loss: 33.1177\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8865 - val_loss: 33.0967\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8826 - val_loss: 33.1142\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 31.8743 - val_loss: 33.0982\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8855 - val_loss: 33.0958\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8817 - val_loss: 33.1138\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8844 - val_loss: 33.1192\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8832 - val_loss: 33.1089\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 31.8793 - val_loss: 33.1002\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 31.8729 - val_loss: 33.0961\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 VAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3584: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  0 INLIERS :  365 ABOD\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  18 INLIERS :  347 FB\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 HBOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 IF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  12 INLIERS :  353 KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  7 INLIERS :  358 Average KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 OCSVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:365: RuntimeWarning: invalid value encountered in greater\n",
      "  self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/base.py:168: RuntimeWarning: invalid value encountered in greater\n",
      "  return (pred_score > self.threshold_).astype('int').ravel()\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:375: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:376: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/pyod/models/sos.py:202: RuntimeWarning: overflow encountered in multiply\n",
      "  beta[i] = beta[i] * 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  0 INLIERS :  365 SOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  18 INLIERS :  347 LOF\n",
      "Model: \"model_88\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_59 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_320 (Dense)               (None, 32)           1056        input_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_321 (Dense)               (None, 128)          4224        dense_320[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_175 (Dropout)           (None, 128)          0           dense_321[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_322 (Dense)               (None, 64)           8256        dropout_175[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_176 (Dropout)           (None, 64)           0           dense_322[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_323 (Dense)               (None, 32)           2080        dropout_176[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_177 (Dropout)           (None, 32)           0           dense_323[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_324 (Dense)               (None, 2)            66          dropout_177[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_325 (Dense)               (None, 2)            66          dropout_177[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 2)            0           dense_324[0][0]                  \n",
      "                                                                 dense_325[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,748\n",
      "Trainable params: 15,748\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_60 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_326 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_327 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_178 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_328 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_179 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_329 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_180 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_330 (Dense)            (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 14,662\n",
      "Trainable params: 14,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_89 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_89.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_90\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_59 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "model_88 (Model)             [(None, 2), (None, 2), (N 15748     \n",
      "_________________________________________________________________\n",
      "model_89 (Model)             (None, 32)                14662     \n",
      "=================================================================\n",
      "Total params: 30,410\n",
      "Trainable params: 30,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 110.0456 - val_loss: 53.2889\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 100us/step - loss: 67.6898 - val_loss: 39.9948\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 51.8959 - val_loss: 34.0838\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 43.9935 - val_loss: 30.3657\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 117us/step - loss: 39.2784 - val_loss: 28.0408\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 98us/step - loss: 36.9712 - val_loss: 26.2360\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 35.3985 - val_loss: 25.9316\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 34.3970 - val_loss: 25.3636\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 33.9482 - val_loss: 25.4411\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 33.7174 - val_loss: 25.4405\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 99us/step - loss: 33.4336 - val_loss: 25.2151\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 33.2906 - val_loss: 25.2450\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 33.1719 - val_loss: 25.2491\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 33.1176 - val_loss: 25.2682\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.9049 - val_loss: 25.2974\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 33.0364 - val_loss: 25.0955\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.9616 - val_loss: 25.0901\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.9271 - val_loss: 25.0790\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.9162 - val_loss: 25.0317\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 32.8977 - val_loss: 25.0861\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.8863 - val_loss: 25.1046\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.8689 - val_loss: 25.0599\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.8861 - val_loss: 25.1170\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.8317 - val_loss: 25.0616\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.8321 - val_loss: 25.0869\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.8188 - val_loss: 25.0999\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7948 - val_loss: 25.1030\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7936 - val_loss: 25.0881\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.8242 - val_loss: 25.1224\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.8097 - val_loss: 25.0678\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.8062 - val_loss: 25.0787\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7983 - val_loss: 25.0735\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.8066 - val_loss: 25.1199\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.8291 - val_loss: 25.0925\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7855 - val_loss: 25.0832\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.8116 - val_loss: 25.0664\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.8083 - val_loss: 25.0614\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.8059 - val_loss: 25.0800\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7951 - val_loss: 25.0824\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7963 - val_loss: 25.0913\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7819 - val_loss: 25.0820\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7998 - val_loss: 25.0563\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7908 - val_loss: 25.0588\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7929 - val_loss: 25.0580\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7912 - val_loss: 25.0776\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7900 - val_loss: 25.0742\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.7920 - val_loss: 25.0859\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7997 - val_loss: 25.0845\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7915 - val_loss: 25.0769\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7780 - val_loss: 25.0536\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7896 - val_loss: 25.0716\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7863 - val_loss: 25.0702\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7955 - val_loss: 25.0704\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7931 - val_loss: 25.0623\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7955 - val_loss: 25.0471\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7745 - val_loss: 25.0762\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7899 - val_loss: 25.0532\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7953 - val_loss: 25.0601\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.7826 - val_loss: 25.0590\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7900 - val_loss: 25.0641\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.7913 - val_loss: 25.0639\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7793 - val_loss: 25.0609\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7907 - val_loss: 25.0650\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7866 - val_loss: 25.0684\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7816 - val_loss: 25.0785\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7811 - val_loss: 25.0523\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7774 - val_loss: 25.0658\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7900 - val_loss: 25.0633\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7793 - val_loss: 25.0633\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7851 - val_loss: 25.0657\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7818 - val_loss: 25.0716\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7805 - val_loss: 25.0540\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.7920 - val_loss: 25.0824\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.7770 - val_loss: 25.0578\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7807 - val_loss: 25.0519\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7949 - val_loss: 25.0692\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.7803 - val_loss: 25.0703\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7811 - val_loss: 25.0705\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7917 - val_loss: 25.0700\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7829 - val_loss: 25.0691\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7777 - val_loss: 25.0677\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.7906 - val_loss: 25.0652\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.7812 - val_loss: 25.0696\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7816 - val_loss: 25.0490\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7873 - val_loss: 25.0578\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7937 - val_loss: 25.0688\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7874 - val_loss: 25.0645\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7839 - val_loss: 25.0673\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7819 - val_loss: 25.0577\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.7914 - val_loss: 25.0612\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7848 - val_loss: 25.0669\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7780 - val_loss: 25.0619\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 32.7859 - val_loss: 25.0597\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 32.7808 - val_loss: 25.0562\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 80us/step - loss: 32.7825 - val_loss: 25.0621\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 32.7784 - val_loss: 25.0729\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7793 - val_loss: 25.0498\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7863 - val_loss: 25.0592\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 79us/step - loss: 32.7842 - val_loss: 25.0661\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 81us/step - loss: 32.7825 - val_loss: 25.0583\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 VAE\n"
     ]
    }
   ],
   "source": [
    "# time the code\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "# empty list to store outliers and inliners\n",
    "df_outliers = []\n",
    "scores_pred_lst = []\n",
    "proba_lst = []\n",
    "predict_rank_lst = []\n",
    "\n",
    "# iterate over different outlier fractions \n",
    "outliers_fraction = [0.01,0.02,0.03,0.04,0.05]\n",
    "for fraction in outliers_fraction:\n",
    "    # Define 13 outlier detection tools to be compared\n",
    "    classifiers = {#'AE' : AutoEncoder(contamination=outliers_fraction, random_state=random_state), # check why error\n",
    "               'ABOD': ABOD(contamination = fraction),\n",
    "              'CBLOF':CBLOF(check_estimator=False, random_state=random_state), # getting 0 \n",
    "              'FB':FeatureBagging(LOF(n_neighbors=35),contamination=fraction,check_estimator=False,random_state=random_state),\n",
    "              'HBOS': HBOS(contamination=fraction),\n",
    "              'IF': IForest(contamination=fraction,random_state=random_state),\n",
    "              'KNN': KNN(contamination=fraction),\n",
    "              'Average KNN': KNN(method='mean',contamination=fraction),\n",
    "              'OCSVM' : OCSVM(contamination=fraction),\n",
    "              'SOS' : SOS(contamination=fraction),\n",
    "              'LOF': LOF(n_neighbors=35, contamination = fraction),\n",
    "              #'LOCI': LOCI(contamination = fraction), # takes a super long time + don't understand\n",
    "              'VAE' : VAE(contamination=fraction, random_state=random_state) # default nbr of neurons too much for blockchain dataset\n",
    "              }\n",
    "\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        clf.fit(X)\n",
    "        # predict raw anomaly score\n",
    "        scores_pred = clf.decision_function(X) * -1 # WHY? * -1\n",
    "\n",
    "        # prediction of a datapoint category outlier or inlier\n",
    "        y_pred = clf.predict(X)\n",
    "\n",
    "        # probability of outlier \n",
    "        probability = clf.predict_proba(X, method = 'linear')\n",
    "\n",
    "        # predict rank\n",
    "        predict_rank = clf._predict_rank(X)\n",
    "\n",
    "        n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "        n_outliers = np.count_nonzero(y_pred == 1)\n",
    "\n",
    "        # copy of dataframe\n",
    "        dfx = X.copy()\n",
    "\n",
    "        dfx[clf_name] = y_pred.tolist()\n",
    "        #print(clf_name,'\\n',i,clf,'\\n',classifiers,'\\n',classifiers.items())\n",
    "        print('Outlier fraction: ', fraction)\n",
    "        print('OUTLIERS : ',n_outliers,'INLIERS : ',n_inliers, clf_name)\n",
    "        #print('scores_pred: ', scores_pred)\n",
    "        scores_pred_lst.append(scores_pred)\n",
    "        df_outliers.append(dfx[clf_name]) \n",
    "        proba_lst.append(probability)\n",
    "        predict_rank_lst.append(predict_rank)\n",
    "t1 = time.time()\n",
    "total = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run:  53.24563932418823 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Time to run: ',total,'s')\n",
    "# we have 50 lists (or essentially 5 with 10 models)\n",
    "len(df_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack lists based on outlier fraction e.g. first 10 = 0.01 outlier fraction\n",
    "frac_oo1 = df_outliers[0:11]\n",
    "frac_oo2 = df_outliers[11:22]\n",
    "frac_oo3 = df_outliers[22:33]\n",
    "frac_oo4 = df_outliers[33:44]\n",
    "frac_oo5 = df_outliers[44:]\n",
    "\n",
    "# use the function to extract the dates\n",
    "frac_oo1_outliers = outlier_subset_function(frac_oo1)\n",
    "frac_oo2_outliers = outlier_subset_function(frac_oo2)\n",
    "frac_oo3_outliers = outlier_subset_function(frac_oo3)\n",
    "frac_oo4_outliers = outlier_subset_function(frac_oo4)\n",
    "frac_oo5_outliers = outlier_subset_function(frac_oo5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Anomaly Dates:  46\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AnomalyCount_Fractions</th>\n",
       "      <th>AnomalyCount_Models</th>\n",
       "      <th>weight_models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.019943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.022792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.031339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.017094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.017094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.042735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.022792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.031339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.022792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.042735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.031339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.031339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.019943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.039886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-09-10</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.017094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.019943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2018-11-24</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.017094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2018-10-25</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.008547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.008547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2018-10-28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2018-10-24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2018-10-29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2018-10-27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2018-08-20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2018-10-23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  AnomalyCount_Fractions  AnomalyCount_Models  weight_models\n",
       "0   2018-04-09                       5                    7       0.019943\n",
       "1   2018-01-12                       5                    8       0.022792\n",
       "2   2018-01-31                       5                    5       0.014245\n",
       "3   2018-02-01                       5                    9       0.025641\n",
       "4   2018-12-31                       5                   11       0.031339\n",
       "5   2018-01-23                       5                    5       0.014245\n",
       "6   2018-01-15                       5                    6       0.017094\n",
       "7   2018-12-30                       5                   10       0.028490\n",
       "8   2018-01-21                       5                    5       0.014245\n",
       "9   2018-01-09                       5                   13       0.037037\n",
       "10  2018-01-04                       5                    9       0.025641\n",
       "11  2018-01-14                       5                    6       0.017094\n",
       "12  2018-01-05                       5                   13       0.037037\n",
       "13  2018-01-02                       5                   15       0.042735\n",
       "14  2018-01-19                       5                    8       0.022792\n",
       "15  2018-12-28                       5                    9       0.025641\n",
       "16  2018-01-06                       5                   11       0.031339\n",
       "17  2018-01-17                       5                   13       0.037037\n",
       "18  2018-01-11                       5                    8       0.022792\n",
       "19  2018-04-08                       5                   15       0.042735\n",
       "20  2018-01-08                       5                   11       0.031339\n",
       "21  2018-01-30                       5                    5       0.014245\n",
       "22  2018-01-03                       5                   11       0.031339\n",
       "23  2018-01-13                       5                    5       0.014245\n",
       "24  2018-12-29                       5                    9       0.025641\n",
       "25  2018-01-10                       5                    9       0.025641\n",
       "26  2018-11-26                       5                    7       0.019943\n",
       "27  2018-01-16                       5                   14       0.039886\n",
       "28  2018-01-07                       5                   13       0.037037\n",
       "29  2018-09-10                       5                   10       0.028490\n",
       "30  2018-01-18                       5                    6       0.017094\n",
       "31  2018-01-01                       5                   13       0.037037\n",
       "32  2018-01-20                       5                    7       0.019943\n",
       "33  2018-01-24                       5                    5       0.014245\n",
       "34  2018-11-24                       5                   10       0.028490\n",
       "35  2018-01-25                       5                    5       0.014245\n",
       "36  2018-01-22                       5                    6       0.017094\n",
       "37  2018-12-27                       4                    5       0.014245\n",
       "38  2018-10-25                       3                    3       0.008547\n",
       "39  2018-10-26                       3                    3       0.008547\n",
       "40  2018-10-28                       2                    2       0.005698\n",
       "41  2018-10-24                       2                    2       0.005698\n",
       "42  2018-10-29                       1                    1       0.002849\n",
       "43  2018-10-27                       1                    1       0.002849\n",
       "44  2018-08-20                       1                    1       0.002849\n",
       "45  2018-10-23                       1                    1       0.002849"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_anomalies = date_ranking(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)\n",
    "print('Unique Anomaly Dates: ',len(social_anomalies.Date.unique()))\n",
    "social_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction 0.01 unique dates:  37\n",
      "Outlier fraction 0.02 unique dates:  38\n",
      "Outlier fraction 0.03 unique dates:  40\n",
      "Outlier fraction 0.04 unique dates:  42\n",
      "Outlier fraction 0.05 unique dates:  46\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>001</th>\n",
       "      <th>002</th>\n",
       "      <th>003</th>\n",
       "      <th>004</th>\n",
       "      <th>005</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-08</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-09</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            001  002  003  004  005\n",
       "2018-01-01    1    1    1    1    1\n",
       "2018-01-02    1    1    1    1    1\n",
       "2018-01-03    1    1    1    1    1\n",
       "2018-01-04    1    1    1    1    1\n",
       "2018-01-05    1    1    1    1    1\n",
       "2018-01-06    1    1    1    1    1\n",
       "2018-01-07    1    1    1    1    1\n",
       "2018-01-08    1    1    1    1    1\n",
       "2018-01-09    1    1    1    1    1\n",
       "2018-01-10    1    1    1    1    1\n",
       "2018-01-11    1    1    1    1    1\n",
       "2018-01-12    1    1    1    1    1\n",
       "2018-01-13    1    1    1    1    1\n",
       "2018-01-14    1    1    1    1    1\n",
       "2018-01-15    1    1    1    1    1\n",
       "2018-01-16    1    1    1    1    1\n",
       "2018-01-17    1    1    1    1    1\n",
       "2018-01-18    1    1    1    1    1\n",
       "2018-01-19    1    1    1    1    1\n",
       "2018-01-20    1    1    1    1    1\n",
       "2018-01-21    1    1    1    1    1\n",
       "2018-01-22    1    1    1    1    1\n",
       "2018-01-23    1    1    1    1    1\n",
       "2018-01-24    1    1    1    1    1\n",
       "2018-01-25    1    1    1    1    1\n",
       "2018-01-30    1    1    1    1    1\n",
       "2018-01-31    1    1    1    1    1\n",
       "2018-02-01    1    1    1    1    1\n",
       "2018-04-08    1    1    1    1    1\n",
       "2018-04-09    1    1    1    1    1\n",
       "2018-08-20    0    0    0    0    1\n",
       "2018-09-10    1    1    1    1    1\n",
       "2018-10-23    0    0    0    0    1\n",
       "2018-10-24    0    0    0    1    1\n",
       "2018-10-25    0    0    1    1    1\n",
       "2018-10-26    0    0    1    1    1\n",
       "2018-10-27    0    0    0    0    1\n",
       "2018-10-28    0    0    0    1    1\n",
       "2018-10-29    0    0    0    0    1\n",
       "2018-11-24    1    1    1    1    1\n",
       "2018-11-26    1    1    1    1    1\n",
       "2018-12-27    0    1    1    1    1\n",
       "2018-12-28    1    1    1    1    1\n",
       "2018-12-29    1    1    1    1    1\n",
       "2018-12-30    1    1    1    1    1\n",
       "2018-12-31    1    1    1    1    1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_per_fraction(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the outliers lists uniquely for later\n",
    "block_df_s = df_outliers\n",
    "# probability lists\n",
    "block_proba_s = proba_lst\n",
    "# rank lists\n",
    "block_rank_s = predict_rank_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Dataset Common anomaly dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr of common date anomalies across three datasets:  20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AnomalyCount_Fractions_x</th>\n",
       "      <th>AnomalyCount_Models_x</th>\n",
       "      <th>weight_models_x</th>\n",
       "      <th>AnomalyCount_Fractions_y</th>\n",
       "      <th>AnomalyCount_Models_y</th>\n",
       "      <th>weight_models_y</th>\n",
       "      <th>AnomalyCount_Fractions</th>\n",
       "      <th>AnomalyCount_Models</th>\n",
       "      <th>weight_models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.050125</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.026128</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.039886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.038005</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.027569</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>0.049881</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.031339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.019002</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.019002</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.017094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>0.059382</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025063</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.035629</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.042735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.045113</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.040380</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.031339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025063</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.028504</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.031339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.032581</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.026128</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.019943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.033254</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.045113</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>0.054632</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.026128</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.022792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.028504</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.040380</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.022792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.028504</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.026128</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.017094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005013</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.014252</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.022792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  AnomalyCount_Fractions_x  AnomalyCount_Models_x  \\\n",
       "0   2018-01-16                         5                     20   \n",
       "1   2018-01-07                         5                     14   \n",
       "2   2018-01-03                         5                     11   \n",
       "3   2018-01-13                         5                      6   \n",
       "4   2018-01-15                         5                      6   \n",
       "5   2018-01-04                         5                     12   \n",
       "6   2018-01-02                         5                     10   \n",
       "7   2018-01-06                         5                     18   \n",
       "8   2018-01-08                         5                     10   \n",
       "9   2018-01-01                         5                     13   \n",
       "10  2018-01-20                         5                      5   \n",
       "11  2018-01-09                         5                      7   \n",
       "12  2018-01-05                         5                     18   \n",
       "13  2018-01-12                         5                      7   \n",
       "14  2018-01-17                         5                     19   \n",
       "15  2018-01-11                         5                      8   \n",
       "16  2018-01-10                         5                      8   \n",
       "17  2018-01-21                         4                      4   \n",
       "18  2018-01-18                         4                      7   \n",
       "19  2018-01-19                         2                      2   \n",
       "\n",
       "    weight_models_x  AnomalyCount_Fractions_y  AnomalyCount_Models_y  \\\n",
       "0          0.050125                         5                     11   \n",
       "1          0.035088                         5                     16   \n",
       "2          0.027569                         5                     21   \n",
       "3          0.015038                         5                      8   \n",
       "4          0.015038                         5                      8   \n",
       "5          0.030075                         5                     25   \n",
       "6          0.025063                         5                     15   \n",
       "7          0.045113                         5                     17   \n",
       "8          0.025063                         5                     12   \n",
       "9          0.032581                         5                     11   \n",
       "10         0.012531                         1                      1   \n",
       "11         0.017544                         5                     14   \n",
       "12         0.045113                         5                     23   \n",
       "13         0.017544                         5                     11   \n",
       "14         0.047619                         5                     12   \n",
       "15         0.020050                         5                     17   \n",
       "16         0.020050                         5                     12   \n",
       "17         0.010025                         1                      1   \n",
       "18         0.017544                         5                     11   \n",
       "19         0.005013                         5                      6   \n",
       "\n",
       "    weight_models_y  AnomalyCount_Fractions  AnomalyCount_Models  \\\n",
       "0          0.026128                       5                   14   \n",
       "1          0.038005                       5                   13   \n",
       "2          0.049881                       5                   11   \n",
       "3          0.019002                       5                    5   \n",
       "4          0.019002                       5                    6   \n",
       "5          0.059382                       5                    9   \n",
       "6          0.035629                       5                   15   \n",
       "7          0.040380                       5                   11   \n",
       "8          0.028504                       5                   11   \n",
       "9          0.026128                       5                   13   \n",
       "10         0.002375                       5                    7   \n",
       "11         0.033254                       5                   13   \n",
       "12         0.054632                       5                   13   \n",
       "13         0.026128                       5                    8   \n",
       "14         0.028504                       5                   13   \n",
       "15         0.040380                       5                    8   \n",
       "16         0.028504                       5                    9   \n",
       "17         0.002375                       5                    5   \n",
       "18         0.026128                       5                    6   \n",
       "19         0.014252                       5                    8   \n",
       "\n",
       "    weight_models  \n",
       "0        0.039886  \n",
       "1        0.037037  \n",
       "2        0.031339  \n",
       "3        0.014245  \n",
       "4        0.017094  \n",
       "5        0.025641  \n",
       "6        0.042735  \n",
       "7        0.031339  \n",
       "8        0.031339  \n",
       "9        0.037037  \n",
       "10       0.019943  \n",
       "11       0.037037  \n",
       "12       0.037037  \n",
       "13       0.022792  \n",
       "14       0.037037  \n",
       "15       0.022792  \n",
       "16       0.025641  \n",
       "17       0.014245  \n",
       "18       0.017094  \n",
       "19       0.022792  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_dataset_anomalies = pd.merge(pd.merge(price_anomalies,block_anomalies,on='Date'),social_anomalies,on='Date')\n",
    "print('Nbr of common date anomalies across three datasets: ',len(three_dataset_anomalies.Date))\n",
    "three_dataset_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection on all 3 datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365, 95)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volumefrom</th>\n",
       "      <th>volumeto</th>\n",
       "      <th>CCI</th>\n",
       "      <th>ichimoku_leadSpanA</th>\n",
       "      <th>ichimoku_leadSpanB</th>\n",
       "      <th>MACD</th>\n",
       "      <th>...</th>\n",
       "      <th>reddit_posts_per_day</th>\n",
       "      <th>reddit_comments_per_hour</th>\n",
       "      <th>reddit_comments_per_day</th>\n",
       "      <th>code_repo_stars</th>\n",
       "      <th>code_repo_forks</th>\n",
       "      <th>code_repo_subscribers</th>\n",
       "      <th>code_repo_open_pull_issues</th>\n",
       "      <th>code_repo_closed_pull_issues</th>\n",
       "      <th>code_repo_open_issues</th>\n",
       "      <th>code_repo_closed_issues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.309506</td>\n",
       "      <td>0.323185</td>\n",
       "      <td>0.314349</td>\n",
       "      <td>0.311447</td>\n",
       "      <td>0.158511</td>\n",
       "      <td>0.165547</td>\n",
       "      <td>0.610031</td>\n",
       "      <td>0.360129</td>\n",
       "      <td>0.336818</td>\n",
       "      <td>0.567335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278695</td>\n",
       "      <td>0.196713</td>\n",
       "      <td>0.196708</td>\n",
       "      <td>0.648723</td>\n",
       "      <td>0.680061</td>\n",
       "      <td>0.737169</td>\n",
       "      <td>0.414537</td>\n",
       "      <td>0.500259</td>\n",
       "      <td>0.401739</td>\n",
       "      <td>0.542089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.173618</td>\n",
       "      <td>0.181712</td>\n",
       "      <td>0.171755</td>\n",
       "      <td>0.174608</td>\n",
       "      <td>0.120752</td>\n",
       "      <td>0.150214</td>\n",
       "      <td>0.147846</td>\n",
       "      <td>0.207782</td>\n",
       "      <td>0.236865</td>\n",
       "      <td>0.167418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169271</td>\n",
       "      <td>0.163744</td>\n",
       "      <td>0.163743</td>\n",
       "      <td>0.265907</td>\n",
       "      <td>0.266401</td>\n",
       "      <td>0.255689</td>\n",
       "      <td>0.215412</td>\n",
       "      <td>0.301799</td>\n",
       "      <td>0.244852</td>\n",
       "      <td>0.291751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.225885</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.236921</td>\n",
       "      <td>0.226146</td>\n",
       "      <td>0.077413</td>\n",
       "      <td>0.061453</td>\n",
       "      <td>0.505965</td>\n",
       "      <td>0.254712</td>\n",
       "      <td>0.192137</td>\n",
       "      <td>0.470921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175227</td>\n",
       "      <td>0.090028</td>\n",
       "      <td>0.090025</td>\n",
       "      <td>0.471224</td>\n",
       "      <td>0.511847</td>\n",
       "      <td>0.601688</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.239695</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.327409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.264223</td>\n",
       "      <td>0.276441</td>\n",
       "      <td>0.267946</td>\n",
       "      <td>0.264287</td>\n",
       "      <td>0.124178</td>\n",
       "      <td>0.117303</td>\n",
       "      <td>0.592183</td>\n",
       "      <td>0.305660</td>\n",
       "      <td>0.234630</td>\n",
       "      <td>0.598206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260633</td>\n",
       "      <td>0.149065</td>\n",
       "      <td>0.149058</td>\n",
       "      <td>0.697656</td>\n",
       "      <td>0.740560</td>\n",
       "      <td>0.814346</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.491935</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>0.547240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.386708</td>\n",
       "      <td>0.406506</td>\n",
       "      <td>0.391708</td>\n",
       "      <td>0.390215</td>\n",
       "      <td>0.211678</td>\n",
       "      <td>0.220504</td>\n",
       "      <td>0.726902</td>\n",
       "      <td>0.458545</td>\n",
       "      <td>0.416687</td>\n",
       "      <td>0.650943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331805</td>\n",
       "      <td>0.240735</td>\n",
       "      <td>0.240726</td>\n",
       "      <td>0.875716</td>\n",
       "      <td>0.915828</td>\n",
       "      <td>0.947679</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.773297</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.781104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            close        high         low        open  volumefrom    volumeto  \\\n",
       "count  365.000000  365.000000  365.000000  365.000000  365.000000  365.000000   \n",
       "mean     0.309506    0.323185    0.314349    0.311447    0.158511    0.165547   \n",
       "std      0.173618    0.181712    0.171755    0.174608    0.120752    0.150214   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.225885    0.230769    0.236921    0.226146    0.077413    0.061453   \n",
       "50%      0.264223    0.276441    0.267946    0.264287    0.124178    0.117303   \n",
       "75%      0.386708    0.406506    0.391708    0.390215    0.211678    0.220504   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "              CCI  ichimoku_leadSpanA  ichimoku_leadSpanB        MACD  ...  \\\n",
       "count  365.000000          365.000000          365.000000  365.000000  ...   \n",
       "mean     0.610031            0.360129            0.336818    0.567335  ...   \n",
       "std      0.147846            0.207782            0.236865    0.167418  ...   \n",
       "min      0.000000            0.000000            0.000000    0.000000  ...   \n",
       "25%      0.505965            0.254712            0.192137    0.470921  ...   \n",
       "50%      0.592183            0.305660            0.234630    0.598206  ...   \n",
       "75%      0.726902            0.458545            0.416687    0.650943  ...   \n",
       "max      1.000000            1.000000            1.000000    1.000000  ...   \n",
       "\n",
       "       reddit_posts_per_day  reddit_comments_per_hour  \\\n",
       "count            365.000000                365.000000   \n",
       "mean               0.278695                  0.196713   \n",
       "std                0.169271                  0.163744   \n",
       "min                0.000000                  0.000000   \n",
       "25%                0.175227                  0.090028   \n",
       "50%                0.260633                  0.149065   \n",
       "75%                0.331805                  0.240735   \n",
       "max                1.000000                  1.000000   \n",
       "\n",
       "       reddit_comments_per_day  code_repo_stars  code_repo_forks  \\\n",
       "count               365.000000       365.000000       365.000000   \n",
       "mean                  0.196708         0.648723         0.680061   \n",
       "std                   0.163743         0.265907         0.266401   \n",
       "min                   0.000000         0.000000         0.000000   \n",
       "25%                   0.090025         0.471224         0.511847   \n",
       "50%                   0.149058         0.697656         0.740560   \n",
       "75%                   0.240726         0.875716         0.915828   \n",
       "max                   1.000000         1.000000         1.000000   \n",
       "\n",
       "       code_repo_subscribers  code_repo_open_pull_issues  \\\n",
       "count             365.000000                  365.000000   \n",
       "mean                0.737169                    0.414537   \n",
       "std                 0.255689                    0.215412   \n",
       "min                 0.000000                    0.000000   \n",
       "25%                 0.601688                    0.258824   \n",
       "50%                 0.814346                    0.400000   \n",
       "75%                 0.947679                    0.541176   \n",
       "max                 1.000000                    1.000000   \n",
       "\n",
       "       code_repo_closed_pull_issues  code_repo_open_issues  \\\n",
       "count                    365.000000             365.000000   \n",
       "mean                       0.500259               0.401739   \n",
       "std                        0.301799               0.244852   \n",
       "min                        0.000000               0.000000   \n",
       "25%                        0.239695               0.211538   \n",
       "50%                        0.491935               0.365385   \n",
       "75%                        0.773297               0.602564   \n",
       "max                        1.000000               1.000000   \n",
       "\n",
       "       code_repo_closed_issues  \n",
       "count               365.000000  \n",
       "mean                  0.542089  \n",
       "std                   0.291751  \n",
       "min                   0.000000  \n",
       "25%                   0.327409  \n",
       "50%                   0.547240  \n",
       "75%                   0.781104  \n",
       "max                   1.000000  \n",
       "\n",
       "[8 rows x 95 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the 3 datasets \n",
    "merged_df = pd.concat([price,block,social], axis = 1)\n",
    "# scale them \n",
    "X = scale(merged_df)\n",
    "print(X.shape)\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  5 INLIERS :  360 ABOD\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 FB\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 HBOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 IF\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  2 INLIERS :  363 Average KNN\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 OCSVM\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 SOS\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  3 INLIERS :  362 LOF\n",
      "Model: \"model_91\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_61 (InputLayer)           (None, 95)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_331 (Dense)               (None, 95)           9120        input_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_332 (Dense)               (None, 128)          12288       dense_331[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_181 (Dropout)           (None, 128)          0           dense_332[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_333 (Dense)               (None, 64)           8256        dropout_181[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_182 (Dropout)           (None, 64)           0           dense_333[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_334 (Dense)               (None, 32)           2080        dropout_182[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_183 (Dropout)           (None, 32)           0           dense_334[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_335 (Dense)               (None, 2)            66          dropout_183[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_336 (Dense)               (None, 2)            66          dropout_183[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 2)            0           dense_335[0][0]                  \n",
      "                                                                 dense_336[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,876\n",
      "Trainable params: 31,876\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_62 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_337 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_338 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_184 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_339 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_185 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_340 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_186 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_341 (Dense)            (None, 95)                12255     \n",
      "=================================================================\n",
      "Total params: 22,789\n",
      "Trainable params: 22,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_61 (InputLayer)        (None, 95)                0         \n",
      "_________________________________________________________________\n",
      "model_91 (Model)             [(None, 2), (None, 2), (N 31876     \n",
      "_________________________________________________________________\n",
      "model_92 (Model)             (None, 95)                22789     \n",
      "=================================================================\n",
      "Total params: 54,665\n",
      "Trainable params: 54,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_92 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_92.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 177.0294 - val_loss: 127.3954\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 108us/step - loss: 125.8545 - val_loss: 115.9374\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 114.9580 - val_loss: 106.0820\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 106.5077 - val_loss: 99.3786\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 115us/step - loss: 99.7615 - val_loss: 93.4210\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 97.0866 - val_loss: 91.8628\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 95.9911 - val_loss: 91.6005\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 113us/step - loss: 95.7707 - val_loss: 91.3361\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 95.6712 - val_loss: 91.1726\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 94us/step - loss: 95.6367 - val_loss: 91.1579\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 91us/step - loss: 95.5608 - val_loss: 91.1268\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.5700 - val_loss: 91.2060\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.5364 - val_loss: 91.1556\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.5254 - val_loss: 91.1082\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.4897 - val_loss: 91.0876\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.4806 - val_loss: 91.0875\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.4833 - val_loss: 91.1451\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.4784 - val_loss: 91.1369\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4647 - val_loss: 91.1063\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.4659 - val_loss: 91.1028\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.4530 - val_loss: 91.0978\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.4681 - val_loss: 91.0684\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4602 - val_loss: 91.0687\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4431 - val_loss: 91.0874\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4519 - val_loss: 91.0807\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4497 - val_loss: 91.0559\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.4580 - val_loss: 91.0872\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4655 - val_loss: 91.0789\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4573 - val_loss: 91.0451\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.4657 - val_loss: 91.0472\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4472 - val_loss: 91.0741\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.4597 - val_loss: 91.0756\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4483 - val_loss: 91.0823\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4415 - val_loss: 91.0833\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4519 - val_loss: 91.0768\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4465 - val_loss: 91.0775\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4429 - val_loss: 91.0810\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.4570 - val_loss: 91.0573\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4443 - val_loss: 91.0905\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4379 - val_loss: 91.0754\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4690 - val_loss: 91.0823\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4439 - val_loss: 91.0568\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4512 - val_loss: 91.0749\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4396 - val_loss: 91.0772\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4386 - val_loss: 91.0779\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4488 - val_loss: 91.0651\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4717 - val_loss: 91.0628\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4441 - val_loss: 91.0869\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4418 - val_loss: 91.0795\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4627 - val_loss: 91.0405\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4708 - val_loss: 91.0903\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4556 - val_loss: 91.0546\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 95.4582 - val_loss: 91.1164\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4477 - val_loss: 91.0368\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4500 - val_loss: 91.0691\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4644 - val_loss: 91.0104\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4478 - val_loss: 91.0857\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4488 - val_loss: 91.0206\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4561 - val_loss: 91.0601\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4627 - val_loss: 91.0906\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.4539 - val_loss: 91.0690\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4454 - val_loss: 91.0869\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4500 - val_loss: 91.0705\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4437 - val_loss: 91.0598\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4377 - val_loss: 91.0710\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4445 - val_loss: 91.0625\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4567 - val_loss: 91.0650\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4571 - val_loss: 91.0673\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4476 - val_loss: 91.0575\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4487 - val_loss: 91.0658\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4431 - val_loss: 91.0572\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4485 - val_loss: 91.0575\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4442 - val_loss: 91.0599\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4438 - val_loss: 91.0547\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4482 - val_loss: 91.0697\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4490 - val_loss: 91.0507\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4427 - val_loss: 91.0742\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4413 - val_loss: 91.0616\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4478 - val_loss: 91.0601\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4486 - val_loss: 91.0700\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 95.4351 - val_loss: 91.0658\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4573 - val_loss: 91.0809\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4478 - val_loss: 91.0594\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4437 - val_loss: 91.0621\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4426 - val_loss: 91.0619\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4430 - val_loss: 91.0535\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4456 - val_loss: 91.0650\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4406 - val_loss: 91.0645\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4478 - val_loss: 91.0455\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.4445 - val_loss: 91.0835\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4527 - val_loss: 91.0581\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.4429 - val_loss: 91.0651\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4431 - val_loss: 91.0584\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4389 - val_loss: 91.0554\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4339 - val_loss: 91.0712\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 95.4356 - val_loss: 91.0757\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4448 - val_loss: 91.0662\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4451 - val_loss: 91.0566\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.4414 - val_loss: 91.0460\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.4459 - val_loss: 91.0585\n",
      "Outlier fraction:  0.01\n",
      "OUTLIERS :  4 INLIERS :  361 VAE\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  10 INLIERS :  355 ABOD\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  7 INLIERS :  358 FB\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 HBOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 IF\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  5 INLIERS :  360 KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  3 INLIERS :  362 Average KNN\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 OCSVM\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 SOS\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  6 INLIERS :  359 LOF\n",
      "Model: \"model_94\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_63 (InputLayer)           (None, 95)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_342 (Dense)               (None, 95)           9120        input_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_343 (Dense)               (None, 128)          12288       dense_342[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_187 (Dropout)           (None, 128)          0           dense_343[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_344 (Dense)               (None, 64)           8256        dropout_187[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_188 (Dropout)           (None, 64)           0           dense_344[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_345 (Dense)               (None, 32)           2080        dropout_188[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_189 (Dropout)           (None, 32)           0           dense_345[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_346 (Dense)               (None, 2)            66          dropout_189[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_347 (Dense)               (None, 2)            66          dropout_189[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 2)            0           dense_346[0][0]                  \n",
      "                                                                 dense_347[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,876\n",
      "Trainable params: 31,876\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_95\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_64 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_348 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_349 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_190 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_350 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_191 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_351 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_192 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_352 (Dense)            (None, 95)                12255     \n",
      "=================================================================\n",
      "Total params: 22,789\n",
      "Trainable params: 22,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_96\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_63 (InputLayer)        (None, 95)                0         \n",
      "_________________________________________________________________\n",
      "model_94 (Model)             [(None, 2), (None, 2), (N 31876     \n",
      "_________________________________________________________________\n",
      "model_95 (Model)             (None, 95)                22789     \n",
      "=================================================================\n",
      "Total params: 54,665\n",
      "Trainable params: 54,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_95 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_95.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 188.8700 - val_loss: 137.7177\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 112us/step - loss: 128.0209 - val_loss: 122.9083\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 114.5487 - val_loss: 115.4691\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 110us/step - loss: 105.5435 - val_loss: 107.1028\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 108us/step - loss: 97.7434 - val_loss: 102.4359\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 95.5597 - val_loss: 100.7908\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 113us/step - loss: 94.9730 - val_loss: 100.7223\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 111us/step - loss: 94.6879 - val_loss: 100.6490\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 112us/step - loss: 94.5324 - val_loss: 100.5947\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 92us/step - loss: 94.4990 - val_loss: 100.6009\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 94.4797 - val_loss: 100.5768\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 94.5125 - val_loss: 100.5222\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.4854 - val_loss: 100.4949\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.4547 - val_loss: 100.4687\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.4301 - val_loss: 100.5277\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 94.4570 - val_loss: 100.5390\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.4617 - val_loss: 100.5271\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 94.4240 - val_loss: 100.4965\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.4512 - val_loss: 100.5597\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.4215 - val_loss: 100.4556\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.4314 - val_loss: 100.5020\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3948 - val_loss: 100.4828\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.4134 - val_loss: 100.4833\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.4222 - val_loss: 100.5027\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.4042 - val_loss: 100.4660\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.3824 - val_loss: 100.4615\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.4003 - val_loss: 100.4472\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.4003 - val_loss: 100.4654\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.4108 - val_loss: 100.4580\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.3952 - val_loss: 100.4347\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.4098 - val_loss: 100.4581\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.3977 - val_loss: 100.4600\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.4082 - val_loss: 100.4538\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3915 - val_loss: 100.4620\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.4085 - val_loss: 100.4374\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.3834 - val_loss: 100.4539\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3939 - val_loss: 100.4486\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.4040 - val_loss: 100.4286\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3929 - val_loss: 100.4436\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3994 - val_loss: 100.4345\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.3825 - val_loss: 100.4411\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.3814 - val_loss: 100.4628\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.3925 - val_loss: 100.4625\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.3927 - val_loss: 100.5207\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.4100 - val_loss: 100.4561\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3773 - val_loss: 100.5118\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3969 - val_loss: 100.4341\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.3803 - val_loss: 100.4416\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.3762 - val_loss: 100.5261\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.3929 - val_loss: 100.5063\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3955 - val_loss: 100.4364\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.4041 - val_loss: 100.4253\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.3876 - val_loss: 100.4660\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.4018 - val_loss: 100.3945\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.3907 - val_loss: 100.4625\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3955 - val_loss: 100.4672\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3990 - val_loss: 100.4546\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.3948 - val_loss: 100.4261\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.4009 - val_loss: 100.4875\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 94.3855 - val_loss: 100.4678\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 94.3940 - val_loss: 100.4674\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.3962 - val_loss: 100.4519\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3742 - val_loss: 100.4758\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 94.3888 - val_loss: 100.4457\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.3968 - val_loss: 100.4721\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3888 - val_loss: 100.4739\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3873 - val_loss: 100.4392\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.3834 - val_loss: 100.4451\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3839 - val_loss: 100.4446\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.3722 - val_loss: 100.4272\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.3793 - val_loss: 100.4394\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.3709 - val_loss: 100.4257\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.4105 - val_loss: 100.4478\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.3886 - val_loss: 100.4550\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.3870 - val_loss: 100.4502\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3684 - val_loss: 100.4597\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3840 - val_loss: 100.5058\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.4048 - val_loss: 100.4919\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.4150 - val_loss: 100.4820\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3821 - val_loss: 100.4264\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3934 - val_loss: 100.4857\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.4007 - val_loss: 100.4517\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 94.3922 - val_loss: 100.4039\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.4058 - val_loss: 100.5041\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.3844 - val_loss: 100.4693\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 94.3868 - val_loss: 100.4338\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3857 - val_loss: 100.4649\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 94.3707 - val_loss: 100.4462\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.3848 - val_loss: 100.4750\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.3998 - val_loss: 100.4998\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3686 - val_loss: 100.4718\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.4044 - val_loss: 100.4232\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3957 - val_loss: 100.5178\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.3948 - val_loss: 100.4675\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 94.3777 - val_loss: 100.4671\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.4064 - val_loss: 100.5160\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 94.3850 - val_loss: 100.4312\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3810 - val_loss: 100.4938\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 94.3779 - val_loss: 100.5073\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 94.4140 - val_loss: 100.4762\n",
      "Outlier fraction:  0.02\n",
      "OUTLIERS :  8 INLIERS :  357 VAE\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  14 INLIERS :  351 ABOD\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  10 INLIERS :  355 FB\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 HBOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 IF\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  7 INLIERS :  358 KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  4 INLIERS :  361 Average KNN\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 OCSVM\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 SOS\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  9 INLIERS :  356 LOF\n",
      "Model: \"model_97\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_65 (InputLayer)           (None, 95)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_353 (Dense)               (None, 95)           9120        input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_354 (Dense)               (None, 128)          12288       dense_353[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_193 (Dropout)           (None, 128)          0           dense_354[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_355 (Dense)               (None, 64)           8256        dropout_193[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_194 (Dropout)           (None, 64)           0           dense_355[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_356 (Dense)               (None, 32)           2080        dropout_194[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_195 (Dropout)           (None, 32)           0           dense_356[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_357 (Dense)               (None, 2)            66          dropout_195[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_358 (Dense)               (None, 2)            66          dropout_195[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 2)            0           dense_357[0][0]                  \n",
      "                                                                 dense_358[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,876\n",
      "Trainable params: 31,876\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_98\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_66 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_359 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_360 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_196 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_361 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_197 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_362 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_198 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_363 (Dense)            (None, 95)                12255     \n",
      "=================================================================\n",
      "Total params: 22,789\n",
      "Trainable params: 22,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_99\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_65 (InputLayer)        (None, 95)                0         \n",
      "_________________________________________________________________\n",
      "model_97 (Model)             [(None, 2), (None, 2), (N 31876     \n",
      "_________________________________________________________________\n",
      "model_98 (Model)             (None, 95)                22789     \n",
      "=================================================================\n",
      "Total params: 54,665\n",
      "Trainable params: 54,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_98 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_98.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 194.1363 - val_loss: 118.7416\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 129.3464 - val_loss: 103.0772\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 116.7705 - val_loss: 94.2593\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 107.0561 - val_loss: 85.0301\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 108us/step - loss: 101.0032 - val_loss: 80.6845\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 110us/step - loss: 98.2345 - val_loss: 79.2913\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 111us/step - loss: 97.3946 - val_loss: 78.7070\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 97.1695 - val_loss: 78.8521\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 97.0869 - val_loss: 78.7612\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 106us/step - loss: 97.0159 - val_loss: 78.6404\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 97.0182 - val_loss: 78.5243\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 96.9849 - val_loss: 78.6150\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.9548 - val_loss: 78.6316\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.9379 - val_loss: 78.4869\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 96.9284 - val_loss: 78.4318\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 91us/step - loss: 96.8991 - val_loss: 78.4906\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 96.9140 - val_loss: 78.5725\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 96.8948 - val_loss: 78.5082\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.9183 - val_loss: 78.5179\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.9114 - val_loss: 78.5008\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8866 - val_loss: 78.4885\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8929 - val_loss: 78.5286\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.8892 - val_loss: 78.4978\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.9235 - val_loss: 78.5044\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 96.9219 - val_loss: 78.4858\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 96.8804 - val_loss: 78.4840\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 96.8708 - val_loss: 78.4805\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 96.8923 - val_loss: 78.4822\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 96.8709 - val_loss: 78.4867\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 92us/step - loss: 96.8676 - val_loss: 78.4811\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.8822 - val_loss: 78.4911\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 92us/step - loss: 96.8822 - val_loss: 78.4540\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 96.8784 - val_loss: 78.4927\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8685 - val_loss: 78.4670\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 96.8758 - val_loss: 78.4778\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8871 - val_loss: 78.4568\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.8900 - val_loss: 78.5031\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8969 - val_loss: 78.4678\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8952 - val_loss: 78.4760\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.8823 - val_loss: 78.4582\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.8799 - val_loss: 78.4519\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 96.8832 - val_loss: 78.4502\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.8806 - val_loss: 78.4731\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 96.8691 - val_loss: 78.4666\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.8686 - val_loss: 78.4613\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 96.8773 - val_loss: 78.4660\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8725 - val_loss: 78.4703\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8742 - val_loss: 78.4659\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 96.8742 - val_loss: 78.4527\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8785 - val_loss: 78.4483\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8694 - val_loss: 78.4520\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.8721 - val_loss: 78.4540\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8748 - val_loss: 78.4598\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8724 - val_loss: 78.4654\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 96.8708 - val_loss: 78.4737\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 96.8697 - val_loss: 78.4563\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8719 - val_loss: 78.4706\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8702 - val_loss: 78.4716\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8707 - val_loss: 78.4507\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 96.8648 - val_loss: 78.4650\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8676 - val_loss: 78.4672\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8685 - val_loss: 78.4522\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8736 - val_loss: 78.4522\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8655 - val_loss: 78.4555\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8701 - val_loss: 78.4696\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 96.8664 - val_loss: 78.4657\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8779 - val_loss: 78.4557\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8700 - val_loss: 78.4723\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8800 - val_loss: 78.4519\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.8689 - val_loss: 78.4696\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 96.8728 - val_loss: 78.4657\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8749 - val_loss: 78.4635\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.8693 - val_loss: 78.4706\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8772 - val_loss: 78.4586\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 96.8622 - val_loss: 78.4733\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.8723 - val_loss: 78.4472\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8750 - val_loss: 78.4645\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8728 - val_loss: 78.4604\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8704 - val_loss: 78.4672\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8785 - val_loss: 78.4502\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8660 - val_loss: 78.4684\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8669 - val_loss: 78.4614\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8686 - val_loss: 78.4661\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8789 - val_loss: 78.4692\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8654 - val_loss: 78.4650\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8739 - val_loss: 78.4616\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8712 - val_loss: 78.4552\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8710 - val_loss: 78.4619\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8755 - val_loss: 78.4661\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8585 - val_loss: 78.4600\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.8708 - val_loss: 78.4651\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8669 - val_loss: 78.4641\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 96.8759 - val_loss: 78.4595\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 96.8590 - val_loss: 78.4570\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8707 - val_loss: 78.4547\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8647 - val_loss: 78.4624\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.8668 - val_loss: 78.4548\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 96.8697 - val_loss: 78.4678\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 96.8767 - val_loss: 78.4453\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 96.8703 - val_loss: 78.4583\n",
      "Outlier fraction:  0.03\n",
      "OUTLIERS :  11 INLIERS :  354 VAE\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  18 INLIERS :  347 ABOD\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  12 INLIERS :  353 FB\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 HBOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 IF\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  11 INLIERS :  354 KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  4 INLIERS :  361 Average KNN\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 OCSVM\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 SOS\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  13 INLIERS :  352 LOF\n",
      "Model: \"model_100\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_67 (InputLayer)           (None, 95)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_364 (Dense)               (None, 95)           9120        input_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_365 (Dense)               (None, 128)          12288       dense_364[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_199 (Dropout)           (None, 128)          0           dense_365[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_366 (Dense)               (None, 64)           8256        dropout_199[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_200 (Dropout)           (None, 64)           0           dense_366[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_367 (Dense)               (None, 32)           2080        dropout_200[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_201 (Dropout)           (None, 32)           0           dense_367[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_368 (Dense)               (None, 2)            66          dropout_201[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_369 (Dense)               (None, 2)            66          dropout_201[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 2)            0           dense_368[0][0]                  \n",
      "                                                                 dense_369[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,876\n",
      "Trainable params: 31,876\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_68 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_370 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_371 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_202 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_372 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_203 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_373 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_204 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_374 (Dense)            (None, 95)                12255     \n",
      "=================================================================\n",
      "Total params: 22,789\n",
      "Trainable params: 22,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_102\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_67 (InputLayer)        (None, 95)                0         \n",
      "_________________________________________________________________\n",
      "model_100 (Model)            [(None, 2), (None, 2), (N 31876     \n",
      "_________________________________________________________________\n",
      "model_101 (Model)            (None, 95)                22789     \n",
      "=================================================================\n",
      "Total params: 54,665\n",
      "Trainable params: 54,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_101 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_101.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 176.0670 - val_loss: 124.1040\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 127.1421 - val_loss: 113.5033\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 102us/step - loss: 116.0898 - val_loss: 105.3272\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 104us/step - loss: 106.5727 - val_loss: 94.9334\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 118us/step - loss: 98.5810 - val_loss: 90.9751\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 103us/step - loss: 96.2417 - val_loss: 90.1916\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 118us/step - loss: 95.8497 - val_loss: 89.7952\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 95.7545 - val_loss: 89.9016\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 95.7670 - val_loss: 89.8272\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 95.6647 - val_loss: 89.7547\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 91us/step - loss: 95.6740 - val_loss: 89.8105\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.6730 - val_loss: 89.7460\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6518 - val_loss: 89.7777\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6640 - val_loss: 89.7754\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.6390 - val_loss: 89.8465\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.6104 - val_loss: 89.8145\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6327 - val_loss: 89.8381\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.5892 - val_loss: 89.7652\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6493 - val_loss: 89.7582\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6008 - val_loss: 89.8002\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6189 - val_loss: 89.7703\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.5996 - val_loss: 89.7653\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6134 - val_loss: 89.7733\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 95.6403 - val_loss: 89.7623\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6242 - val_loss: 89.7713\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6245 - val_loss: 89.7368\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6205 - val_loss: 89.7491\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6108 - val_loss: 89.7551\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5991 - val_loss: 89.7420\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5789 - val_loss: 89.7323\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5881 - val_loss: 89.7248\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.5933 - val_loss: 89.7398\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6023 - val_loss: 89.7783\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.5949 - val_loss: 89.7944\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 95.6252 - val_loss: 89.7534\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6018 - val_loss: 89.7503\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5897 - val_loss: 89.7754\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6129 - val_loss: 89.7621\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6056 - val_loss: 89.7395\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.6158 - val_loss: 89.7502\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5865 - val_loss: 89.7373\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.5952 - val_loss: 89.7506\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.6008 - val_loss: 89.7334\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 90us/step - loss: 95.5948 - val_loss: 89.7555\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5948 - val_loss: 89.7409\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5880 - val_loss: 89.7509\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 95.6001 - val_loss: 89.7371\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.5912 - val_loss: 89.7341\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.6021 - val_loss: 89.7431\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5982 - val_loss: 89.7456\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5895 - val_loss: 89.7679\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5960 - val_loss: 89.7560\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6008 - val_loss: 89.7551\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5979 - val_loss: 89.7286\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5977 - val_loss: 89.7578\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5928 - val_loss: 89.7368\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 95.5910 - val_loss: 89.7369\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.5922 - val_loss: 89.7196\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 95.6012 - val_loss: 89.7545\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6183 - val_loss: 89.7431\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5939 - val_loss: 89.7437\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5838 - val_loss: 89.7522\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 95.6110 - val_loss: 89.7124\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5945 - val_loss: 89.7169\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5969 - val_loss: 89.7228\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5911 - val_loss: 89.7594\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.6014 - val_loss: 89.7101\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5976 - val_loss: 89.7509\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5960 - val_loss: 89.7667\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5990 - val_loss: 89.7431\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5964 - val_loss: 89.7531\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5905 - val_loss: 89.7477\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6021 - val_loss: 89.7442\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 95.5804 - val_loss: 89.7325\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5962 - val_loss: 89.7237\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.6122 - val_loss: 89.7279\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 95.6091 - val_loss: 89.7542\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5899 - val_loss: 89.7205\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6028 - val_loss: 89.7396\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 82us/step - loss: 95.6038 - val_loss: 89.7930\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5930 - val_loss: 89.7074\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5956 - val_loss: 89.7342\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6000 - val_loss: 89.7012\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5705 - val_loss: 89.7590\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.5868 - val_loss: 89.7039\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5821 - val_loss: 89.7705\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.6039 - val_loss: 89.7647\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5793 - val_loss: 89.7952\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5899 - val_loss: 89.7373\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5961 - val_loss: 89.6815\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5969 - val_loss: 89.7396\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6110 - val_loss: 89.7418\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 95.6142 - val_loss: 89.7389\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5931 - val_loss: 89.7559\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6006 - val_loss: 89.7442\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.5973 - val_loss: 89.7678\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6087 - val_loss: 89.7466\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 95.5928 - val_loss: 89.7243\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 95.5971 - val_loss: 89.7587\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 95.6085 - val_loss: 89.7351\n",
      "Outlier fraction:  0.04\n",
      "OUTLIERS :  15 INLIERS :  350 VAE\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  21 INLIERS :  344 ABOD\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  37 INLIERS :  328 CBLOF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  18 INLIERS :  347 FB\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 HBOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 IF\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  12 INLIERS :  353 KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  4 INLIERS :  361 Average KNN\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 OCSVM\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 SOS\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  17 INLIERS :  348 LOF\n",
      "Model: \"model_103\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_69 (InputLayer)           (None, 95)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_375 (Dense)               (None, 95)           9120        input_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_376 (Dense)               (None, 128)          12288       dense_375[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_205 (Dropout)           (None, 128)          0           dense_376[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_377 (Dense)               (None, 64)           8256        dropout_205[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_206 (Dropout)           (None, 64)           0           dense_377[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_378 (Dense)               (None, 32)           2080        dropout_206[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_207 (Dropout)           (None, 32)           0           dense_378[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_379 (Dense)               (None, 2)            66          dropout_207[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_380 (Dense)               (None, 2)            66          dropout_207[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 2)            0           dense_379[0][0]                  \n",
      "                                                                 dense_380[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,876\n",
      "Trainable params: 31,876\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_104\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_70 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_381 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_382 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_208 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_383 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_209 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_384 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_210 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_385 (Dense)            (None, 95)                12255     \n",
      "=================================================================\n",
      "Total params: 22,789\n",
      "Trainable params: 22,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_105\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_69 (InputLayer)        (None, 95)                0         \n",
      "_________________________________________________________________\n",
      "model_103 (Model)            [(None, 2), (None, 2), (N 31876     \n",
      "_________________________________________________________________\n",
      "model_104 (Model)            (None, 95)                22789     \n",
      "=================================================================\n",
      "Total params: 54,665\n",
      "Trainable params: 54,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ehren/.local/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_104 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_104.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples, validate on 37 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 171.5343 - val_loss: 160.1204\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 109us/step - loss: 122.6468 - val_loss: 143.2328\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 112.4472 - val_loss: 133.0979\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 108us/step - loss: 104.3959 - val_loss: 125.3717\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 130us/step - loss: 97.1387 - val_loss: 118.1297\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 105us/step - loss: 93.6021 - val_loss: 116.9388\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 107us/step - loss: 92.9654 - val_loss: 116.6177\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 139us/step - loss: 92.8251 - val_loss: 116.5731\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 108us/step - loss: 92.8306 - val_loss: 116.4090\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 95us/step - loss: 92.7871 - val_loss: 116.4448\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 97us/step - loss: 92.7501 - val_loss: 116.4553\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 92.7169 - val_loss: 116.3127\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 92.7713 - val_loss: 116.4094\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7156 - val_loss: 116.3481\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 92.7433 - val_loss: 116.3606\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7315 - val_loss: 116.2975\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7007 - val_loss: 116.2895\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.7324 - val_loss: 116.3101\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 92.7140 - val_loss: 116.3146\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7137 - val_loss: 116.2930\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.7118 - val_loss: 116.2697\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.6997 - val_loss: 116.2769\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.7018 - val_loss: 116.2645\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7272 - val_loss: 116.2352\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.7115 - val_loss: 116.1676\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.7036 - val_loss: 116.2161\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7173 - val_loss: 116.2278\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7074 - val_loss: 116.1968\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7196 - val_loss: 116.1910\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7094 - val_loss: 116.2016\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.7155 - val_loss: 116.2311\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.7024 - val_loss: 116.1939\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 92.6927 - val_loss: 116.1766\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 92.7230 - val_loss: 116.2216\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 92.6931 - val_loss: 116.2179\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.6976 - val_loss: 116.1727\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7113 - val_loss: 116.1737\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.6947 - val_loss: 116.1763\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.6908 - val_loss: 116.2347\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7167 - val_loss: 116.1754\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.7047 - val_loss: 116.2222\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.6958 - val_loss: 116.2333\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.7154 - val_loss: 116.1830\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.7011 - val_loss: 116.2158\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7038 - val_loss: 116.2545\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6954 - val_loss: 116.1785\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 92.6933 - val_loss: 116.2102\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6932 - val_loss: 116.1708\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7030 - val_loss: 116.1824\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.7008 - val_loss: 116.1519\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6933 - val_loss: 116.2396\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.6967 - val_loss: 116.1827\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6938 - val_loss: 116.2197\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 92.7028 - val_loss: 116.2184\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.6966 - val_loss: 116.2197\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.6982 - val_loss: 116.1671\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.7137 - val_loss: 116.1530\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.7053 - val_loss: 116.2107\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.7131 - val_loss: 116.1950\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.6988 - val_loss: 116.1553\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.7069 - val_loss: 116.1804\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6898 - val_loss: 116.1604\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.6819 - val_loss: 116.2166\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6819 - val_loss: 116.1737\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.6991 - val_loss: 116.2696\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6855 - val_loss: 116.2733\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.7029 - val_loss: 116.2520\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.7127 - val_loss: 116.1949\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 92.6985 - val_loss: 116.1777\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7001 - val_loss: 116.2258\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.7032 - val_loss: 116.1355\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6946 - val_loss: 116.1523\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.7014 - val_loss: 116.2063\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 83us/step - loss: 92.7012 - val_loss: 116.2026\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 92.6992 - val_loss: 116.2050\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.7034 - val_loss: 116.2007\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.6980 - val_loss: 116.2178\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 92.6965 - val_loss: 116.1584\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7037 - val_loss: 116.1420\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 88us/step - loss: 92.6970 - val_loss: 116.1488\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6944 - val_loss: 116.1336\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.7009 - val_loss: 116.1482\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.7025 - val_loss: 116.1631\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.6975 - val_loss: 116.1855\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 92.6976 - val_loss: 116.1938\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6951 - val_loss: 116.1506\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.6884 - val_loss: 116.2101\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6971 - val_loss: 116.2254\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6961 - val_loss: 116.1595\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.6843 - val_loss: 116.1592\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 84us/step - loss: 92.7081 - val_loss: 116.1631\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6997 - val_loss: 116.2295\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.6884 - val_loss: 116.2249\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.6932 - val_loss: 116.2259\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 85us/step - loss: 92.6893 - val_loss: 116.2302\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6771 - val_loss: 116.2923\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 89us/step - loss: 92.6644 - val_loss: 116.1537\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.6783 - val_loss: 116.3099\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 87us/step - loss: 92.7106 - val_loss: 116.1902\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 86us/step - loss: 92.6784 - val_loss: 116.3115\n",
      "Outlier fraction:  0.05\n",
      "OUTLIERS :  19 INLIERS :  346 VAE\n"
     ]
    }
   ],
   "source": [
    "# time the code\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "# empty list to store outliers and inliners\n",
    "df_outliers = []\n",
    "scores_pred_lst = []\n",
    "proba_lst = []\n",
    "predict_rank_lst = []\n",
    "\n",
    "# iterate over different outlier fractions \n",
    "for fraction in outliers_fraction:\n",
    "    classifiers = {#'AE' : AutoEncoder(contamination=outliers_fraction, random_state=random_state), # check why error\n",
    "               'ABOD': ABOD(contamination = fraction),\n",
    "              'CBLOF':CBLOF(check_estimator=False, random_state=random_state), # getting 0 \n",
    "              'FB':FeatureBagging(LOF(n_neighbors=35),contamination=fraction,check_estimator=False,random_state=random_state),\n",
    "              'HBOS': HBOS(contamination=fraction),\n",
    "              'IF': IForest(contamination=fraction,random_state=random_state),\n",
    "              'KNN': KNN(contamination=fraction),\n",
    "              'Average KNN': KNN(method='mean',contamination=fraction),\n",
    "              'OCSVM' : OCSVM(contamination=fraction),\n",
    "              'SOS' : SOS(contamination=fraction),\n",
    "              'LOF': LOF(n_neighbors=35, contamination = fraction),\n",
    "              #'LOCI': LOCI(contamination = fraction), # takes a super long time + don't understand\n",
    "              'VAE' : VAE(contamination=fraction, random_state=random_state) # default nbr of neurons too much for blockchain dataset\n",
    "              }\n",
    "\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        clf.fit(X)\n",
    "        # predict raw anomaly score\n",
    "        scores_pred = clf.decision_function(X) * -1 # WHY? * -1\n",
    "\n",
    "        # prediction of a datapoint category outlier or inlier\n",
    "        y_pred = clf.predict(X)\n",
    "\n",
    "        # probability of outlier \n",
    "        probability = clf.predict_proba(X, method = 'linear')\n",
    "\n",
    "        # predict rank\n",
    "        predict_rank = clf._predict_rank(X)\n",
    "\n",
    "        n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "        n_outliers = np.count_nonzero(y_pred == 1)\n",
    "\n",
    "        # copy of dataframe\n",
    "        dfx = X.copy()\n",
    "\n",
    "        dfx[clf_name] = y_pred.tolist()\n",
    "        #print(clf_name,'\\n',i,clf,'\\n',classifiers,'\\n',classifiers.items())\n",
    "        print('Outlier fraction: ', fraction)\n",
    "        print('OUTLIERS : ',n_outliers,'INLIERS : ',n_inliers, clf_name)\n",
    "        #print('scores_pred: ', scores_pred)\n",
    "        scores_pred_lst.append(scores_pred)\n",
    "        df_outliers.append(dfx[clf_name]) \n",
    "        proba_lst.append(probability)\n",
    "        predict_rank_lst.append(predict_rank)\n",
    "t1 = time.time()\n",
    "total = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run:  44.486748933792114 s\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "print('Time to run: ',total,'s')\n",
    "# we have 50 lists (or essentially 5 with 10 models)\n",
    "print(len(df_outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack lists based on outlier fraction e.g. first 10 = 0.01 outlier fraction\n",
    "frac_oo1 = df_outliers[0:11]\n",
    "frac_oo2 = df_outliers[11:22]\n",
    "frac_oo3 = df_outliers[22:33]\n",
    "frac_oo4 = df_outliers[33:44]\n",
    "frac_oo5 = df_outliers[44:]\n",
    "\n",
    "# use the function to extract the dates\n",
    "frac_oo1_outliers = outlier_subset_function(frac_oo1)\n",
    "frac_oo2_outliers = outlier_subset_function(frac_oo2)\n",
    "frac_oo3_outliers = outlier_subset_function(frac_oo3)\n",
    "frac_oo4_outliers = outlier_subset_function(frac_oo4)\n",
    "frac_oo5_outliers = outlier_subset_function(frac_oo5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Anomaly Dates:  61\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AnomalyCount_Fractions</th>\n",
       "      <th>AnomalyCount_Models</th>\n",
       "      <th>weight_models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.023981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.035971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.035971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.026379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-22</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.031175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.031175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.038369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.021583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-02-08</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-11-14</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.023981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.014388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-05-11</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.019185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.014388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-11-24</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.023981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.019185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-08-08</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.014388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.014388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-12-20</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.035971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.047962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.035971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-11-20</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.035971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.033573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-12-23</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.016787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018-03-30</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-11-15</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.023981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018-04-23</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.021583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.016787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2018-03-10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-02-19</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.040767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2018-02-03</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2018-10-12</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2018-11-16</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2018-11-17</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.016787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2018-10-11</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.019185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2018-06-10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2018-08-27</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2018-08-06</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2018-02-13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2018-10-14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  AnomalyCount_Fractions  AnomalyCount_Models  weight_models\n",
       "0   2018-01-11                       5                   10       0.023981\n",
       "1   2018-01-07                       5                   15       0.035971\n",
       "2   2018-01-17                       5                   15       0.035971\n",
       "3   2018-01-09                       5                   11       0.026379\n",
       "4   2018-12-22                       5                    5       0.011990\n",
       "5   2018-01-04                       5                   13       0.031175\n",
       "6   2018-01-08                       5                   13       0.031175\n",
       "7   2018-01-16                       5                   16       0.038369\n",
       "8   2018-02-06                       5                    9       0.021583\n",
       "9   2018-02-08                       5                    5       0.011990\n",
       "10  2018-11-14                       5                   10       0.023981\n",
       "11  2018-01-13                       5                    6       0.014388\n",
       "12  2018-05-11                       5                    8       0.019185\n",
       "13  2018-02-05                       5                    6       0.014388\n",
       "14  2018-11-24                       5                   10       0.023981\n",
       "15  2018-02-04                       5                    5       0.011990\n",
       "16  2018-02-01                       5                    8       0.019185\n",
       "17  2018-08-08                       5                    6       0.014388\n",
       "18  2018-01-15                       5                    6       0.014388\n",
       "19  2018-12-20                       5                    5       0.011990\n",
       "20  2018-01-02                       5                   15       0.035971\n",
       "21  2018-01-06                       5                   20       0.047962\n",
       "22  2018-04-08                       5                   15       0.035971\n",
       "23  2018-11-20                       5                    5       0.011990\n",
       "24  2018-01-01                       5                   15       0.035971\n",
       "25  2018-01-03                       5                   14       0.033573\n",
       "26  2018-12-23                       5                    5       0.011990\n",
       "27  2018-02-17                       5                    5       0.011990\n",
       "28  2018-01-12                       5                    7       0.016787\n",
       "29  2018-03-29                       5                    5       0.011990\n",
       "30  2018-03-30                       5                    5       0.011990\n",
       "31  2018-11-15                       5                   10       0.023981\n",
       "32  2018-04-23                       5                    5       0.011990\n",
       "33  2018-01-10                       5                    9       0.021583\n",
       "34  2018-01-14                       5                    7       0.016787\n",
       "35  2018-03-10                       5                    5       0.011990\n",
       "36  2018-02-19                       5                    5       0.011990\n",
       "37  2018-03-17                       5                    5       0.011990\n",
       "38  2018-01-05                       5                   17       0.040767\n",
       "39  2018-02-03                       4                    4       0.009592\n",
       "40  2018-10-12                       4                    4       0.009592\n",
       "41  2018-04-09                       4                    4       0.009592\n",
       "42  2018-11-16                       4                    4       0.009592\n",
       "43  2018-11-17                       4                    4       0.009592\n",
       "44  2018-01-18                       4                    7       0.016787\n",
       "45  2018-10-11                       4                    8       0.019185\n",
       "46  2018-04-25                       3                    3       0.007194\n",
       "47  2018-03-04                       3                    3       0.007194\n",
       "48  2018-01-20                       3                    3       0.007194\n",
       "49  2018-02-02                       2                    2       0.004796\n",
       "50  2018-06-10                       2                    2       0.004796\n",
       "51  2018-01-22                       2                    2       0.004796\n",
       "52  2018-08-27                       2                    2       0.004796\n",
       "53  2018-08-06                       2                    2       0.004796\n",
       "54  2018-02-13                       1                    1       0.002398\n",
       "55  2018-01-19                       1                    1       0.002398\n",
       "56  2018-10-14                       1                    1       0.002398\n",
       "57  2018-01-21                       1                    1       0.002398\n",
       "58  2018-06-12                       1                    1       0.002398\n",
       "59  2018-11-26                       1                    1       0.002398\n",
       "60  2018-06-13                       1                    1       0.002398"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allthree_anomalies = date_ranking(frac_oo1_outliers,frac_oo2_outliers,frac_oo3_outliers,frac_oo4_outliers,frac_oo5_outliers)\n",
    "print('Unique Anomaly Dates: ',len(allthree_anomalies.Date.unique()))\n",
    "allthree_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all close price values \n",
    "close_anomalies = []\n",
    "for dates in allthree_anomalies['Date']:\n",
    "    # get close price \n",
    "    d = X.loc[str(dates),'close']\n",
    "    close_anomalies.append(d)\n",
    "close_anomalies = pd.Series(close_anomalies)\n",
    "table_dates = allthree_anomalies['Date']\n",
    "anomalies = pd.concat([table_dates,close_anomalies], axis = 1)\n",
    "anomalies.columns = ['Date','Close']\n",
    "anomalies.Date = pd.to_datetime(anomalies.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filter for same dates \n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "common = \\\n",
    "    set.intersection(set(X.index), set(anomalies.Date))\n",
    "filter1 = X[X.index.isin(common)]\n",
    "filter1 = filter1[['close']]\n",
    "\n",
    "# Converting the index as date\n",
    "X.index = pd.to_datetime(X.index)\n",
    "# Alter size for the plot\n",
    "plt.subplots(dpi=100,figsize=(16,6))\n",
    "# plot all close price data\n",
    "plt.plot(X.index, X.close,  alpha=0.5)\n",
    "# set x-axis label and specific size\n",
    "plt.xlabel('Date',size=16)\n",
    "# set y-axis label and specific size\n",
    "plt.ylabel('Close Price',size=16)\n",
    "# set plot title with specific size\n",
    "plt.title('Unique Anomalies between all Models',size=16)\n",
    "\n",
    "# plot anomalies \n",
    "plt.scatter(filter1.index, filter1.close, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it would be interesting to see if there are any \"signals\" for data identified as an anomaly right before price **spikes** or **drops**\n",
    "- I would say major anomalies were flagged but some are still missing (visially) granted only 5 models were used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green color for those voted by 2 \n",
    "# different colors for years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Be Continued "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outlier_proba = np.transpose(pd.DataFrame(probability)).T\n",
    "outlier_proba.index = X.index\n",
    "#outlier_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Date Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "btc_outliers = np.transpose(pd.DataFrame(df_outliers))\n",
    "#btc_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Rank Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_rank_df = np.transpose(pd.DataFrame(predict_rank_lst))\n",
    "pred_rank_df.columns = classifiers.keys()\n",
    "pred_rank_df.index = merged_df.index\n",
    "#pred_rank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersection Between Merge and different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of outliers identified per method \n",
    "outlier_dates = [] # store dates here \n",
    "for col in btc_outliers.columns:\n",
    "    outliers = btc_outliers[btc_outliers[col] == 1] # subset outlier data\n",
    "    #df_outliers.timestamp.apply(lambda x: x.strftime('%Y-%m-%d')) # datetime to string \n",
    "    outlier_dates.append(outliers.index) # append the string/date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nbr of common anomalies among all models: ',len(intersection(outlier_dates[0],outlier_dates[1],outlier_dates[2],\n",
    "        outlier_dates[3],outlier_dates[4],outlier_dates[5],outlier_dates[6])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBLOF intersection dates across tests (arbitrary model check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in three[3] if x in prices_df[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices and blockchain\n",
    "intersection(prices_df[1], block_df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection(prices_df[1], social_df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_knn = list(btc_outliers[btc_outliers['K Nearest Neighbors (KNN)']==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "pca = PCA(n_components=3)  # Reduce to k=3 dimensions\n",
    "X_reduce = pca.fit_transform(X)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_zlabel(\"x_composite_3\")\n",
    "# Plot the compressed data points\n",
    "ax.scatter(X_reduce[:, 0], X_reduce[:, 1], zs=X_reduce[:, 2], s=4, lw=1, label=\"inliers\",c=\"green\")\n",
    "# Plot x's for the ground truth outliers\n",
    "ax.scatter(X_reduce[dates_knn,0],X_reduce[dates_knn,1], X_reduce[dates_knn,2],\n",
    "           lw=2, s=60, marker=\"x\", c=\"red\", label=\"outliers\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN\n",
    "\n",
    "clf_name = 'KNN'\n",
    "clf = KNN(contamination=0.05)\n",
    "clf.fit(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict raw anomaly score \n",
    "# np.set_printoptions(precision=None)\n",
    "\n",
    "#clf.decision_scores_\n",
    "\n",
    "scores_pred = clf.decision_function(X) \n",
    "print(scores_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction on the test data\n",
    "y_pred = clf.predict(X)  # outlier labels (0 or 1)\n",
    "print(y_pred.shape)\n",
    "n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "n_outliers = np.count_nonzero(y_pred == 1)\n",
    "print('OUTLIERS : ',n_outliers,'INLIERS : ',n_inliers, clf_name)\n",
    "# threshold value to consider a datapoint inlier or outlier\n",
    "threshold = stats.scoreatpercentile(scores_pred,100 * 0.05)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# evaluate and print the results\n",
    "print(\"\\nOn Training Data:\")\n",
    "evaluate_print(clf_name, y_train, y_train_scores)\n",
    "print(\"\\nOn Test Data:\")\n",
    "evaluate_print(clf_name, y_test, y_test_scores)\n",
    "\n",
    "visualize(clf_name, X_train, y_train, X_test, y_test, y_train_pred,\n",
    "          y_test_pred, show_figure=True, save_figure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
